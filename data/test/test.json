{"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Parsing a natural language sentence can be viewed as making a sequence of disambiguation decisions : determining the part-of-speech of the words , choosing between possible constituent structures , and selecting labels for the constituents ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Traditionally , disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "However , these approaches have proved too brittle for most interesting natural language problems ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This work addresses the problem of automatically discovering the disambiguation criteria for all of the decisions made during the parsing process , given the set of possible features which can act as disambiguators ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The candidate disambiguators are the words in the sentence , relationships among the words , and relationships among constituents already constructed in the parsing process ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Since most natural language rules are not absolute , the disambiguation criteria discovered in this work are never applied deterministically ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Instead , all decisions are pursued non-deterministically according to the probability of each choice ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "These probabilities are estimated using statistical decision tree models ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The probability of a complete parse tree ( T ) of a sentence ( S ) is the product of each decision (  ) conditioned on all previous decisions :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Each decision sequence constructs a unique parse , and the parser selects the parse whose decision sequence yields the highest cumulative probability ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "By combining a stack decoder search with a breadth-first algorithm with probabilistic pruning , it is possible to identify the highest-probability parse for any sentence using a reasonable amount of memory and time ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The claim of this work is that statistics from a large corpus of parsed sentences combined with information-theoretic classification and training algorithms can produce an accurate natural language parser without the aid of a complicated knowledge base or grammar ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This claim is justified by constructing a parser , called SPATTER ( Statistical PATTErn Recognizer ) , based on very limited linguistic information , and comparing its performance to a state-of-the-art grammar-based parser on a common task ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "It remains to be shown that an accurate broad-coverage parser can improve the performance of a text processing application ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This will be the subject of future experiments ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "One of the important points of this work is that statistical models of natural language should not be restricted to simple , context-insensitive models ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In a problem like parsing , where long-distance lexical information is crucial to disambiguate interpretations accurately , local models like probabilistic context-free grammars are inadequate ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This work illustrates that existing decision-tree technology can be used to construct and estimate models which selectively choose elements of the context which contribute to disambiguation decisions , and which have few enough parameters to be trained using existing resources ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "I begin by describing decision-tree modeling , showing that decision-tree models are equivalent to interpolated n-gram models ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Then I briefly describe the training and parsing procedures used in SPATTER ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Finally , I present some results of experiments comparing SPATTER with a grammarian 's rule-based statistical parser , along with more recent results showing SPATTER applied to the Wall Street Journal domain ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Much of the work in this paper depends on replacing human decision-making skills with automatic decision-making algorithms ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The decisions under consideration involve identifying constituents and constituent labels in natural language sentences ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Grammarians , the human decision-makers in parsing , solve this problem by enumerating the features of a sentence which affect the disambiguation decisions and indicating which parse to select based on the feature values ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The grammarian is accomplishing two critical tasks : identifying the features which are relevant to each decision , and deciding which choice to select based on the values of the relevant features ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Decision-tree classification algorithms account for both of these tasks , and they also accomplish a third task which grammarians classically find difficult ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "By assigning a probability distribution to the possible choices , decision trees provide a ranking system which not only specifies the order of preference for the possible choices , but also gives a measure of the relative likelihood that each choice is the one which should be selected ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A decision tree is a decision-making device which assigns a probability to each of the possible choices based on the context of the decision :  , where f is an element of the future vocabulary ( the set of choices ) and h is a history ( the context of the decision ) ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This probability   is determined by asking a sequence of questions  about the context , where the ith question asked is uniquely determined by the answers to the i - 1 previous questions ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For instance , consider the part-of-speech tagging problem ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The first question a decision tree might ask is :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the word being tagged ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "If the answer is the , then the decision tree needs to ask no more questions ; it is clear that the decision tree should assign the tag  with probability 1 ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "If , instead , the answer to question 1 is bear , the decision tree might next ask the question : 2 ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the tag of the previous word ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "If the answer to question 2 is determiner , the decision tree might stop asking questions and assign the tag  with very high probability , and the tag  with much lower probability ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "However , if the answer to question 2 is noun , the decision tree would need to ask still more questions to get a good estimate of the probability of the tagging decision ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The decision tree described in this paragraph is shown in Figure  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Each question asked by the decision tree is represented by a tree node ( an oval in the figure ) and the possible answers to this question are associated with branches emanating from the node ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Each node defines a probability distribution on the space of possible decisions ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A node at which the decision tree stops asking questions is a leaf node ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The leaf nodes represent the unique states in the decision-making problem , i.e. all contexts which lead to the same leaf node have the same probability distribution for the decision ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A decision-tree model is not really very different from an interpolated n-gram model ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In fact , they are equivalent in representational power ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The main differences between the two modeling techniques are how the models are parameterized and how the parameters are estimated ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "First , let 's be very clear on what we mean by an n-gram model ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Usually , an n-gram model refers to a Markov process where the probability of a particular token being generating is dependent on the values of the previous n - 1 tokens generated by the same process ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "By this definition , an n-gram model has  parameters , where | W | is the number of unique tokens generated by the process ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "However , here let 's define an n-gram model more loosely as a model which defines a probability distribution on a random variable given the values of n - 1 random variables ,  There is no assumption in the definition that any of the random variables F or  range over the same vocabulary ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The number of parameters in this n-gram model is  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Using this definition , an n-gram model can be represented by a decision-tree model with n - 1 questions ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For instance , the part-of-speech tagging model  can be interpreted as a 4-gram model , where  is the variable denoting the word being tagged ,  is the variable denoting the tag of the previous word , and  is the variable denoting the tag of the word two words back ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Hence , this 4-gram tagging model is the same as a decision-tree model which always asks the sequence of 3 questions :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the word being tagged ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the tag of the previous word ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the tag of the word two words back ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "But can a decision-tree model be represented by an n-gram model ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "No , but it can be represented by an interpolated n-gram model ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The proof of this assertion is given in the next section ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The standard approach to estimating an n-gram model is a two step process ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The first step is to count the number of occurrences of each n-gram from a training corpus ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This process determines the empirical distribution ,"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The second step is smoothing the empirical distribution using a separate , held-out corpus ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This step improves the empirical distribution by finding statistically unreliable parameter estimates and adjusting them based on more reliable information ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A commonly-used technique for smoothing is deleted interpolation ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Deleted interpolation estimates a model  by using a linear combination of empirical models  where  and  for all"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For example , a model  might be interpolated as follows :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "where  for all histories  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The optimal values for the  functions can be estimated using the forward-backward algorithm  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A decision-tree model can be represented by an interpolated n-gram model as follows ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A leaf node in a decision tree can be represented by the sequence of question answers , or history values , which leads the decision tree to that leaf ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Thus , a leaf node defines a probability distribution based on values of those questions :  where  and  and where  is the answer to one of the questions asked on the path from the root to the leaf ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "But this is the same as one of the terms in the interpolated n-gram model ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "So , a decision tree can be defined as an interpolated n-gram model where the  function is defined as :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The point of showing the equivalence between n-gram models and decision-tree models is to make clear that the power of decision-tree models is not in their expressiveness , but instead in how they can be automatically acquired for very large modeling problems ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "As n grows , the parameter space for an n-gram model grows exponentially , and it quickly becomes computationally infeasible to estimate the smoothed model using deleted interpolation ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Also , as n grows large , the likelihood that the deleted interpolation process will converge to an optimal or even near-optimal parameter setting becomes vanishingly small ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "On the other hand , the decision-tree learning algorithm increases the size of a model only as the training data allows ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Thus , it can consider very large history spaces , i.e. n-gram models with very large n. Regardless of the value of n , the number of parameters in the resulting model will remain relatively constant , depending mostly on the number of training examples ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The leaf distributions in decision trees are empirical estimates , i.e. relative-frequency counts from the training data ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Unfortunately , they assign probability zero to events which can possibly occur ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Therefore , just as it is necessary to smooth empirical n-gram models , it is also necessary to smooth empirical decision-tree models ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The growing algorithm is an adaptation of the CART algorithm in  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For detailed descriptions and discussions of the decision-tree algorithms used in this work , see  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For example , a question about a word is represented as 30 binary questions ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "These 30 questions are determined by growing a classification tree on the word vocabulary as described in  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The 30 questions represent 30 different binary partitions of the word vocabulary , and these questions are defined such that it is possible to identify each word by asking all 30 questions ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For more discussion of the use of binary decision-tree questions , see  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A parse tree for a sentence is constructed by starting with the sentence 's words as leaves of a tree structure , and labeling and extending nodes these nodes until a single-rooted , labeled tree is constructed ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This pattern recognition process is driven by the decision-tree models described in the previous section ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A parse tree can be viewed as an n-ary branching tree , with each node in a tree labeled by either a non-terminal label or a part-of-speech label ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "If a parse tree is interpreted as a geometric pattern , a constituent is no more than a set of edges which meet at the same tree node ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For instance , the noun phrase , `` a brown cow , '' consists of an edge extending to the right from `` a , '' an edge extending to the left from `` cow , '' and an edge extending straight up from `` brown '' ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In SPATTER , a parse tree is encoded in terms of four elementary components , or features : words , tags , labels , and extensions ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Each feature has a fixed vocabulary , with each element of a given feature vocabulary having a unique representation ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The word feature can take on any value of any word ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The tag feature can take on any value in the part-of-speech tag set ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The label feature can take on any value in the non-terminal set ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The extension can take on any of the following five values :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "right - the node is the first child of a constituent ;"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "left - the node is the last child of a constituent ;"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "up - the node is neither the first nor the last child of a constituent ;"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "unary - the node is a child of a unary constituent ;"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "root - the node is the root of the tree ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For an n word sentence , a parse tree has n leaf nodes , where the word feature value of the ith leaf node is the ith word in the sentence ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The word feature value of the internal nodes is intended to contain the lexical head of the node 's constituent ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A deterministic lookup table based on the label of the internal node and the labels of the children is used to approximate this linguistic notion ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The SPATTER representation of the sentence"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "is shown in Figure  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The nodes are constructed bottom-up from left-to-right , with the constraint that no constituent node is constructed until all of its children have been constructed ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The order in which the nodes of the example sentence are constructed is indicated in the figure ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "SPATTER consists of three main decision-tree models : a part-of-speech tagging model , a node-extension model , and a node-labeling model ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Each of these decision-tree models are grown using the following questions , where X is one of word , tag , label , or extension , and Y is either left and right :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the X at the current node ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the X at the node to the Y ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the X at the node two nodes to the Y ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the X at the current node 's first child from the Y ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "What is the X at the current node 's second child from the Y ?"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For each of the nodes listed above , the decision tree could also ask about the number of children and span of the node ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For the tagging model , the values of the previous two words and their tags are also asked , since they might differ from the head words of the previous two constituents ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The training algorithm proceeds as follows ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The training corpus is divided into two sets , approximately 90 % for tree growing and 10 % for tree smoothing ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For each parsed sentence in the tree growing corpus , the correct state sequence is traversed ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Each state transition from  to  is an event ; the history is made up of the answers to all of the questions at state  and the future is the value of the action taken from state  to state  Each event is used as a training example for the decision-tree growing process for the appropriate feature 's tree ( e.g. each tagging event is used for growing the tagging tree , etc ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": ") ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "After the decision trees are grown , they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The parsing procedure is a search for the highest probability parse tree ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The probability of a parse is just the product of the probability of each of the actions made in constructing the parse , according to the decision-tree models ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Because of the size of the search space , ( roughly  where | T | is the number of part-of-speech tags , n is the number of words in the sentence , and | N | is the number of non-terminal labels ) , it is not possible to compute the probability of every parse ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "However , the specific search algorithm used is not very important , so long as there are no search errors ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "SPATTER 's search procedure uses a two phase approach to identify the highest probability parse of a sentence ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "First , the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Once the stack decoder has found a complete parse of reasonable probability (  ) , it switches to a breadth-first mode to pursue all of the partial parses which have not been explored by the stack decoder ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In this second mode , it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Using these two search modes , SPATTER guarantees that it will find the highest probability parse ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The only limitation of this search technique is that , for sentences which are modeled poorly , the search might exhaust the available memory before completing both phases ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "However , these search errors conveniently occur on sentences which SPATTER is likely to get wrong anyway , so there isn't much performance lossed due to the search errors ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Experimentally , the search algorithm guarantees the highest probability parse is found for over 96 % of the sentences parsed ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In the absence of an NL system , SPATTER can be evaluated by comparing its top-ranking parse with the treebank analysis for each test sentence ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The parser was applied to two different domains , IBM Computer Manuals and the Wall Street Journal ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The first experiment uses the IBM Computer Manuals domain , which consists of sentences extracted from IBM computer manuals ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The training and test sentences were annotated by the University of Lancaster ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The Lancaster treebank uses 195 part-of-speech tags and 19 non-terminal labels ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This treebank is described in great detail in  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based , unification-style probabilistic context-free grammar for parsing this domain ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The purpose of the experiment was to estimate SPATTER 's ability to learn the syntax for this domain directly from a treebank , instead of depending on the interpretive expertise of a grammarian ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The parser was trained on the first 30,800 sentences from the Lancaster treebank ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The test set included 1,473 new sentences , whose lengths range from 3 to 30 words , with a mean length of 13.7 words ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "These sentences are the same test sentences used in the experiments reported for IBM 's parser in  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In  , IBM 's parser was evaluated using the 0-crossing - brackets measure , which represents the percentage of sentences for which none of the constituents in the parser 's parse violates the constituent boundaries of any constituent in the correct parse ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "After over ten years of grammar development , the IBM parser achieved a 0-crossing - brackets score of 69 % ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "On this same test set , SPATTER scored 76 % ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The experiment is intended to illustrate SPATTER 's ability to accurately parse a highly-ambiguous , large-vocabulary domain ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "These experiments use the Wall Street Journal domain , as annotated in the Penn Treebank , version 2 ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The Penn Treebank uses 46 part-of-speech tags and 27 non-terminal labels ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The WSJ portion of the Penn Treebank is divided into 25 sections , numbered 00 - 24 ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In these experiments , SPATTER was trained on sections 02 - 21 , which contains approximately 40,000 sentences ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The test results reported here are from section 00 , which contains 1920 sentences ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Sections 01 , 22 , 23 , and 24 will be used as test data in future experiments ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The Penn Treebank is already tokenized and sentence detected by human annotators , and thus the test results reported here reflect this ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "SPATTER parses word sequences , not tag sequences ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Furthermore , SPATTER does not simply pre-tag the sentences and use only the best tag sequence in parsing ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Instead , it uses a probabilistic model to assign tags to the words , and considers all possible tag sequences according to the probability they are assigned by the model ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "No information about the legal tags for a word are extracted from the test corpus ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In fact , no information other than the words is used from the test corpus ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For the sake of efficiency , only the sentences of 40 words or fewer are included in these experiments ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "For this test set , SPATTER takes on average 12 seconds per sentence on an SGI R4400 with 160 megabytes of RAM ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "To evaluate SPATTER 's performance on this domain , I am using the PARSEVAL measures , as defined in  ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Precision"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Recall"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Crossing Brackets"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "no . of constituents which violate constituent boundaries with a constituent in the treebank parse ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The precision and recall measures do not consider constituent labels in their evaluation of a parse , since the treebank label set will not necessarily coincide with the labels used by a given grammar ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Since SPATTER uses the same syntactic label set as the Penn Treebank , it makes sense to report labelled precision and labelled recall ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "These measures are computed by considering a constituent to be correct if and only if it 's label matches the label in the treebank ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Table  shows the results of SPATTER evaluated against the Penn Treebank on the Wall Street Journal section 00 ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Figures  ,  , and  illustrate the performance of SPATTER as a function of sentence length ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "SPATTER 's performance degrades slowly for sentences up to around 28 words , and performs more poorly and more erratically as sentences get longer ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Figure  indicates the frequency of each sentence length in the test corpus ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Regardless of what techniques are used for parsing disambiguation , one thing is clear : if a particular piece of information is necessary for solving a disambiguation problem , it must be made available to the disambiguation mechanism ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The words in the sentence are clearly necessary to make parsing decisions , and in some cases long-distance structural information is also needed ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Statistical models for parsing need to consider many more features of a sentence than can be managed by n-gram modeling techniques and many more examples than a human can keep track of ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text , as is evidenced by their poor performance on domains like the Wall Street Journal , and by the movement away from parsing-based approaches to text-processing in general ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In this paper , I describe SPATTER , a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "This work is based on the following premises :"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "grammars are too complex and detailed to develop manually for most interesting domains ;"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "parsing models must rely heavily on lexical and contextual information to analyze sentences accurately ; and"}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "existing n-gram modeling techniques are inadequate for parsing models ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "In experiments comparing SPATTER with IBM 's computer manuals parser , SPATTER significantly outperforms the grammar-based parser ."}
 {"title": "Statistical Decision-Tree Models for Parsing", "sentence": "Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures , SPATTER achieves 86 % precision , 86 % recall , and 1.3 crossing brackets per sentence for sentences of 40 words or less , and 91 % precision , 90 % recall , and 0.5 crossing brackets for sentences between 10 and 20 words in length ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Semitic is known amongst computational linguists , in particular computational morphologists , for its highly inflexional morphology ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Its root-and-pattern phenomenon not only poses difficulties for a morphological system , but also makes error detection a difficult task ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "This paper aims at presenting a morphographemic model which can cope with both issues ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The following convention has been adopted ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Morphemes are represented in braces , { } , surface ( phonological ) forms in solidi , / / , and orthographic strings in acute brackets ,   ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In examples of grammars , variables begin with a capital letter ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Cs denote consonants , Vs denote vowels and a bar denotes complement ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "An asterisk , * , indicates ill-formed strings ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The difficulties in morphological analysis and error detection in Semitic arise from the following facts :"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Non-Linearity"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A Semitic stem consists of a root and a vowel melody , arranged according to a canonical pattern ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example , Arabic /kuttib/ ` caused to write - perfect passive ' is composed from the root morpheme { ktb } ` notion of writing ' and the vowel melody morpheme { ui } ` perfect passive ' ; the two are arranged according to the pattern morpheme { CVCCVC } ` causative ' ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "This phenomenon is analysed by  along the lines of autosegmental phonology  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The analysis appears in  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Vocalisation"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Orthographically , Semitic texts appear in three forms :"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "consonantal texts do not incorporate any short vowels but matres lectionis , e.g. Arabic  ktb  for /katab/ , /kutib/ and /kutub/ , but  kaatb  for /kaatab/ and /kaatib/ ;"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "partially vocalised texts incorporate some short vowels to clarify ambiguity , e.g.  kutb  for /kutib/ to distinguish it from /katab/ ; and"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "vocalised texts incorporate full vocalisation , e.g.  tada ' 043 ra for /tada ' 043 ra/ ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Vowel and Diacritic Shifts"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Semitic languages employ a large number of diacritics to represent enter alia short vowels , doubled letters , and nunation ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Most editors allow the user to enter such diacritics above and below letters ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "To speed data entry , the user usually enters the base characters ( say a paragraph ) and then goes back and enters the diacritics ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A common mistake is to place the cursor one extra position to the left when entering diacritics ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "This results in the vowels being shifted one position , e.g. *  wkatubi  instead of  wakutib  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Vocalisms"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The quality of the perfect and imperfect vowels of the basic forms of the Semitic verbs are idiosyncratic ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example , the Syriac root { ktb } takes the perfect vowel a , e.g. /ktab/ , while the root  takes the vowel e , e.g.  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "It is common among learners to make mistakes such as */kteb/ or  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Phonetic Syncopation"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A consonantal segment may be omitted from the phonetic surface form , but maintained in the orthographic surface from ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example , Syriac  mdnt  ` city ' is pronounced /mdt/ ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Idiosyncrasies"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The application of a morphographemic rule may have constraints as on which lexical morphemes it may or may not apply ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example , the glottal stop [ ' 145 ] at the end of a stem may become [ w ] when followed by the relative adjective morpheme { iyy } , as in Arabic /samaa + iyy/  /samaawiyy/ ` heavenly ' , but /hawaa + iyy/  /hawaa ' 145 iyy/ ` of air ' ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Morphosyntactic Issues"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In broken plurals , diminutives and deverbal nouns , the user may enter a morphologically sound , but morphosyntactically ill-formed word ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "We shall discuss this in more detail in section  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "To the above , one adds language-independent issues in spell checking such as the four  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "This section presents a morphographemic model which handles error detection in non-linear strings ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Subsection  presents the formalism used , and subsection  describes the model ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In order to handle the non-linear phenomenon of Arabic , our model adopts the two-level formalism presented by  , with the multi tape extensions in  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Their formalism appears in  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The special symbol * is a wildcard matching any context , with no length restrictions ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The operator  caters for obligatory rules ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A lexical string maps to a surface string iff they can be partitioned into pairs of lexical-surface subsequences , where each pair is licenced by a  or  rule , and no partition violates a  rule ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In the multi-tape version , lexical expressions ( i.e. LLC , LEX and RLC ) are n-tuple of regular expressions of the form  : the ith expression refers to symbols on the ith tape ; a nill slot is indicated by  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Another extension is giving LLC the ability to contain ellipsis ,  , which indicates the ( optional ) omission from LLC of tuples , provided that the tuples to the left of  are the first to appear on the left of LEX ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In our morphographemic model , we add a similar formalism for expressing error rules  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The error rules capture the correspondence between the error surface and the correct surface , given the surrounding partition into surface and lexical contexts ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "They happily utilise the multi-tape format and integrate seamlessly into morphological analysis ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "PLC and PRC above are the left and right contexts of both the lexical and ( correct ) surface levels ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Only the  is used ( error is not obligatory ) ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Morphological analysis is first called with the assumption that the word is free of errors ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "If this fails , analysis is attempted again without the ` no error ' restriction ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The error rules are then considered when ordinary morphological rules fail ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "If no error rules succeed , or lead to a successful partition of the word , analysis backtracks to try the error rules at successively earlier points in the word ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For purposes of simplicity and because on the whole is it likely that words will contain no more than one error  ,  , normal ` no error ' analysis usually resumes if an error rule succeeds ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The exception occurs with a vowel shift error  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "If this error rule succeeds , an expectation of further shifted vowels is set up , but no other error rule is allowed in the subsequent partitions ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For this reason rules are marked as to whether they can occur more than once ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Once an error rule is selected , the corrected surface is substituted for the error surface , and normal analysis continues - at the same position ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The substituted surface may be in the form of a variable , which is then ground by the normal analysis sequence of lexical matching over the lexicon tree ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In this way only lexical words are considered , as the variable letter can only be instantiated to letters branching out from the current position on the lexicon tree ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Normal prolog backtracking to explore alternative rules / lexical branches applies throughout ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "We demonstrate our model on the Arabic verbal stems shown in   ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Verbs are classified according to their measure ( M ) : there are 15 trilateral measures and 4 quadrilateral ones ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Moving horizontally across the table , one notices a change in vowel melody ( active { a } , passive { ui } ) ; everything else remains invariant ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Moving vertically , a change in canonical pattern occurs ; everything else remains invariant ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Subsection  presents a simple two-level grammar which describes the above data ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Subsection  presents error checking ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The lexical level maintains three lexical tapes  ,  : pattern tape , root tape and vocalism tape ; each tape scans a lexical tree ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Examples of pattern morphemes are :  ,  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The root morphemes are { ktb } and  , and the vocalism morphemes are { a } ( active ) and { ui } ( passive ) ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The following two-level grammar handles the above data ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Each lexical expression is a triple ; lexical expressions with one symbol assume  on the remaining positions ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "gives three general rules :  allows any character on the first lexical tape to surface , e.g. infixes , prefixes and suffixes ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "states that any P  on the first ( pattern ) tape and C on the second ( root ) tape with no transition on the third ( vocalism ) tape corresponds to C on the surface tape ; this rule sanctions consonants ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Similarly ,  states that any P  on the pattern tape and V on vocalism tape with no transition on the root tape corresponds to V on the surface tape ; this rule sanctions vowels ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "gives two boundary rules :  is used for non-stem morphemes , e.g. prefixes and suffixes ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "applies to stem morphemes reading three boundary symbols simultaneously ; this marks the end of a stem ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Notice that LLC ensures that the right boundary rule is invoked at the right time ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Before embarking on the rest of the rules , an illustrated example seems in order ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The derivation of /d ' 043 unri a/ ( M Q 5 , passive ) , from the three morphemes { c  c  v  nc  v  c  } , { d ' 043 r } and { ui } , and the suffix { a } ` 3rd person ' is illustrated in  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The numbers between the surface tape and the lexical tapes indicate the rules which sanction the moves ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Resuming the description of the grammar ,  presents spreading rules ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Notice the use of ellipsis to indicate that there can be tuples separating LEX and LLC , as far as the tuples in LLC are the nearest ones to LEX ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "sanctions the spreading ( and gemination ) of consonants ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "sanctions the spreading of the first vowel ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Spreading examples appear in  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The following rules allow for the different possible orthographic vocalisations in Semitic texts :"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "where  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "and  allow the optional deletion of short vowels in non-stem and stem morphemes , respectively ; note that the lexical contexts make sure that long vowels are not deleted ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "allows the optional deletion of a short vowel what is the cause of spreading ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example the rules sanction both /katab/ ( M 1 , active ) and /kutib/ ( M 1 , passive ) as interpretations of  ktb  as showin in  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Below are outlined error rules resulting from peculiarly Semitic problems ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Error rules can also be constructed in a similar vein to deal with typographical"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A vowel shift error rule will be tried with a partition on a ( short ) vowel which is not an expected ( lexical ) vowel at that position ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Short vowels can legitimately be omitted from an orthographic representation - it is this fact which contributes to the problem of vowel shifts ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A vowel is considered shifted if the same vowel has been omitted earlier in the word ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The rule deletes the vowel from the surface ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Hence in the next pass of ( normal ) analysis , the partition is analysed as a legitimate omission of the expected vowel ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "This prepares for the next shifted vowel to be treated in exactly the same way as the first ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The expectation of this reapplication is allowed for in reap = y ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In the rules above , ` X ' is the shifted vowel ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "It is deleted from the surface ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The partition contextual tuples consist of [ RULE NAME , SURF , LEX ] ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The LEX element is a tuple itself of [ PATTERN , ROOT , VOCALISM ] ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In E 0 the shifted vowel was analysed earlier as an omitted stem vowel ( om_stmv ) , whereas in E 1 it was analysed earlier as an omitted spread vowel ( om_sprv ) ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The surface / lexical restrictions in the contexts could be written out in more detail , but both rules make use of the fact that those contexts are analysed by other partitions , which check that they meet the conditions for an omitted stem vowel or omitted spread vowel ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example ,  will be interpreted as  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The ` E 0 's on the rule number line indicate where the vowel shift rule was applied to replace an error surface vowel with  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The error surface vowels are written in italics ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Problems resulting from phonetic syncopation can be treated as accidental omission of a consonant , e.g.  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Although the error probably results from a different fault , a deleted long vowel can be treated in the same way as a deleted consonant ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "With current transcription practice , long vowels are commonly written as two characters - they are possibly better represented as a single , distinct character ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The form ] can be interpreted as either  with a deleted consonant ( geminated ` t ' ) or  with a deleted long vowel ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "One type of morphographemic error is that consonant substitution may not take place before appending a suffix ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example  ` heaven ' +  ` relative adjective ' surfaces as  , where  in the given context ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A common mistake is to write it as  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The ` glottal _ change ' rule would be a normal morphological spelling change rule , incorporating contextual constraints ( e.g. for the morpheme boundary ) as necessary ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "This section deals with morphosyntactic errors which are independent of the two-level analysis ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The data described below was obtained from  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Recall that a Semitic stems consists of a root morpheme and a vocalism morpheme arranged according to a canonical pattern morpheme ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "As each root does not occur in all vocalisms and patterns , each lexical entry is associated with a feature structure which indicates inter alia the possible patterns and vocalisms for a particular root ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Consider the nominal data in  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Patterns marked with * are morphologically plausible , but do not occur lexically with the cited nouns ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A common mistake is to choose the wrong pattern ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "In such a case , the two-level model succeeds in finding two-level analyses of the word in question , but fails when parsing the word morphosyntactically : at this stage , the parser is passed a root , vocalism and pattern whose feature structures do not unify ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Usually this feature-clash situation creates the problem of which constituent to give preference to  ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Here the vocalism indicates the inflection ( e.g. broken plural ) and the preferance of vocalism pattern for that type of inflection belongs to the root ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "For example  would be analysed as  with a broken plural vocalism ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The pattern type of the vocalism clashes with the broken plural pattern that the root expects ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "To correct , the morphological analyser is executed in generation mode to generate the broken plural form of { kd } in the normal way ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The same procedure can be applied on diminutive and deverbal nouns ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The model presented corrects errors resulting from combining nonconcatenative strings as well as more standard morphological or spelling errors ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "It covers Semitic errors relating to vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Morphosyntactic issues of broken plurals , diminutives and deverbal nouns can be handled by a complementary correction strategy which also depends on morphological analysis ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Other than the economic factor , an important advantage of combining morphological analysis and error detection / correction is the way the lexical tree associated with the analysis can be used to determine correction possibilities ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The morphological analysis proceeds by selecting rules that hypothesise lexical strings for a given surface string ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The rules are accepted / rejected by checking that the lexical string ( s ) can extend along the lexical tree ( s ) from the current position ( s ) ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Variables introduced by error rules into the surface string are then instantiated by associating surface with lexical , and matching lexical strings to the lexicon tree ( s ) ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The system is unable to consider correction characters that would be lexical impossibilities ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "This paper introduces a spelling correction system which integrates seamlessly with morphological analysis using a multi-tape formalism ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "Handling of various Semitic error problems is illustrated , with reference to Arabic and Syriac examples ."}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "The model handles errors vocalisation , diacritics , phonetic syncopation and morphographemic idiosyncrasies , in addition to"}
 {"title": "A Morphographemic Model for Error Correction in Nonconcatenative Strings", "sentence": "A complementary correction strategy for morphologically sound but morphosyntactically ill-formed words is outlined ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Categorial Grammar ( CG ) and in particular Lambek Categorial Grammar ( LCG ) have their well-known benefits for the formal treatment of natural language syntax and semantics ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The most outstanding of these benefits is probably the fact that the specific way , how the complete grammar is encoded , namely in terms of ` combinatory potentials ' of its words , gives us at the same time recipes for the construction of meanings , once the words have been combined with others to form larger linguistic entities ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Although both frameworks are equivalent in weak generative capacity -- both derive exactly the context-free languages -- , LCG is superior to CG in that it can cope in a natural way with extraction and unbounded dependency phenomena ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "For instance , no special category assignments need to be stipulated to handle a relative clause containing a trace , because it is analyzed , via hypothetical reasoning , like a traceless clause with the trace being the hypothesis to be discharged when combined with the relative pronoun ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Figure  illustrates this proof-logical behaviour ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Notice that this natural-deduction-style proof in the type logic corresponds very closely to the phrase-structure tree one would like to adopt in an analysis with traces ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We thus can derive Bill misses  as an s from the hypothesis that there is a `` phantom '' np in the place of the trace ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Discharging the hypothesis , indicated by index 1 , results in Bill misses being analyzed as an s / np from zero hypotheses ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Observe , however , that such a bottom-up synthesis of a new unsaturated type is only required , if that type is to be consumed ( as the antecedent of an implication ) by another type ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Otherwise there would be a simpler proof without this abstraction ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "In our example the relative pronoun has such a complex type triggering an extraction ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "A drawback of the pure Lambek Calculus L is that it only allows for so-called ` peripheral extraction ' , i.e. , in our example the trace should better be initial or final in the relative clause ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "This inflexibility of Lambek Calculus is one of the reasons why many researchers study richer systems today ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "For instance , the recent work by  gives a systematic in-depth study of mixed Lambek systems , which integrate the systems L , NL , NLP , and LP ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "These ingredient systems are obtained by varying the Lambek calculus along two dimensions : adding the permutation rule ( P ) and / or dropping the assumption that the type combinator ( which forms the sequences the systems talk about ) is associative ( N for non-associative ) ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Taken for themselves these variants of L are of little use in linguistic descriptions ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "But in"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The relative pronoun which would , for instance , receive category  with  being implication in LP , i.e. , it requires as an argument `` an s lacking an np somewhere '' ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The present paper studies the computational complexity of a variant of the Lambek Calculus that lies between L and LP, the Semidirectional Lambek Calculus SDL ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Since derivability is known to be NP-complete , it is interesting to study restrictions on the use of the operator  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "A restriction that leaves its proposed linguistic applications intact is to admit a type  only as the argument type in functional applications , but never as the functor ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Stated prove-theoretically for Gentzen-style systems , this amounts to disallowing the left rule for  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Surprisingly , the resulting system SDL can be stated without the need for structural rules , i.e. , as a monolithic system with just one structural connective , because the ability of the abstracted-over formula to permute can be directly encoded in the right rule for  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Note that our purpose for studying SDL is not that it might be in any sense better suited for a theory of grammar ( except perhaps , because of its simplicity ) , but rather , because it exhibits a core of logical behaviour that any richer system also needs to include , at least if it should allow for non-peripheral extraction ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The sources of complexity uncovered here are thus a forteriori present in all these richer systems as well ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The semidirectional Lambek calculus ( henceforth SDL ) is a variant of  's original calculus of syntactic types ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We start by defining the Lambek calculus and extend it to obtain SDL ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Formulae ( also called `` syntactic types '' ) are built from a set of propositional variables ( or `` primitive types '' )  and the three binary connectives  ,  , / , called product , left implication , and right implication ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We use generally capital letters A , B , C , ... to denote formulae and capitals towards the end of the alphabet T , U , V , ... to denote sequences of formulae ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The concatenation of sequences U and V is denoted by ( U , V ) ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The ( usual ) formal framework of these logics is a Gentzen-style sequent calculus ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Sequents are pairs ( U , A ) , written as  , where A is a type and U is a sequence of types ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The claim embodied by sequent  can be read as `` formula A is derivable from the structured database U '' ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Figure  shows"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "First of all , since we don't need products to obtain our results and since they only complicate matters , we eliminate products from consideration in the sequel ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "In Semidirectional Lambek Calculus we add as additional connective the LP implication  , but equip it only with a right rule ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Let us define the polarity of a subformula of a sequent  as follows : A has positive polarity , each of  have negative polarity and if B / C or  has polarity p , then B also has polarity p and C has the opposite polarity of p in the sequent ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "A consequence of only allowing the  rule , which is easily proved by induction , is that in any derivable sequent  may only appear in positive polarity ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Hence ,  may not occur in the ( cut ) formula A of a  application and any subformula  which occurs somewhere in the prove must also occur in the final sequent ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "When we assume the final sequent 's RHS to be primitive ( or  - less ) , then the  rule will be used exactly once for each ( positively ) occuring  - subformula ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "In other words ,  may only do what it is supposed to do : extraction , and we can directly read off the category assignment which extractions there will be ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We can show Cut Elimination for this calculus by a straight-forward adaptation of the Cut elimination proof for ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We omit the proof for reasons of space ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The cut-free system enjoys , as usual for Lambek-like logics , the Subformula Property : in any proof only subformulae of the goal sequent may appear ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "In our considerations below we will make heavy use of the well-known count invariant for Lambek systems  , which is an expression of the resource-consciousness of these logics ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Define  ( the b-count of A ) , a function counting positive and negative occurrences of primitive type b in an arbitrary type A , to be"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The invariant now states that for any primitive b , the b-count of the RHS and the LHS of any derivable sequent are the same ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "By noticing that this invariant is true for Ax and is preserved by the rules , we immediately can state :"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Let us in parallel to SDL consider the fragment of it in which  and  are disallowed ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We call this fragment SDL- ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Remarkable about this fragment is that any positive occurrence of an implication must be  and any negative one must be / or  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We extend the lexical map l to nonempty strings of terminals by setting  for  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The language generated by a Lambek grammar  is defined as the set of all strings  for which there exists a sequence of types  and  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We denote this language by L(G) ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "An SDL-grammar is defined exactly like a Lambek grammar , except that  replaces  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Given a grammar G and a string  , the parsing ( or recognition ) problem asks the question , whether w is in L(G) ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "It is not immediately obvious , how the generative capacity of SDL-grammars relate to Lambek grammars or nondirectional Lambek grammars ( based on calculus LP ) ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Whereas Lambek grammars generate exactly the context-free languages ( modulo the missing empty word )  , the latter generate all permutation closures of context-free languages  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "This excludes many context-free or even regular languages , but includes some context-sensitive ones , e.g. , the permutation closure of  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Concerning SDL , it is straightforward to show that all context-free languages can be generated by SDL-grammars ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Proof ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We can use a the standard transformation of an arbitrary cfr. grammar  to a categorial grammar G ' ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Since  does not appear in G ' each SDL-proof of a lexical assignment must be also an SDL-proof , i.e. exactly the same strings are judged grammatical by SDL as are judged by L ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Note that since the  subset of L already accounts for the cfr. languages , this observation extends to SDL- ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Moreover , some languages which are not context-free can also be generated ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Example ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Consider the following grammar G for the language  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We use primitive types  and define the lexical map for  as follows :"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The distinguished primitive type is x. To simplify the argumentation , we abbreviate types as indicated above ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Now , observe that a sequent  , where U is the image of some string over  , only then may have balanced primitive counts , if U contains exactly one occurrence of each of  ,  and  ( accounting for the one supernumerary x and balanced y and z counts ) and for some number  , n occurrences of each of  ,  , and  ( because , resource-oriented speaking , each  and  `` consume '' a b and c , resp. , and each  `` provides '' a pair b , c ) ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Hence , only strings containing the same number of a 's , b 's and c 's may be produced ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Furthermore , due to the Subformula Property we know that in a cut-free proof of  , the main formula in abstractions ( right rules ) may only be either  or  , where  , since all other implication types have primitive antecedents ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Hence , the LHS of any sequent in the proof must be a subsequence of U , with some additional b types and c types interspersed ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "But then it is easy to show that U can only be of the form"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "since any / connective in U needs to be introduced via  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "It remains to be shown , that there is actually a proof for such a sequent ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "It is given in Figure  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The sequent marked with  is easily seen to be derivable without abstractions ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "A remarkable point about SDL- 's ability to cover this language is that neither L nor LP can generate it ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Hence , this example substantiates the claim made in  that the inferential capacity of mixed Lambek systems may be greater than the sum of its component parts ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Moreover , the attentive reader will have noticed that our encoding also extends to languages having more groups of n symbols , i.e. , to languages of the form  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Finally , we note in passing that for this grammar the rules  and  are irrelevant , i.e. that it is at the same time an grammar ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We show that the Parsing Problem for SDL-grammars is NP-complete by a reduction of the 3-Partition Problem to it ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "This well-known NP-complete problem is cited in  as follows ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Here is our reduction ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Let  be a given 3-Partition instance ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "For notational convenience we abbreviate  by  and similarly  by  , but note that this is just an abbreviation in the product-free fragment ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Moreover the notation  stands for"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We then define the SDL-grammar  as follows :"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The word we are interested in is  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We do not care about other words that might be generated by  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Our claim now is that a given 3-Partition problem  is solvable if and only if  is in  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We consider each direction in turn ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Proof ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We have to show , when given a solution to  , how to choose a type sequence  and construct an SDL proof for  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Suppose  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "From a given solution ( set of triples )  we can compute in polynomial time a mapping k that sends the index of an element to the index of its solution triple , i.e. ,  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "To obtain the required sequence U , we simply choose for the  terminals the type  ( resp ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "for  ) ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Hence the complete sequent to solve is :"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Let  be a shorthand for ( * ) , and let X stand for the sequence of primitive types ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Using rule  only , we can obviously prove  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Now , applying  3 m + Nm times we can obtain  , since there are in total , for each i , 3  and N  in X ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "As final step we have"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "which completes the proof ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Proof ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Let  and"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "be a witnessing derivable sequent , i.e. , for  ,  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Now , since the counts of this sequent must be balanced , the sequence  must contain for each  exactly 3  and exactly N  as subformulae ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Therefore we can read off the solution to  from this sequent by including in  ( for  ) those three  for which  has an occurrence of  , say these are  ,  and  ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We verify , again via balancedness of the primitive counts , that  holds , because these are the numbers of positive and negative occurrences of  in the sequent ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "This completes the proof ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The reduction above proves NP-hardness of the parsing problem ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We need strong NP-completeness of 3-Partition here , since our reduction uses a unary encoding ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Moreover , the parsing problem also lies within NP , since for a given grammar G proofs are linearly bound by the length of the string and hence , we can simply guess a proof and check it in polynomial time ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Therefore we can state the following :"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Finally , we observe that for this reduction the rules  and  are again irrelevant and that we can extend this result to ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We have defined a variant of"}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Grammars based on SDL can generate any context-free language and more than that ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "The parsing problem for SDL , however , we have shown to be NP-complete ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "This result indicates that efficient parsing for grammars that allow for large numbers of unbounded dependencies from within one node may be problematic , even in the categorial framework ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "Note that the fact , that this problematic case doesn't show up in the correct analysis of normal NL sentences , doesn't mean that a parser wouldn't have to try it , unless some arbitrary bound to that number is assumed ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "For practical grammar engineering one can devise the motto avoid accumulation of unbounded dependencies by whatever means ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "On the theoretical side we think that this result for SDL is also of some importance , since SDL exhibits a core of logical behaviour that any ( Lambek-based ) logic must have which accounts for non-peripheral extraction by some form of permutation ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "And hence , this result increases our understanding of the necessary computational properties of such richer systems ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "To our knowledge the question , whether the Lambek calculus itself or its associated parsing problem are NP-hard , are still open ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "In semidirectional Lambek calculus SDL there is an additional non-directional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "SDL grammars are able to generate each context-free language and more than that ."}
 {"title": "Parsing for Semidirectional Lambek Grammar is NP-Complete", "sentence": "We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "For example ,  has shown that in order to understand a scalar implicature , one must analyze the conversants ' beliefs and intentions ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "To recognize normal state implicatures one must consider mutual beliefs and plans  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "To understand conversational implicatures associated with indirect replies one must consider discourse expectations , discourse plans , and discourse relations ,  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Some presuppositions are inferrable when certain lexical constructs ( factives , aspectuals , etc ) or syntactic constructs ( cleft and pseudo-cleft sentences ) are used ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Despite all the complexities that individualize the recognition stage for each of these inferences , all of them can be defeated by context , by knowledge , beliefs , or plans of the agents that constitute part of the context , or by other pragmatic rules ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Defeasibility is a notion that is tricky to deal with , and scholars in logics and pragmatics have learned to circumvent it or live with it ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The first observers of the phenomenon preferred to keep defeasibility outside the mathematical world ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "For  ,  , and  `` everything exists '' ; therefore , in their logical systems , it is impossible to formalize the cancellation of the presupposition that definite referents exist ,  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We can taxonomize previous approaches to defeasible pragmatic inferences into three categories ( we omit here work on defeasibility related to linguistic phenomena such as discourse , anaphora , or speech acts ) ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Most linguistic approaches account for the defeasibility of pragmatic inferences by analyzing them in a context that consists of all or some of the previous utterances , including the current one ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Context  ,  , procedural rules  ,  , lexical and syntactic structure  , intentions  , or anaphoric constraints  ,  decide what presuppositions or implicatures are projected as pragmatic inferences for the utterance that is analyzed ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The problem with these approaches is that they assign a dual life to pragmatic inferences : in the initial stage , as members of a simple or complex utterance , they are defeasible ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "However , after that utterance is analyzed , there is no possibility left of cancelling that inference ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "But it is natural to have implicatures and presuppositions that are inferred and cancelled as a sequence of utterances proceeds : research in conversation repairs  abounds in such examples ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We address this issue in more detail in section  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "One way of accounting for cancellations that occur later in the analyzed text is simply to extend the boundaries within which pragmatic inferences are evaluated , i.e. , to look ahead a few utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "assumes that implicatures are connected to discourse entities and not to utterances , but her approach still does not allow cancellations across discourse units ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Another way of allowing pragmatic inferences to be cancelled is to assign them the status of defeasible information ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "formalizes presuppositions in a logical framework that handles defaults  , but this approach is not tractable and it treats natural disjunction as an exclusive-or and implication as logical equivalence ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Computational approaches fail to account for the cancellation of pragmatic inferences : once presuppositions  or implicatures  ,  are generated , they can never be cancelled ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We are not aware of any formalism or computational approach that offers a unified explanation for the cancellability of pragmatic inferences in general , and of no approach that handles cancellations that occur in sequences of utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "It is our aim to provide such an approach here ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "In doing this , we assume the existence , for each type of pragmatic inference , of a set of necessary conditions that must be true in order for that inference to be triggered ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Once such a set of conditions is met , the corresponding inference is drawn , but it is assigned a defeasible status ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "It is the role of context and knowledge of the conversants to `` decide '' whether that inference will survive or not as a pragmatic inference of the structure ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We put no boundaries upon the time when such a cancellation can occur , and we offer a unified explanation for pragmatic inferences that are inferable when simple utterances , complex utterances , or sequences of utterances are considered ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We propose a new formalism , called `` stratified logic '' , that correctly handles the pragmatic inferences , and we start by giving a very brief introduction to the main ideas that underlie it ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We give the main steps of the algorithm that is defined on the backbone of stratified logic ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We then show how different classes of pragmatic inferences can be captured using this formalism , and how our algorithm computes the expected results for a representative class of pragmatic inferences ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The results we report here are obtained using an implementation written in Common Lisp that uses Screamer  , a macro package that provides nondeterministic constructs ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We can offer here only a brief overview of stratified logic ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The reader is referred to  for a comprehensive study ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Stratified logic supports one type of indefeasible information and two types of defeasible information , namely , infelicitously defeasible and felicitously defeasible ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The notion of infelicitously defeasible information is meant to capture inferences that are anomalous to cancel , as in :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The notion of felicitously defeasible information is meant to capture the inferences that can be cancelled without any abnormality , as in :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The lattice in figure  underlies the semantics of stratified logic ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The lattice depicts the three levels of strength that seem to account for the inferences that pertain to natural language semantics and pragmatics : indefeasible information belongs to the u layer , infelicitously defeasible information belongs to the i layer , and felicitously defeasible information belongs to the d layer ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Each layer is partitioned according to its polarity in truth ,  , and falsity ,  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The lattice shows a partial order that is defined over the different levels of truth ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "For example , something that is indefeasibly false ,  , is stronger ( in a sense to be defined below ) than something that is infelicitously defeasibly true ,  , or felicitously defeasibly false ,  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Formally , we say that the u level is stronger than the i level , which is stronger than the d level :  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "At the syntactic level , we allow atomic formulas to be labelled according to the same underlying lattice ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Compound formulas are obtained in the usual way ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "This will give us formulas such as  , or  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The satisfaction relation is split according to the three levels of truth into u-satisfaction , i-satisfaction , and d-satisfaction :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Definition  extends in a natural way to negated and compound formulas ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Having a satisfaction definition associated with each level of strength provides a high degree of flexibility ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The same theory can be interpreted from a perspective that allows more freedom ( u-satisfaction ) , or from a perspective that is tighter and that signals when some defeasible information has been cancelled ( i - and d-satisfaction ) ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Possible interpretations of a given set of utterances with respect to a knowledge base are computed using an extension of the semantic tableau method ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "This extension has been proved to be both sound and complete  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "A partial ordering ,  , determines the set of optimistic interpretations for a theory ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "An interpretation  is preferred to , or is more optimistic than , an interpretation  (  ) if it contains more information and that information can be more easily updated in the future ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "That means that if an interpretation  makes an utterance true by assigning to a relation R a defeasible status , while another interpretation  makes the same utterance true by assigning the same relation R a stronger status ,  will be the preferred or optimistic one , because it is as informative as  and it allows more options in the future ( R can be defeated ) ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Pragmatic inferences are triggered by utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "To differentiate between them and semantic inferences , we introduce a new quantifier ,  , whose semantics is defined such that a pragmatic inference of the form  is instantiated only for those objects  from the universe of discourse that pertain to an utterance having the form  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Hence , only if the antecedent of a pragmatic rule has been uttered can that rule be applied ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "A meta-logical construct uttered applies to the logical translation of utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "This theory yields the following definition :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Our algorithm , described in detail by  , takes as input a set of first-order stratified formulas  that represents an adequate knowledge base that expresses semantic knowledge and the necessary conditions for triggering pragmatic inferences , and the translation of an utterance or set of utterances uttered ( u ) ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The algorithm builds the set of all possible interpretations for a given utterance , using a generalization of the semantic tableau technique ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The model-ordering relation filters the optimistic interpretations ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Among them , the defeasible inferences that have been triggered on pragmatic grounds are checked to see whether or not they are cancelled in any optimistic interpretation ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Those that are not cancelled are labelled as pragmatic inferences for the given utterance or set of utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We present a set of examples that covers a representative group of pragmatic inferences ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "In contrast with most other approaches , we provide a consistent methodology for computing these inferences and for determining whether they are cancelled or not for all possible configurations : simple and complex utterances and sequences of utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "A factive such as the verb regret presupposes its complement , but as we have seen , in positive environments , the presupposition is stronger : it is acceptable to defeat a presupposition triggered in a negative environment  , but is infelicitous to defeat one that belongs to a positive environment  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Therefore , an appropriate formalization of utterance  and the requisite pragmatic knowledge will be as shown in  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The stratified semantic tableau that corresponds to theory  is given in figure  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The tableau yields two model schemata ( see figure  ) ; in both of them , it is defeasibly inferred that Mary came to the party ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The model-ordering relation  establishes  as the optimistic model for the theory because it contains as much information as  and is easier to defeat ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Model  explains why Mary came to the party is a presupposition for utterance  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Consider utterance  , and its implicatures  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "An appropriate formalization is given in  , where the second formula captures the defeasible scalar implicatures and the third formula reflects the relevant semantic information for all ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The theory provides one optimistic model schema ( figure  ) that reflects the expected pragmatic inferences , i.e. , ( Not most / Not many / Not all ) of the boys went to the theatre ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Assume now , that after a moment of thought , the same person utters :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "By adding the extra utterance to the initial theory  , uttered ( went ( all ( boys ) , theatre ) ) , one would obtain one optimistic model schema in which the conventional implicatures have been cancelled ( see figure  ) ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The Achilles heel for most theories of presupposition has been their vulnerability to the projection problem ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Our solution for the projection problem does not differ from a solution for individual utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Consider the following utterances and some of their associated presuppositions  ( the symbol  precedes an inference drawn on pragmatic grounds ) :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Chris is not a bachelor presupposes that Chris is a male adult ; Chris regrets that Mary came to the party presupposes that Mary came to the party ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "There is no contradiction between these two presuppositions , so one would expect a conversant to infer both of them if she hears an utterance such as  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "However , when one examines utterance  , one observes immediately that there is a contradiction between the presuppositions carried by the individual components ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Being a bachelor presupposes that Chris is a male , while being a spinster presupposes that Chris is a female ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Normally , we would expect a conversant to notice this contradiction and to drop each of these elementary presuppositions when she interprets  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We now study how stratified logic and the model-ordering relation capture one 's intuitions ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "An appropriate formalization for utterance  and the necessary semantic and pragmatic knowledge is given in  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Besides the translation of the utterance , the initial theory contains a formalization of the defeasible implicature that natural disjunction is used as an exclusive or , the knowledge that Mary is not a name for males , the lexical semantics for the word bachelor , and the lexical pragmatics for bachelor and regret ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The stratified semantic tableau generates 12 model schemata ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Only four of them are kept as optimistic models for the utterance ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The models yield Mary came to the party ; Chris is a male ; and Chris is an adult as pragmatic inferences of utterance  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Consider now utterance  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The stratified semantic tableau that corresponds to its logical theory yields 16 models , but only Chris is an adult satisfies definition  and is projected as presupposition for the utterance ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "We have already mentioned that speech repairs constitute a good benchmark for studying the generation and cancellation of pragmatic inferences along sequences of utterances  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Suppose , for example , that Jane has two friends -- John Smith and John Pevler -- and that her roommate Mary has met only John Smith , a married fellow ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Assume now that Jane has a conversation with Mary in which Jane mentions only the name John because she is not aware that Mary does not know about the other John , who is a five-year-old boy ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "In this context , it is natural for Mary to become confused and to come to wrong conclusions ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "For example , Mary may reply that John is not a bachelor ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Although this is true for both Johns , it is more appropriate for the married fellow than for the five-year-old boy ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Mary knows that John Smith is a married male , so the utterance makes sense for her ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "At this point Jane realizes that Mary misunderstands her : all the time Jane was talking about John Pevler , the five-year-old boy ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The utterances in  constitute a possible answer that Jane may give to Mary in order to clarify the problem ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The first utterance in the sequence presupposes  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Utterance  warns Mary that is very likely she misunderstood a previous utterance  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The warning is conveyed by implicature ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "At this point , the hearer , Mary , starts to believe that one of her previous utterances has been elaborated on a false assumption , but she does not know which one ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The third utterance  comes to clarify the issue ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "It explicitly expresses that John is not an adult ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Therefore , it cancels the early presupposition  :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Note that there is a gap of one statement between the generation and the cancellation of this presupposition ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The behavior described is mirrored both by our theory and our program ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The same methodology can be applied to modeling conversational implicatures in indirect replies  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": ""}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The following dialog is considered  :"}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Answer  conveys a `` yes '' , but a reply consisting only of  would implicate a `` no '' ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "As  ,  , processing (  will block the implicature generated by  ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": ""}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Our approach does not exhibit these constraints ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "As in the previous example , the one dealing with a sequence of utterances , we obtain a different interpretation after each step ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "When the question is asked , there is no conversational implicature ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Answer  makes the necessary conditions for implicating `` no '' true , and the implication is computed ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Answer  reinforces a previous condition ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Answer  makes the preconditions for implicating a `` no '' false , and the preconditions for implicating a `` yes '' true ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Therefore , the implicature at the end of the dialogue is that the conversant who answered went shopping ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Unlike most research in pragmatics that focuses on certain types of presuppositions or implicatures , we provide a global framework in which one can express all these types of pragmatic inferences ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Each pragmatic inference is associated with a set of necessary conditions that may trigger that inference ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "When such a set of conditions is met , that inference is drawn , but it is assigned a defeasible status ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "An extended definition of satisfaction and a notion of `` optimism '' with respect to different interpretations yield the preferred interpretations for an utterance or sequences of utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "These interpretations contain the pragmatic inferences that have not been cancelled by context or conversant 's knowledge , plans , or intentions ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The formalism yields an algorithm that has been implemented in Common Lisp with Screamer ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "This algorithm computes uniformly pragmatic inferences that are associated with simple and complex utterances and sequences of utterances , and allows cancellations of pragmatic inferences to occur at any time in the discourse ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "This research was supported in part by a grant from the Natural Sciences and Engineering Research Council of Canada ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "Drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "This paper provides a theoretical framework , called stratified logic , that can accommodate defeasible pragmatic inferences ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The framework yields an algorithm that computes the conversational , conventional , scalar , clausal , and normal state implicatures ; and the presuppositions that are associated with utterances ."}
 {"title": "A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances", "sentence": "The algorithm applies equally to simple and complex utterances and sequences of utterances ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It seems a perfectly valid rule of conversation not to tell people what they already know ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Indeed ,  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Thus , the notion of what is informative is judged against a background of what is presupposed , i.e. propositions that all conversants assume are mutually known or believed ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "These propositions are known as the COMMON GROUND ,  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The various formulations of this ` no redundancy ' rule permeate many computational analyses of natural language and notions of cooperativity ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "However consider the following excerpt from the middle of an advisory dialogue between Harry ( h ) , a talk show host , and Ray ( r ) his caller ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "h. YUP THAT KNOCKS HER OUT ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In standard information theoretic terms , both  and  are REDUNDANT ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Harry 's assertion in  simply paraphrases what was said in  and  and so it cannot be adding beliefs to the common ground ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Furthermore , the truth of  cannot be in question , for instead of  , Harry could not say Yup , but that doesn't knock her out ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "So why does Ray ( r ) in  REPEAT Harry 's ( h ) assertion of it does , and why does Harry PARAPHRASE himself and Ray in  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "My claim is that informationally redundant utterances ( IRU 's ) have two main discourse functions :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "to provide EVIDENCE to support the assumptions underlying the inference of mutual beliefs ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "to CENTER a proposition , ie. make or keep a proposition salient  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This paper will focus on  leaving  for future work ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "First consider the notion of evidence ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "One reason why agents need EVIDENCE for beliefs is that they only have partial information about :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "the state of world ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "the effects of actions ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "other agent 's beliefs , preferences and goals ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This is especially true when it comes to modelling the effects of linguistic actions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Linguistic actions are different than physical actions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "An agent 's prior beliefs , preferences and goals cannot be ascertained by direct inspection ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This means that it is difficult for the speaker to verify when an action has achieved its expected result , and so giving and receiving evidence is critical and the process of establishing mutual beliefs is carefully monitored by the conversants ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The characterization of IRU 's as informationally redundant follows from an axiomatization of action in dialogue that I will call the DETERMINISTIC MODEL ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This model consists of a number of simplifying assumptions such as :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Propositions are are either believed or not believed ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Propositions representing beliefs and intentions get added to the context by the unilateral action of one conversant ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Agents are logically omniscient ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The context of a discourse is an undifferentiated set of propositions with no specific relations between them ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "I claim that these assumptions must be dropped in order to explain the function of IRU 's in dialogue ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Section  discusses assumption  ; section  shows how assumption  can be dropped ; section  discusses  ; section  shows that some IRU 's facilitate the inference of relations between adjacent propositions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The account proposed here of how the COMMON GROUND is augmented , is based is ,  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In this model , mutual beliefs depend on evidence , openly available to the conversants , plus a number of underlying assumptions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Shared Environment Mutual Belief Induction Schema"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It is mutually believed in a population P that  if and only if some situation  holds such that :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Everyone in P has reason to believe that  holds ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "indicates to everyone in P that everyone in P has reason to believe that  holds ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "indicates to everyone in P that  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The situation  , used above in the mutual belief induction schema , is the context of what has been said ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This schema supports a weak model of mutual beliefs , that is more akin to mutual assumptions or mutual suppositions  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Mutual beliefs can be inferred based on some evidence , but these beliefs may depend on underlying assumptions that are easily defeasible ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This model can be implemented using  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "A key part of this model is that some types of evidence provide better support for beliefs than other types ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The types of evidence considered are categorized and ordered based on the source of the evidence : hypothesis  <  default  <  inference  <  linguistic  <  physical  ,  ) ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This ordering reflects the relative defeasibility of different assumptions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Augmenting the strength of an assumption thus decreases its relative defeasibility ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "A claim of this paper is that one role of IRU 's is to ensure that these assumptions are supported by evidence , thus decreasing the defeasibility of the mutual beliefs that depend on them  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Thus mutual beliefs depend on a defeasible inference process ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "All inferences depend on the evidence to support them , and stronger evidence can defeat weaker evidence ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "So a mutual belief supported as an inference can get defeated by linguistic information ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In addition , I adopt an an assumption that a chain of reasoning is only as strong as its weakest link :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Weakest Link Assumption :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The strength of a belief P depending on a set of underlying assumptions  is MIN ( Strength (  ) ) ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This seems intuitively plausible and means that the strength of belief depends on the strength of underlying assumptions , and that for all inference rules that depend on multiple premises , the strength of an inferred belief is the weakest of the supporting beliefs ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This representation of mutual belief differs from the common representation in terms of an iterated conjunction  in that :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "it relocates information from mental states to the environment in which utterances occur ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "it allows one to represent the different kinds of evidence for mutual belief ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "it controls reasoning when discrepancies in mutual beliefs are discovered since evidence and assumptions can be inspected ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "it does not consist of an infinite list of statements ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This section examines the assumption from the DETERMINISTIC MODEL that :  Propositions representing beliefs and intentions get added to the context by the unilateral action of one conversant ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This assumption will also be examined in section  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The key claim of this section is that agents monitor the effects of their utterance actions and that the next action by the addressee is taken as evidence of the effect of the speaker 's utterance ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "That the utterance will have the intended effect is only a hypothesis at the point where the utterance has just been made , irrespective of the intentions of the speaker ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This distinguishes this account from others that assume either that utterance actions always succeed or that they succeed unless the addressee previously believed otherwise ,  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "I adopt the assumption that the participants in a dialogue are trying to achieve some purpose  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Some aspects of the structure of dialogue arises from the structure of these purposes and their relation to one another ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The minimal purpose of any dialogue is that an utterance be understood , and this goal is a prerequisite to achieving other goals in dialogue , such as commitment to future action ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Thus achieving mutual belief of understanding is an instance of the type of activity that agents must perform as they collaborate to achieve the purposes of the dialogue ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "I claim that a model of the achievement of mutual belief of understanding can be extended to the achievement of other goals in dialogue ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Achieving understanding is not unproblematic , it is a process that must be managed , just as other goal achieving processes are  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Inference of mutual understanding relies upon some evidence , e.g. the utterance that is made , and a number of underlying assumptions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The assumptions are given with the inference rule below ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This schema means that when A says u to B intending to convey p , that this leads to the mutual belief that B understands u as p under certain assumptions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The assumptions are that A and B were copresent , that B was attending to the utterance event , that B heard the utterance , and that B believes that the utterance u realizes the intended meaning p ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The [ evidence-type ] annotation indicates the strength of evidence supporting the assumption ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "All of the assumptions start out supported by no evidence ; their evidence type is therefore hypothesis ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It isn't until after the addressee 's next action that an assumption can have its strength modified ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The claim here is that one class of IRU 's addresses these assumptions underlying the inference of mutual understanding ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Each type of IRU , the assumption addressed and the evidence type provided is given in Figure  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Examples are provided in sections  and  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It is also possible that A intends that BY saying u , which realizes p , B should make a certain inference q. Then B 's understanding of u should include B making this inference ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This adds an additional assumption :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Thus assuming that q was inferred relies on the assumption that B believes that p licenses q in the context ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Figure  says that prompts , repetitions , paraphrases and making inferences explicit all provide linguistic evidence of attention ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "All that prompts such as uh huh do is provide evidence of attention ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "However repetitions , paraphrases and making inferences explicit also demonstrate complete hearing ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In addition , a paraphrase and making an inference explicit provides linguistic evidence of what proposition the paraphraser believes the previous utterance realizes ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Explicit inferences additionally provide evidence of what inferences the inferrer believes the realized proposition licenses in this context ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In each case , the IRU addresses one or more assumptions that have to be made in order to infer that mutual understanding has actually been achieved ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The assumption , rather than being a hypothesis or a default , get upgraded to a support type of linguistic as a result of the IRU ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The fact that different IRU 's address different assumptions leads to the perception that some IRU 's are better evidence for understanding than others , e.g. a PARAPHRASE is stronger evidence of understanding than a REPEAT  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In addition , any next utterance by the addressee can upgrade the strength of the underlying assumptions to default ( See Figure  ) ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Of course default evidence is weaker than linguistic evidence ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The basis for these default inferences will be discussed in section  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Consider example  in section  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Ray , in  , repeats Harry 's assertion from  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This upgrades the evidence for the assumptions of hearing and attention associated with utterance  from hypothesis to linguistic ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The assumption about what proposition p 7 is realized by u 7 remains a default ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This instantiates the inference rule for understanding as follows :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Because of the WEAKEST LINK assumption , the belief about understanding is still a default ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Consider the following excerpt :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Harry 's utterance of  is said with a falling intonational contour and hence is unlikely to be a question ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This utterance results in an instantiation of the inference rule as follows :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In this case , the belief about understanding is supported by linguistic evidence since all of the supporting assumptions are supported by linguistic evidence ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Thus a paraphrase provides excellent evidence that an agent actually understood what another agent meant ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In addition , these IRU 's leave a proposition salient , where otherwise the discourse might have moved on to other topics ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This is part of the CENTERING function of IRU 's and is left to future work ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This section discusses assumption  of the determistic model , namely that : Agents are logically omniscient ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This assumption is challenged by a number of cases in naturally occurring dialogues where inferences that follow from what has been said are made explicit ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "I restrict the inferences that I discuss to those that are"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "based on information explicitly provided in the dialogue or ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "licensed by applications of Gricean Maxims such as scalar implicature inferences  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "For example the logical omniscience assumption would mean that if  and  below are in the context , then  will be as well since it is entailed from  and  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "You can buy an I R A if and only if you do NOT have an existing pension plan ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "You have an existing pension plan ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "You cannot buy an I R A ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The following excerpt demonstrates this structure ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Utterance  realizes  , utterance  realizes  , and utterance  makes the inference explicit that is given in  for the particular tax year of 1981 ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "After  , since the propositional content of  is inferrable , the assumption that Harry has made this inference is supported by the inference evidence type :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "According to the model of achieving mutual understanding that was outlined in section  , utterance  provides linguistic evidence that Harry ( h ) believes that the proposition realized by utterance  licenses the inference of  in this context ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Furthermore , the context here consists of a discussion of two tax years 1981 and 1982 ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Utterance  selects eighty one , with a narrow focus pitch accent ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This implicates that there is some other tax year for which Joe is eligible , namely 1982  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Joe 's next utterance , but I am for 82 , reinforces the implicature that Harry makes in  , and upgrades the evidence underlying the assumption that  licenses  to linguistic ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "A subcase of ensuring that certain inferences get made involves the juxtaposition of two propositions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "These cases challenge the assumption that :  The context of a discourse is an undifferentiated set of propositions with no specific relations between them ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "While this assumption is certainly not made in most discourse models , it is often made in semantic models of the context  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In the following segment , Jane ( j ) describes her financial situation to Harry ( h ) and a choice between a settlement and an annuity ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Harry interrupts her at  since he believes he has enough information to suggest a course of action , and tells her take your money ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "To provide SUPPORT for this course of action he produces an inference that follows from what she has told him in  , namely You 're only getting 1500 ( dollars ) a year ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "SUPPORT is a general relation that holds between beliefs and intentions in this model ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Presumably Jane would have no trouble calculating that $125.45 a month for 12 months amounts to a little over $1500 a year , and thus can easily accept this statement that is intended to provide the necessary SUPPORT relation , ie. the juxtaposition of this fact against the advice to take the money conveys that the fact that she is only getting 1500 dollars a year is a reason for her to adopt the goal of taking the money , although this is not explicitly stated ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In section  , I examine the assumption that :  Propositions representing beliefs and intentions get added to the context by the unilateral action of one conversant ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "I suggested that this assumption can be replaced by adopting a model in which agents ' behavior provides evidence for whether or not mutual understanding has been achieved ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "I also discussed some of the effects of resource bounds , ie. cases of ensuring that or providing evidence that certain inferences dependent on what is said are made ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Achieving understanding and compensating for resource bounds are issues for a model of dialogue whether or not agents are autonomous ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "But agents ' autonomy means there are a number of other reasons why A 's utterance to B conveying a proposition p might not achieve its intended effect :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "p may not cohere with B 's beliefs ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "B may not think that p is relevant ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "B may believe that p does not contribute to the common goal ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "B may prefer doing or believing some q where p is mutually exclusive with q ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "If p is about an action , B may want to partially modify p with additional constraints about how , or when p ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Therefore it is important to distinguish an agent actually ACCEPTING the belief that p or intending to perform an action described by p from merely understanding that p was conveyed ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Other accounts legislate that helpful agents should adopt other 's beliefs and intentions or that acceptance depends on whether or not the agent previously believed  p ,  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "But agents can decide whether as well as how to revise their beliefs  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Evidence of acceptance may be given explicitly , but acceptance can be inferred in some dialogue situations via the operation of a simple principle of cooperative dialogue :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "COLLABORATIVE PRINCIPLE :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Conversants must provide evidence of a detected discrepancy in belief as soon as possible ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This principle claims that evidence of conflict should be made apparent in order to keep default inferences about acceptance or understanding from going through ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "IRU 's such as PROMPTS , REPETITIONS , PARAPHRASES , and making an INFERENCE explicit cannot function as evidence for conflicts in beliefs or intentions via their propositional content since they are informationally redundant ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "If they are realized with question intonation , the inference of acceptance is blocked ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In the dialogue below between Harry ( h ) and Ruth ( r ) , Ruth in  , first ensures that she understood Harry correctly , and then provides explicit evidence of non-acceptance in  , based on her autonomous preferences about how her money is invested ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In the following example , Joe in  makes a statement that provides propositional content that conflicts with Harry 's statement in  and thus provides evidence of non-acceptance ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Joe 's statement is based on his prior beliefs ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In both of these cases this evidence for conflict is given immediately ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "However when there is no evidence to the contrary , and goals of the discourse require achievement of acceptance , inferences about acceptance are licensed as default ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "They can be defeated later by stronger evidence ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Without this principle , a conversant might not bring up an objection until much later in the conversation , at which point the relevant belief and some inferences following from that belief will have been added to the common ground as defaults ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The result of this is that the retraction of that belief results in many beliefs being revised ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The operation of this principle helps conversants avoid replanning resulting from inconsistency in beliefs , and thus provides a way to manage the augmentation of the common ground efficiently ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The first point to note is that the examples here are only a subset of the types of IRU 's that occur in dialogues ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "I use the term antecedent to refer to the most recent utterance which should have added the proposition to the context ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This paper has mainly focused on cases where the IRU :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "is adjacent to its antecedent , rather than remote ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "realizes a proposition whose antecedent was said by another conversant ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "has only one antecedent ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It is with respect to this subset of the data that the alternate hypotheses are examined ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "A distributional analysis of a subset of the corpus ( 171 IRU 's from 24 dialogues consisting of 976 turns ) , on the relation of an IRU to its antecedent and the context , shows that 35 % of the tokens occur remotely from their antecedents , that 32 % have more than one antecedent , that 48 % consist of the speaker repeating something that he said before and 52 % consist of the speaker repeating something that the other conversant said ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "So the data that this paper focuses on accounts for about 30 % of the data ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In example  of section  , an alternative account of Ray 's repetition in  is that it is a question of some kind ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This raises a number of issues :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Why doesn't it have the form of a question ? ,"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "What is it a question about ? , and"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Why is it never denied ?"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Of 171 IRU 's , only 28 are realized with rising question intonation ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Of these 28 , 6 are actually redundant questions with question syntax , and 14 are followed by affirmations ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "If these are generally questions , then one possible answer to what the question is about is that Ray is questioning whether he actually heard properly ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "But then why doesn't he use an intonational contour that conveys this fact as Ruth does in example  ?"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "On an efficiency argument , it is hard to imagine that it would have cost Ray any more effort to have done so ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Finally , if it were a question it would seem that it should have more than one answer ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "While 50 of these IRU 's are followed by an affirmation such as that 's correct , right , yup , none of them are ever followed by a denial of their content ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It seems an odd question that only has one answer ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Another hypothesis is that IRU 's result from the radio talk show environment in which silence is not tolerated ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "So agents produce IRU 's because they cannot think of anything else to say but feel as though they must say something ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The first point to note is that IRU 's actually occur in dialogues that aren't on the radio  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The second question is why an agent would produce an IRU , rather than some other trivial statement such as I didn't know that ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Third , why don't these utterance correlate with typical stalling behavior such as false starts , pauses , and filled pauses such as uhhh ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The dead air hypothesis would seem to rely on an assumption that at unpredictable intervals , agents just can't think very well ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "My claim is that IRU 's are related to goals , that they support inferencing and address assumptions underlying mutual beliefs , ie. they are not random ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In order to prove this it must be possible to test the hypothesis that it is only important propositions that get repeated , paraphrased or made explicit ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This can be based on analyzing when the information that is repeated has been specifically requested , such as in the caller 's opening question or by a request for information from Harry ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It should also be possible to test whether the IRU realizes a proposition that plays a role in the final plan that Harry and the caller negotiate ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "However this type of strong evidence against the dead air hypothesis is left to future work ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It should be apparent from the account that the types of utterances examined here are not really redundant ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The reason that many models of belief transfer in dialogue would characterize them as redundant follows from a combination of facts :"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The representation of belief in these models has been binary ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The effects of utterance actions are either assumed to always hold , or to hold as defaults unless the listener already believed otherwise ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This means that these accounts cannot represent the fact that a belief must be supported by some kind of evidence and that the evidence may be stronger or weaker ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "It also follows from  that these models assume that agents are not autonomous , or at least do not have control over their own mental states ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "But belief revision is surely an autonomous process ; agents can choose whether to accept a new belief or revise old beliefs ,  ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The occurrence of IRU 's in dialogue has many ramifications for a model of dialogue ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Accounting for IRU 's has two direct effects on a dialogue model ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "First it requires a model of mutual beliefs that specifies how mutual beliefs are inferred and how some mutual beliefs can be as weak as mutual suppositions ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "One function of IRU 's is to address the assumptions on which mutual beliefs are based ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Second the assumption that propositions representing beliefs and intentions get added to the context by the unilateral action of one conversant must be dropped ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "This account replaces that assumption with a model in which the evidence of the hearer must be considered to establish mutual beliefs ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The claim here is that both understanding and acceptance are monitored ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The model outlined here can be used for different types of dialogue , including dialogues in which agents are constructing mutual beliefs to support future action by them jointly or alone ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "How and when agents decide to augment the strength of evidence for a belief has not been addressed in this work as yet ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Future work includes analyzing the corpus with respect to whether the IRU plays a role in the final plan that is negotiated between the conversants ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In dialogues in which both agents are autonomous , each agent deliberates whether to accept or reject the contributions of the current speaker ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "A speaker cannot simply assume that a proposal or an assertion will be accepted ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "However , an examination of a corpus of naturally-occurring problem-solving dialogues shows that agents often do not explicitly indicate acceptance or rejection ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "Rather the speaker must infer whether the hearer understands and accepts the current contribution based on indirect evidence provided by the hearer 's next dialogue contribution ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "In this paper , I propose a model of the role of informationally redundant utterances in providing evidence to support inferences about mutual understanding and acceptance ."}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "The model"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "requires a theory of mutual belief that supports mutual beliefs of various strengths ;"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "explains the function of a class of informationally redundant utterances that cannot be explained by other accounts ; and"}
 {"title": "Redundancy in Collaborative Dialogue", "sentence": "contributes to a theory of dialogue by showing how mutual beliefs can be inferred in the absence of the master-slave assumption ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Word vectors reflecting word meanings are expected to enable numerical approaches to semantics ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Some early attempts at vector representation in psycholinguistics were the semantic differential approach  and the associative distribution approach  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "However , they were derived manually through psychological experiments ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "An early attempt at automation was made by  using co-occurrence statistics ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Since then , there have been some promising results from using co-occurrence vectors , such as word sense disambiguation  , and word clustering  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "However , using the co-occurrence statistics requires a huge corpus that covers even most rare words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We recently developed word vectors that are derived from an ordinary dictionary by measuring the inter-word distances in the word definitions  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "This method , by its nature , has no problem handling rare words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "In this paper we examine the usefulness of these distance vectors as semantic representations by comparing them with co-occurrence vectors ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "A reference network of the words in a dictionary ( Fig.  ) is used to measure the distance between words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The network is a graph that shows which words are used in the definition of each word  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The network shown in Fig.  is for a very small portion of the reference network for the Collins English Dictionary ( 1979 edition ) in the CD-ROM I  , with 60K head words + 1.6M definition words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "For example , the definition for dictionary is `` a book in which the words of a language are listed alphabetically ... ''"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The word dictionary is thus linked to the words book , word , language , and alphabetical ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "A word vector is defined as the list of distances from a word to a certain set of selected words , which we call origins ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The words in Fig.  marked with  ( unit , book , and people ) are assumed to be origin words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "In principle , origin words can be freely chosen ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "In our experiments we used middle frequency words : the 51st to 1050th most frequent words in the reference Collins English Dictionary ( CED ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The distance vector for dictionary is derived as follows :"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The i-th element is the distance ( the length of the shortest path ) between dictionary and the i-th origin ,  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "To begin , we assume every link has a constant length of 1 ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The actual definition for link length will be given later ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "If word A is used in the definition of word B , these words are expected to be strongly related ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "This is the basis of our hypothesis that the distances in the reference network reflect the associative distances between words  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Use of Reference Networks"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Reference networks have been successfully used as neural networks ( by  for word sense disambiguation ) and as fields for artificial association , such as spreading activation ( by  for context-coherence measurement ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The distance vector of a word can be considered to be a list of the activation strengths at the origin nodes when the word node is activated ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Therefore , distance vectors can be expected to convey almost the same information as the entire network , and clearly they are much easier to handle ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Dependence on Dictionaries"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "As a semantic representation of words , distance vectors are expected to depend very weakly on the particular source dictionary ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We compared two sets of distance vectors , one from LDOCE  and the other from COBUILD  , and verified that their difference is at least smaller than the difference of the word definitions themselves  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We will now describe some technical details about the derivation of distance vectors ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Link Length"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Distance measurement in a reference network depends on the definition of link length ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Previously , we assumed for simplicity that every link has a constant length ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "However , this simple definition seems unnatural because it does not reflect word frequency ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Because a path through low-frequency words ( rare words ) implies a strong relation , it should be measured as a shorter path ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Therefore , we use the following definition of link length , which takes account of word frequency ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "This shows the length of the links between words W  in Fig.  , where N  denotes the total number of links from and to W  and n denotes the number of direct links between these two words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Normalization"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Distance vectors are normalized by first changing each coordinate into its deviation in the coordinate :"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "where  are the average and the standard deviation of the distances from the  - th origin ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Next , each coordinate is changed into its deviation in the vector :"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "where  are the average and the standard deviation of"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We use ordinary co-occurrence statistics and measure the co-occurrence likelihood between two words , X and Y , by the mutual information estimate  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "where  is the occurrence density of word X in a whole corpus , and the conditional probability  is the density of X in a neighborhood of word Y. Here the neighborhood is defined as 50 words before or after any appearance of word Y ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "( There is a variety of neighborhood definitions such as `` 100 surrounding words ''  and `` within a distance of no more than 3 words ignoring function words ''  . )"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The logarithm with ` + ' is defined to be 0 for an argument less than 1 ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Negative estimates were neglected because they are mostly accidental except when X and Y are frequent enough  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "A co-occurence vector of a word is defined as the list of co-occurrence likelihood of the word with a certain set of origin words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We used the same set of origin words as for the distance vectors ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "When the frequency of X or Y is zero , we can not measure their co-occurence likelihood , and such cases are not exceptional ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "This sparseness problem is well-known and serious in the co-occurrence statistics ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We used as a corpus the 1987 Wall Street Journal in the CD-ROM I  , which has a total of 20 M words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The number of words which appeared at least once was about 50 % of the total 62 K head words of CED , and the percentage of the word-origin pairs which appeared at least once was about 16 % of total 62 K  1 K ( = 62 M ) pairs ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "When the co-occurrence likelihood can not be measured , the value  was set to 0 ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We compared the two vector representations by using them for the following two semantic tasks ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The first is word sense disambiguation ( WSD ) based on the similarity of context vectors ; the second is the learning of or meanings from example words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "With WSD , the precision by using co-occurrence vectors from a 20 M words corpus was higher than by using distance vectors from the CED ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Word sense disambiguation is a serious semantic problem ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "A variety of approaches have been proposed for solving it ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "For example ,  used reference networks as neural networks ,  used ( shallow ) syntactic similarity between contexts ,  used simulated annealing for quick parallel disambiguation , and  used co-occurrence statistics between words and thesaurus categories ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Our disambiguation method is based on the similarity of context vectors , which was originated by  ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "In this method , a context vector is the sum of its constituent word vectors ( except the target word itself ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "That is , the context vector for context ,"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "is"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The similarity of contexts is measured by the angle of their vectors ( or actually the inner product of their normalized vectors ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Let word  , and each sense have the following context examples ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We infer that the sense of word  in an arbitrary context  is  , is maximum among all the context examples ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Another possible way to infer the sense is to choose sense  such that the average of  over  is maximum ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We selected the first method because a peculiarly similar example is more important than the average similarity ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Figure  ( next page ) shows the disambiguation precision for 9 words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "For each word , we selected two senses shown over each graph ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "These senses were chosen because they are clearly different and we could collect sufficient number ( more than 20 ) of context examples ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The names of senses were chosen from the category names in Roget 's International Thesaurus , except organ 's ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The results using distance vectors are shown by dots (    ) , and using co-occurrence vectors from the 1987 WSJ ( 20 M words ) by circles (    ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "A context size ( x-axis ) of , for example , 10 means 10 words before the target word and 10 words after the target word ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We used 20 examples per sense ; they were taken from the 1988 WSJ ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The test contexts were from the 1987 WSJ : The number of test contexts varies from word to word ( 100 to 1000 ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The precision is the simple average of the respective precisions for the two senses ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The results of Fig.  show that the precision by using co-occurrence vectors are higher than that by using distance vectors except two cases , interest and customs ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "And we have not yet found a case where the distance vectors give higher precision ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Therefore we conclude that co-occurrence vectors are advantageous over distance vectors to WSD based on the context similarity ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The sparseness problem for co-occurrence vectors is not serious in this case because each context consists of plural words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Another experiment using the same two vector representations was done to measure the learning of or meanings ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Figure  shows the changes in the precision ( the percentage of agreement with the authors ' combined judgement ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The x-axis indicates the number of example words for each or pair ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Judgement was again done by using the nearest example ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The example and test words are shown in Tables  and  , respectively ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "In this case , the distance vectors were advantageous ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The precision by using distance vectors increased to about 80 % and then leveled off , while the precision by using co-occurrence vectors stayed around 60 % ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "We can therefore conclude that the property of positive-or-negative is reflected in distance vectors more strongly than in co-occurrence vectors ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The sparseness problem is supposed to be a major factor in this case ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "In the experiments discussed above , the corpus size for co-occurrence vectors was set to 20 M words ( ' 87 WSJ ) and the vector dimension for both co-occurrence and distance vectors was set to 1000 ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Here we show some supplementary data that support these parameter settings"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Corpus size ( for co-occurrence vectors ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Figure  shows the change in disambiguation precision as the corpus size for co-occurrence statistics increases from 200 words to 20 M words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "( The words are suit , issue and race , the context size is 10 , and the number of examples per sense is 10 . ) These three graphs level off after around 1 M words ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Therefore , a corpus size of 20 M words is not too small ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Vector Dimension ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Figure  ( next page ) shows the dependence of disambiguation precision on the vector dimension for"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "co-occurrence and"}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "distance vectors ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "As for co-occurrence vectors , the precision levels off near a dimension of 100 ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "Therefore , a dimension size of 1000 is sufficient or even redundant ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "However , in the distance vector 's case , it is not clear whether the precision is leveling or still increasing around 1000 dimension ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "A comparison was made of co-occurrence vectors from large text corpora and of distance vectors from dictionary definitions ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "For the word sense disambiguation based on the context similarity , co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was advantageous over distance vectors from the Collins English Dictionary ( head words + definition words ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "For learning or meanings from example words , distance vectors gave remarkably higher precision than co-occurrence vectors ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "This suggests , though further investigation is required , that distance vectors contain some different semantic information from co-occurrence vectors ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "A comparison was made of vectors derived by using ordinary co-occurrence statistics from large text corpora and of vectors derived by measuring the inter-word distances in dictionary definitions ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "The precision of word sense disambiguation by using co-occurrence vectors from the 1987 Wall Street Journal ( 20 M total words ) was higher than that by using distance vectors from the Collins English Dictionary ( head words + definition words ) ."}
 {"title": "Occurrence Vectors from Corpora Vs. Distance Vectors from Dictionaries", "sentence": "However , other experimental results suggest that distance vectors contain some different semantic information from co-occurrence vectors ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The pattern matching capabilities of neural networks can be used to detect syntactic constituents of natural language ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This approach bears comparison with probabilistic systems , but has the advantage that negative as well as positive information can be modelled ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Also , most computation is done in advance , when the nets are trained , so the run time computational load is low ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In this work neural networks are used as part of a fully automated system that finds a partial parse of declarative sentences ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The connectionist processors operate within a grammatic framework , and are supported by pre-processors that filter the data and reduce the problem to a computationally tractable size ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A prototype can be accessed via the Internet , on which users can try their own text ( details from the authors ) ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It will take a sentence , locate the subject and then find the head of the subject ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Typically 10 sentences take about 2 seconds , 50 sentences about 4 seconds , to process on a Sparc10 workstation ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Using the prototype on technical manuals the subject and its head can be detected in over 90 % of cases ( See Section  ) ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The well known complexity of parsing is addressed by decomposing the problem , and then locating one syntactic constituent at a time ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The sentence is first decomposed into the broad syntactic categories"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "pre-subject - subject - predicate"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "by locating the subject"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Then these constituents can be processed further ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The underlying principle employed at each step is to take a sentence , or part of a sentence , and generate strings with the boundary markers of the syntactic constituent in question placed in all possible positions ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Then a neural net selects the string with the correct placement ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This paper gives an overview of how natural language is converted to a representation that the neural nets can handle , and how the problem is reduced to a manageable size ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It then outlines the neural net selection process ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A comprehensive account is given in  ; descriptions of the neural net process are also in  and  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This is a hybrid system ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The core process is data driven , as the parameters of the neural networks are derived from training text ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The neural net is trained in supervised mode on examples that have been manually marked `` correct '' and `` incorrect '' ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It will then be able to classify unseen examples ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , the initial processing stages , in which the problem size is constrained , operate within a skeletal grammatic framework ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Computational tractability is further addressed by reducing data through the application of prohibitive rules as local constraints ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The pruning process is remarkably effective ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This work has principally been developed on text of technical manuals from Perkins Engines Ltd. , which have been translated by a semi-automatic process  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Now , a partial parse can support such a process ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For instance , frequently occurring modal verbs such as `` must '' are not distinguished by number in English , but they are in many other languages ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It is necessary to locate the subject , then identify the head and determine its number in order to translate the main verb correctly in sentences like  below ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This parser has been trained to find the syntactic subject head that agrees in number with the main verb ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The manuals are written using the PACE ( Perkins Approved Clear English ) guidelines , with the aim of producing clear , unambiguous texts ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "All declarative sentences have been extracted for processing : about half were imperatives ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This level of classification can be done automatically in future ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Table  and Figure  show some of the characteristics of the corpus ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Punctuation marks are counted as words , formulae as 1 word ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In order to reconcile computational feasibility to empirical realism an appropriate form of language representation is critical ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The first step in constraining the problem size is to partition an unlimited vocabulary into a restricted number of part-of-speech tags ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Different stages of processing place different requirements on the classification system , so customised tagsets have been developed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For the first processing stage we need to place the subject markers , and , as a further task , disambiguate tags ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It was not found necessary to use number information at this stage ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For example , consider the sentence :"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The word `` waters '' could be a 3rd person , singular , present verb or a plural noun ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , in order to disambiguate the tag and place the subject markers it is only necessary to know that it is a noun or else a verb ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The sentence parsed at the first level returns :"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The tagset used at this stage , mode 1 , has 21 classes , not distinguished for number ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , the head of the subject is then found and number agreement with the verb can be assessed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "At this stage the tagset , mode 2 , includes number information and has 28 classes ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Devising optimal tagsets for given tasks is a field in which further work is planned ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We need larger tagsets to capture more linguistic information , but smaller ones to constrain the computational load ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Information theoretic tools can be used to find the entropy of different tag sequence languages , and support decisions on representation ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A functional approach is taken to tagging : words are allocated to classes depending on their syntactic role ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For instance , superlative adjectives can act as nouns , so they are initially given the 2 tags : noun or adjective ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This approach can be extended by taking adjacent words which act jointly as single lexical items as a unit ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Thus the pair `` most  <  adjective  >  '' is taken as a single superlative adjective ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Text is automatically tagged using the first modules of the CLAWS program ( 1985 version ) , in which words are allocated one or more tags from 134 classes  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "These 134 tags are then mapped onto the small customised tagsets ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Tag disambiguation is part of the parsing task , handled by the neural net and its pre-processor ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This version of CLAWS has a dictionary of about 6,300 words only ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Other words are tagged using suffix information , or else defaults are invoked ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The correct tag is almost always included in the set allocated , but more tags than necessary are often proposed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A larger dictionary in later versions will address this problem ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In the same way that tags are allocated to words , or to punctuation marks , they can represent the boundaries of syntactic constituents , such as noun phrases and verb phrases ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Boundary markers can be considered invisible tags , or hypertags , which have probabilistic relationships with adjacent tags in the same way that words do ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "and  have used this approach ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If embedded syntactic constituents are sought in a single pass , this can lead to computation\nal overload  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Our approach uses a similar concept , but differs in that embedded syntactic constituents are detected one at a time in separate steps ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "There are only 2 hypertags - the opening and closing brackets marking the possible location ( s ) of the syntactic constituent in question ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This system generates sets of tag strings for each sentence , with the hypertags placed in all possible positions ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Thus , for the subject detection task :"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "will generate strings of tags including :"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Hypertags are always inserted in pairs , so that closure is enforced ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "There were arbitrary limits of a maximum of 10 words in the pre-subject and 10 words within the subject for the initial work described here ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "These are now extended to 15 words in the pre-subject , 12 in the subject - see Section  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "There must be at least one word beyond the end of the subject and before the end-of-sentence mark ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Therefore , using the initial restrictions , in a sentence of 22 words or more ( counting punctuation marks as words ) there could be 100 alternative placements ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , some words will have more than one possible tag ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For instance , in sentence  above 5 words have 2 alternative tags , which will generate  possible strings before the hypertags are inserted ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Since there are 22 words ( including punctuation ) the total number of strings would be"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It is not feasible to detect one string out of this number : if the classifier marked all strings incorrect the percentage wrongly classified would only be  , yet it would be quite useless ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In order to find the correct string most of the outside candidates must be dropped ,"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A minimal grammar , set out in  in EBNF form , is composed of 9 rules ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For instance , the subject must contain a noun-type word ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Applying this particular rule to sentence  above would eliminate candidate strings  and  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We also have the 2 arbitrary limits on length of pre-subject and subject ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "There is a small set of 4 extensions to the grammar , or semi-local constraints ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For instance , if a relative pronoun occurs , then a verb must follow in that constituent ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "On the technical manuals the constraints of the grammatic framework put up to 6 % of declarative sentences outside our system , most commonly because the pre-subject is too long ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A small number are excluded because the system cannot handle a co-ordinated head ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "With the length of pre-subject extended to 15 words , and subject to 12 words , an average of 2 % are excluded ( 7 out of 351 ) ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The grammatic framework alone does not reduce the number of candidate strings sufficiently for the subject detection stage ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This problem is addressed further by a method suggested by  that local constraints can rein in the generation of an intractable number of possibilities ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In our system the local constraints are prohibited tag pairs and triples ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "These are adjacent tags which are not allowed , such as `` determiner - verb or '' `` start of subject - verb '' ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If during the generation of a candidate string a prohibited tuple is encountered , then the process is aborted ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "There are about 100 prohibited pairs and 120 triples ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "By using these methods the number of candidate strings is drastically reduced ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For the technical manuals an average of 4 strings , seldom more than 15 strings , are left ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Around 25 % of sentences are left with a single string ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "These filters or `` rules '' differ fundamentally from generative rules that produce allowable strings in a language ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In those cases only productions that are explicitly admitted are allowed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Here , in contrast , anything that is not expressly prohibited is allowed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "At this stage the data is ready to present to the neural net ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Figure  gives an overview of the whole process ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Different network architectures have been investigated , but they all share the same input and output representation ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The output from the net is a vector whose 2 elements , or nodes , represent `` correct '' and `` incorrect '' , `` yes '' and `` no '' - see Figure  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The input to the net is derived from the candidate strings , the sequences of tags and hypertags ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "These must be converted to binary vectors ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Each element of the vector will represent a feature that is flagged 0 or 1 , absent or present ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Though the form in which the vector is written may give an illusion of representing order , no sequential order is maintained ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A method of representing a sequence must be chosen ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The sequential order of the input is captured here , partially , by taking adjacent tags , pairs and triples , as the feature elements ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The individual tags are converted to a bipos and tripos representation ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Using this method each tag is in 3 tripos and 2 bipos elements ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This highly redundant code will aid the processing of sparse data typical of natural language ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For most of the work described here the sentence was dynamically truncated 2 words beyond the hypertag marking the close of the subject ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This process has now been improved by going further along the sentence ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The net that gave best results was a simple single layer net ( Figure  ) , derived from the Hodyne net of  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This is conventionally a `` single layer '' net , since there is one layer of processing nodes ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Multi-layer networks , which can process linearly inseparable data , were also investigated , but are not necessary for this particular processing task ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The linear separability of data is related to its order , and this system uses higher order pairs and triples as input ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The question of appropriate network architecture is examined in  ,  and  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The net is presented with training strings whose desired classification has been manually marked ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The weights on the connections between input and output nodes are adjusted until a required level of performance is reached ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Then the weights are fixed and the trained net is ready to classify unseen sentences ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The prototype accessible via the Internet has been trained on sentences from the technical manuals , slightly augmented ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Initially the weighted links are disabled ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "When a string is presented to the network in training mode , it activates a set of input nodes ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If an input node is not already linked to the output node representing the desired response , it will be connected and the weight on the connection will be initialised to 1.0 ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Most input nodes are connected to both outputs , since most tuples occur in both grammatical and ungrammatical strings ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , some will only be connected to one output - see Figure  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The input layer potentially has a node for each possible tuple ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "With 28 tags , 2 hypertags and a start symbol the upper bound on the number of input nodes is  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In practice the maximum activated is currently about 1000 ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In testing mode , if a previously unseen tuple appears it makes zero contribution to the result ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The activations at the input layer are fed forward through the weighted connections to the output nodes , where they are summed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The highest output marks the winning node ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If the desired node wins , then no action is taken ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If the desired node does not win , then the weight on connections to the desired node are incremented , while the weights on connections to the unwanted node are decremented ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This algorithm differs from some commonly used methods ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In feed forward networks trained in supervised mode to perform a classification task different penalty measures can be used to trigger a weight update ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Back propagation and some single layer training methods typically minimise a metric based on the least squared error ( LSE ) between desired and actual activation of the output nodes ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The reason why a differentiable error measure of this sort is necessary for multi-layer nets is well documented  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , for single layer nets we can choose to update weights directly : the error at an output node can trigger weight updates on the connections that feed it ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Solutions with LSE are not necessarily the same as minimising the number of misclassifications , and for certain types of data this second method of direct training may be appropriate ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Now , in the natural language domain it is desirable to get information from infrequent as well as common events ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Rare events , rather than being noise , can make a useful contribution to a classification task ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We need a method that captures information from infrequent events , and adopt a direct measure of misclassification ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This may be better suited to data with a `` Zipfian '' distribution  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The update factor is chosen to meet several requirements ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It should always be positive , and asymptotic to maximum and minimum bounds ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The factor should be greatest in the central region , least as it moves away in either direction ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We are currently still using the original Hodyne function because it works well in practice ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The update factor is given in the following formula ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If  for strengthening weights and  for weakening them , then"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Recall that weights are initialised to 1.0 ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "After training we find that the weight range is bounded by"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Total time for training is measured in seconds ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The number of iterative cycles that are necessary depends on the threshold chosen for the trained net to cross , and on details of the vector representation ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The demonstration prototype takes about 15 seconds ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "With the most recent improved representation about 1000 strings can be trained in 1 second , to 97 % ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The results from using these nets are given in Table  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It was found that triples alone gave as good results as pairs and triples together ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "And though the nets easily train to 99 % correct , the lower threshold gives slightly better generalisation and thus gives better results on the test data ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "When the trained net is run on unseen data the weights on the links are fixed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Any link that is still disabled is activated and initialised to 0 , so that tuples which have not occurred in the training corpus make no contribution to the classification task ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Sentences are put through the pre-processer one at a time and the candidate strings which are generated are then presented to the network ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The output is now interpreted differently ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The difference between the `` yes '' and `` no '' activation levels is recorded for each string , and this score is considered a measure of grammaticality ,  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The string with the highest  score is taken as the correct one ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For the results given below , the networks were trained on part of the corpus and tested on another part of the corpus ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For the prototype in which users can process their own text , the net was trained on the whole corpus , slightly augmented ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "There are several measures of correctness that can be taken when results are evaluated ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The most lenient is whether or not the subject and head markers are placed correctly - the type of measure used in the IBM / Lancaster work  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Since we are working towards a hierarchical language structure , we may want the words within constituents correctly tagged , ready for the next stage of processing ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "`` correct - A '' also requires that the words within the subject are correctly tagged ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The results in Tables  and  give an indication of performance levels ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "When parses are postulated for a sentence negative as well as positive examples are likely to occur ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Now , in natural language negative correlations are an important source of information : the occurrence of some words or groups of words inhibit others from following ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We wish to exploit these constraints ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "recognised this , and introduced the idea of distituents ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "These are elements of a sentence that should be separated , as opposed to elements of constituents that cling together ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": ""}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Distituency is marked by a mutual information minima ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "His method is supported by a small 4 rule grammar ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , this approach does not fully capture the sense in which inhibitory factors play a negative and not just a neutral role ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We want to distinguish between items that are unlikely to occur ever , and those that have just not happened to turn up in the training data ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "For example , in sentence [ CREF] above strings  ,  and [ CREF] can never be correct ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "These should be distinguished from possibly correct parses that are not in the training data ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In order that `` improbabilities '' can be modelled by inhibitory connections  show how a Hidden Markov Model can be implemented by a neural network ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The theoretical ground for incorporating negative examples in a language learning process originates with the work of  , developed by  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "He examined the process of learning the grammar of a formal language from examples ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "He showed that , for languages at least as high in the Chomsky hierarchy as CFGs , inference from positive data alone is strictly less powerful than inference from both positive and negative data together ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "To illustrate this informally consider a case of inference from a number of examples : as they are presented to the inference machine , possible grammars are postulated ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , with positive data alone a problem of over generalization arises : the postulated grammar may be a superset of the real grammar , and sentences that are outside the real grammar could be accepted ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If both positive and negative data is used , counter examples will reduce the postulated grammar so that it is nearer the real grammar ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": ""}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "A grammar may be inferred from positive examples alone for certain subsets of regular languages  , or an inference process may degenerate into a look up procedure if every possible positive example is stored ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In these cases negative information is not required , but they are not plausible models for unbounded natural language ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In our method the required parse is found by inferring the grammar from both positive and negative information , which is effectively modelled by the neural net ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Future work will investigate the effect of training the networks on the positive examples alone ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "With our current size corpus there is not enough data ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The relationship between the neural net and the rules in the prohibition table should be seen in the following way ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Any single rule prohibiting a tuple of adjacent tags could be omitted and the neural network would handle it by linking the node representing that tuple to `` no '' only ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , for some processing steps we need to reduce the number of candidate tag strings presented to the neural network to manageable proportions ( see Section  ) ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The data must be pre-processed by filtering through the prohibition rule constraints ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If the number of candidate strings is within desirable bounds , such as for the head detection task , no rules are used ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Our system is data driven as far as possible : the rules are invoked if they are needed to make the problem computationally tractable ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Our working prototype indicates that the methods described here are worth developing , and that connectionist methods can be used to generalise from the training corpus to unseen text ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Since data can be represented as higher order tuples , single layer networks can be used ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The traditional problems of training times do not arise ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We have also used multi-layer nets on this data : they have no advantages , and perform slightly less well  ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The supporting role of the grammatic framework and the prohibition filters should not be underestimated ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Whenever the scope of the system is extended it has been found necessary to enhance these elements ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The most laborious part of this work is preparing the training data ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Each time the representation is modified a new set of strings is generated that need marking up ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "An autodidactic check is now included which speeds up this task ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "We run marked up training data through an early version of the network trained on the same data , so the results should be almost all correct ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "If an `` incorrect '' parse occurs we can then check whether that sentence was properly marked up ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Some of the features of the system described here could be used in a stochastic process ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "However , connectionist methods have low computational loads at runtime ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Moreover , they can utilise more of the implicit information in the training data by modelling negative relationships ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This is a powerful concept that can be exploited in the effort to squeeze out every available piece of useful information for natural language processing ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "Future work is planned to extend this very limited partial parser , and decompose sentences further into their hierarchical constituent parts ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "In order to do this a number of subsidiary tasks will be addressed ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The system is being improved by identifying groups of words that act as single lexical items ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The decomposition of the problem can be investigated further : for instance , should the tag disambiguation task precede the placement of the subject boundary markers in a separate step ?"}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "More detailed investigation of language representation issues will be undertaken ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "And the critical issues of investigating the most appropriate network architectures will be carried on ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "This paper describes a fully automated hybrid system , using neural nets operating within a grammatic framework ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "It addresses the representation of language for connectionist processing , and describes methods of constraining the problem size ."}
 {"title": "A fast partial parse of natural language sentences using a connectionist method", "sentence": "The function of the network is briefly explained , and results are given ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Words unknown to the lexicon present a substantial problem to part-of-speech ( POS ) tagging of real-world texts ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Taggers assign a single POS - tag to a word-token , provided that it is known what parts-of-speech this word can take on in principle ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "So , first words are looked up in the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "However , 3 to 5 % of word tokens are usually missing in the lexicon when tagging real-world texts ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This is where word-POS guessers take their place -- they employ the analysis of word features , e.g. word leading and trailing characters , to figure out its possible POS categories ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "A set of rules which on the basis of ending characters of unknown words , assign them with sets of possible POS - tags is supplied with the Xerox tagger  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "A similar approach was taken in  where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS , its capitalisation feature and its ending ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In  a system of rules which uses both ending-guessing and more morphologically motivated rules is described ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The best of these methods are reported to achieve 82 - 85 % of tagging accuracy on unknown words ,  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The major topic in the development of word-POS guessers is the strategy which is to be used for the acquisition of the guessing rules ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "A rule-based tagger described in  is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In  a system for the automated learning of morphological word-formation rules is described ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This system divides a string into three regions and from training examples infers their correspondence to underlying morphological features ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "A statistical-based suffix learner is presented in  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "From a pre-tagged training corpus it constructs the suffix tree where every suffix is associated with its information measure ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Although the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art , they require specially prepared training data -- a pre-tagged training corpus , training examples , etc ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In this paper we describe a new fully automatic technique for learning part-of-speech guessing rules ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This technique does not require specially prepared training data and employs fully unsupervised statistical learning using the lexicon supplied with the tagger and word-frequencies obtained from a raw corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The learning is implemented as a two-staged process with feedback ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "First , setting certain parameters a set of guessing rules is acquired , then it is evaluated and the results of evaluation are used for re-acquisition of a better tuned rule-set ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "As was pointed out above , one of the requirements in many techniques for automatic learning of part-of-speech guessing rules is specially prepared training data -- a pre-tagged training corpus , training examples , etc ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In our approach we decided to reuse the data which come naturally with a tagger , viz. the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Another source of information which is used and which is not prepared specially for the task is a text corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Unlike other approaches we don't require the corpus to be pre-annotated but use it in its raw form ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In our experiments we used the lexicon and word-frequencies derived from the Brown Corpus  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "There are a number of reasons for choosing the Brown Corpus data for training ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The most important ones are that the Brown Corpus provides a model of general multi-domain language use , so general language regularities can be induced from it , and second , many taggers come with data trained on the Brown Corpus which is useful for comparison and evaluation ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This , however , by no means restricts the described technique to that or any other tag-set , lexicon or corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Moreover , despite the fact that the training is performed on a particular lexicon and a particular corpus , the obtained guessing rules suppose to be domain and corpus independent and the only training-dependent feature is the tag-set in use ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The acquisition of word-POS guessing rules is a three-step procedure which includes the rule extraction , rule scoring and rule merging phases ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "At the rule extraction phase , three sets of word-guessing rules ( morphological prefix guessing rules , morphological suffix guessing rules and ending-guessing rules ) are extracted from the lexicon and cleaned from coincidental cases ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "At the scoring phase , each rule is scored in accordance with its accuracy of guessing and the best scored rules are included into the final rule-sets ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "At the merging phase , rules which have not scored high enough to be included into the final rule-sets are merged into more general rules , then re-scored and depending on their score added to the final rule-sets ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Morphological word-guessing rules describe how one word can be guessed given that another word is known ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example , the rule :  says that prefixing the string `` un '' to a word , which can act as past form of verb ( VBD ) and participle ( VBN ) , produces an adjective ( JJ ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For instance , by applying this rule to the word `` undeveloped '' , we first segment the prefix `` un '' and if the remaining part `` developed '' is found in the lexicon as ( VBD VBN ) , we conclude that the word `` undeveloped '' is an adjective ( JJ ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The first POS - set in a guessing rule is called the initial class ( I-class ) and the POS - set of the guessed word is called the resulting class ( R-class ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In the example above ( VBD VBN ) is the I-class of the rule and ( JJ ) is the R-class ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In English , as in many other languages , morphological word formation is realised by affixation : prefixation and suffixation ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Although sometimes the affixation is not just a straightforward concatenation of the affix with the stem , the majority of cases clearly obey simple concatenative regularities ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "So , we decided first to concentrate only on simple concatenative cases ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "There are two kinds of morphological rules to be learned : suffix rules (  ) -- rules which are applied to the tail of a word , and prefix rules (  ) -- rules which are applied to the beginning of a word ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "says that if by stripping the suffix `` ed '' from an unknown word we produce a word with the POS - class ( NN VB ) , the unknown word is of the class - ( JJ VBD VBN ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This rule works , for instance , for [ book  booked ] , [ water  watered ] , etc ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To extract such rules a special operator  is applied to every pair of words from the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "It tries to segment an affix by leftmost string subtraction for suffixes and rightmost string subtraction for prefixes ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "If the subtraction results in an non-empty string it creates a morphological rule by storing the POS - class of the shorter word as the I-class and the POS - class of the longer word as the R-class ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The  operator is applied to all possible lexicon-entry pairs and if a rule produced by such an application has already been extracted from another pair , its frequency count ( f ) is incremented ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus two different sets of guessing rules -- prefix and suffix morphological rules together with their frequencies -- are produced ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Next , from these sets of guessing rules we need to cut out infrequent rules which might bias the further learning process ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To do that we eliminate all the rules with the frequency f less than a certain threshold  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Such filtering reduces the rule-sets more than tenfold and does not leave clearly coincidental cases among the rules ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Unlike morphological guessing rules , ending-guessing rules do not require the main form of an unknown word to be listed in the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "These rules guess a POS - class for a word just on the basis of its ending characters and without looking up its stem in the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Such rules are able to cover more unknown words than morphological guessing rules but their accuracy will not be as high ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example , an ending-guessing rule"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "says that if a word ends with `` ing '' it can be an adjective , a noun or a gerund ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Unlike a morphological rule , this rule does not ask to check whether the substring preceeding the `` ing '' - ending is a word with a particular POS - tag ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus an ending-guessing rule looks exactly like a morphological rule apart from the I-class which is always void ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To collect such rules we set the upper limit on the ending length equal to five characters and thus collect from the lexicon all possible word-endings of length 1 , 2 , 3 , 4 and 5 , together with the POS - classes of the words where these endings were detected to appear ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This is done by the operator  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example , from the word [ different ( JJ ) ] the  operator will produce five ending-guessing rules :  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The  operator is applied to each entry in the lexicon in the way described for the  operator of the morphological rules and then infrequent rules with  are filtered out ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Of course , not all acquired rules are equally good as plausible guesses about word-classes : some rules are more accurate in their guessings and some rules are more frequent in their application ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "So , for every acquired rule we need to estimate whether it is an effective rule which is worth retaining in the final rule-set ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For such estimation we perform a statistical experiment as follows : for every rule we calculate the number of times this rule was applied to a word token from a raw corpus and the number of times it gave the right answer ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Note that the task of the rule is not to disambiguate a word 's POS but to provide all and only possible POS s it can take on ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "If the rule is correct in the majority of times it was applied it is obviously a good rule ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "If the rule is wrong most of the times it is a bad rule which should not be included into the final rule-set ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To perform this experiment we take one-by-one each rule from the rule-sets produced at the rule extraction phase , take each word token from the corpus and guess its POS - set using the rule if the rule is applicable to the word ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example , if a guessing rule strips a particular suffix and a current word from the corpus does not have this suffix , we classify these word and rule as incompatible and the rule as not applicable to that word ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "If the rule is applicable to the word we perform look-up in the lexicon for this word and then compare the result of the guess with the information listed in the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "If the guessed POS - set is the same as the POS - set stated in the lexicon , we count it as success , otherwise it is failure ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The value of a guessing rule , thus , closely correlates with its estimated proportion of success (  ) which is the proportion of all positive outcomes ( x ) of the rule application to the total number of the trials ( n ) , which are , in fact , attempts to apply the rule to all the compatible words in the corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We also smooth  so as not to have zeros in positive or negative outcome probabilities :  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "estimate is a good indicator of rule accuracy ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "However , it frequently suffers from large estimation error due to insufficient training data ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example , if a rule was detected to work just twice and the total number of observations was also two , its estimate  is very high ( 1 , or 0.83 for the smoothed version ) but clearly this is not a very reliable estimate because of the tiny size of the sample ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Several smoothing methods have been proposed to reduce the estimation error ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For different reasons all these smoothing methods are not very suitable in our case ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In our approach we tackle this problem by calculating the lower confidence limit  for the rule estimate ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This can be seen as the minimal expected value of  for the rule if we were to draw a large number of samples ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus with certain confidence  we can assume that if we used more training data , the rule estimate  would be no worse than the  limit ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The lower confidence limit  is calculated as :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This function favours the rules with higher estimates obtained over larger samples ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Even if one rule has a high estimate but that estimate was obtained over a small sample , another rule with a lower estimate but over a large sample might be valued higher ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Note also that since  itself is smoothed we will not have zeros in positive (  ) or negative (  ) outcome probabilities ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This estimation of the rule value in fact resembles that used by  for scoring POS - disambiguation rules for the French tagger ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The main difference between the two functions is that there the z value was implicitly assumed to be 1 which corresponds to the confidence of 68 % ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "A more standard approach is to adopt a rather high confidence value in the range of 90 - 95 % ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We adopted 90 % confidence for which  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus we can calculate the score for the ith rule as :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Another important consideration for scoring a word-guessing rule is that the longer the affix or ending of the rule the more confident we are that it is not a coincidental one , even on small samples ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For example , if the estimate for the word-ending `` o '' was obtained over a sample of 5 words and the estimate for the word-ending `` fulness '' was also obtained over a sample of 5 words , the later case is more representative even though the sample size is the same ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus we need to adjust the estimation error in accordance with the length of the affix or ending ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "A good way to do that is to divide it by a value which increases along with the increase of the length ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "After several experiments we obtained :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "When the length of the affix or ending is 1 the estimation error is not changed since  is 0 ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For the rules with the affix or ending length of 2 the estimation error is reduced by  , for the length 3 this will be  , etc ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The longer the length the smaller the sample which will be considered representative enough for a confident rule estimation ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Setting the threshold  at a certain level lets only the rules whose score is higher than the threshold to be included into the final rule-sets ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The method for setting up this threshold is based on empirical evaluations of the rule-sets and is described in Section  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Rules which have scored lower than the threshold  can be merged into more general rules which if scored above the threshold are also included into the final rule-sets ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We can merge two rules which have scored below the threshold and have the same affix ( or ending ) and the initial class ( I ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The score of the resulting rule will be higher than the scores of the merged rules since the number of positive observations increases and the number of the trials remains the same ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "After a successful application of the merging , the resulting rule substitutes the two merged ones ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To perform such rule-merging over a rule-set , first , the rules which have not been included into the final set are sorted by their score and best-scored rules are merged first ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This is done recursively until the score of the resulting rule does not exceed the threshold in which case it is added to the final rule-set ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This process is applied until no merges can be done to the rules which have scored below the threshold ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "There are two important questions which arise at the rule acquisition stage - how to choose the scoring threshold  and what is the performance of the rule-sets produced with different thresholds ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The task of assigning a set of POS - tags to a word is actually quite similar to the task of document categorisation where a document should be assigned with a set of descriptors which represent its contents ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The performance of such assignment can be measured in :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "recall - the percentage of POS s which were assigned correctly by the guesser to a word ; ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "precision - the percentage of POS s the guesser assigned correctly over the total number of POS s it assigned to the word ; ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "coverage - the proportion of words which the guesser was able to classify , but not necessarily correctly ; ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In our experiments we measured word precision and word recall ( micro-average ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "There were two types of data in use at this stage ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "First , we evaluated the guessing rules against the actual lexicon : every word from the lexicon , except for closed-class words and words shorter than five characters , was guessed by the different guessing strategies and the results were compared with the information the word had in the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In the other evaluation experiment we measured the performance of the guessing rules against the training corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For every word we computed its metrics exactly as in the previous experiment ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Then we multiplied these results by the corpus frequency of this particular word and averaged them ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus the most frequent words had the greatest influence on the aggreagte measures ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "First , we concentrated on finding the best thresholds  for the rule-sets ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To do that for each rule-set produced using different thresholds we recorded the three metrics and chose the set with the best aggregate ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In Table  some results of that experiment are shown ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The best thresholds were detected : for ending rules - 75 points , for suffix rules - 60 , and for prefix rules - 80 ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "One can notice a slight difference in the results obtained over the lexicon and the corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The corpus results are better because the training technique explicitly targeted the rule-sets to the most frequent cases of the corpus rather than the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In average ending-guessing rules were detected to cover over 96 % of the unknown words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The precision of 74 % roughly can be interpreted as that for words which take on three different POS s in their POS - class , the ending-guessing rules will assign four , but in 95 % of the times ( recall ) the three required POS s will be among the four assigned by the guess ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In comparison with the Xerox word-ending guesser taken as the base-line model we detect a substantial increase in the precision by about 22 % and a cheerful increase in coverage by about 6 % ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This means that the Xerox guesser creates more ambiguity for the disambiguator , assigning five instead of three POS s in the example above ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "It can also handle 6 % less unknown words which , in fact , might decrease its performance even lower ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In comparison with the ending-guessing rules , the morphological rules have much better precision and hence better accuracy of guessing ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Virtually almost every word which can be guessed by the morphological rules is guessed exactly correct ( 97 % recall and 97 % precision ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Not surprisingly , the coverage of morphological rules is much lower than that of the ending-guessing ones - for the suffix rules it is less than 40 % and for the prefix rules about 5 - 6 ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "After obtaining the optimal rule-sets we performed the same experiment on a word-sample which was not included into the training lexicon and corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We gathered about three thousand words from the lexicon developed for the Wall Street Journal corpus and collected frequencies of these words in this corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "At this experiment we obtained similar metrics apart from the coverage which dropped about 0.5 % for Ending 75 and Xerox rule-sets and 7 % for the Suffix 60 rule-set ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This , actually , did not come as a surprise , since many main forms required by the suffix rules were missing in the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In the next experiment we evaluated whether the morphological rules add any improvement if they are used in conjunction with the ending-guessing rules ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We also evaluated in detail whether a conjunctive application with the Xerox guesser would boost the performance ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "As in the previous experiment we measured the precision , recall and coverage both on the lexicon and on the corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Table  demonstrates some results of this experiment ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The first part of the table shows that when the Xerox guesser is applied before the E  guesser we measure a drop in the performance ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "When the Xerox guesser is applied after the E  guesser no sufficient changes to the performance are noticed ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This actually proves that the E  rule-set fully supercedes the Xerox rule-set ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The second part of the table shows that the cascading application of the morphological rule-sets together with the ending-guessing rules increases the overall precision of the guessing by a further 5 % ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This makes the improvements against the base-line Xerox guesser 28 % in precision and 7 % in coverage ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The direct evaluation of the rule-sets gave us the grounds for the comparison and selection of the best performing guessing rule-sets ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The task of unknown word guessing is , however , a subtask of the overall part-of-speech tagging process ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus we are mostly interested in how the advantage of one rule-set over another will affect the tagging performance ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "So , we performed an independent evaluation of the impact of the word guessers on tagging accuracy ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In this evaluation we tried two different taggers ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "First , we used a tagger which was a C++ re-implementation of the LISP implemented HMM Xerox tagger described in  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The other tagger was the rule-based tagger of  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Both of the taggers come with data and word-guessing components pre-trained on the Brown Corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This , actually gave us the search-space of four combinations : the Xerox tagger equipped with the original Xerox guesser ,"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For words which failed to be guessed by the guessing rules we applied the standard method of classifying them as common nouns ( NN ) if they are not capitalised inside a sentence and proper nouns ( NP ) otherwise ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "As the base-line result we measured the performance of the taggers with all known words on the same word sample ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In the evaluation of tagging accuracy on unknown words we pay attention to two metrics ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "First we measure the accuracy of tagging solely on unknown words :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This metric gives us the exact measure of how the tagger has done on unknown words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In this case , however , we do not account for the known words which were mis-tagged because of the guessers ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To put a perspective on that aspect we measure the overall tagging performance :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Since the Brown Corpus model is a general language model , it , in principle , does not put restrictions on the type of text it can be used for , although its performance might be slightly lower than that of a model specialised for this particular sublanguage ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Here we want to stress that our primary task was not to evaluate the taggers themselves but rather their performance with the word-guessing modules ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "So we did not worry too much about tuning the taggers for the texts and used the Brown Corpus model instead ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We tagged several texts of different origins , except from the Brown Corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "These texts were not seen at the training phase which means that neither the taggers nor the guessers had been trained on these texts and they naturally had words unknown to the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "For each text we performed two tagging experiments ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In the first experiment we tagged the text with the Brown Corpus lexicon supplied with the taggers and hence had only those unknown words which naturally occur in this text ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In the second experiment we tagged the same text with the lexicon which contained only closed-class and short words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This small lexicon contained only 5,456 entries out of 53,015 entries of the original Brown Corpus lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "All other words were considered as unknown and had to be guessed by the guessers ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We obtained quite stable results in these experiments ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Here is a typical example of tagging a text of 5970 words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This text was detected to have 347 unknown words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "First , we tagged the text by the four different combinations of the taggers with the word-guessers using the full-fledged lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The results of this tagging are summarised in Table  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "When using the Xerox tagger with its original guesser , 63 unknown words were incorrectly tagged and the accuracy on the unknown words was measured at 81.8 % ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "When the Xerox tagger was equipped with our cascading guesser its accuracy on unknown words increased by almost 9 % upto 90.5 % ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The same situation was detected with"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The cascading guesser performed better than"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The accuracy of the taggers on the set of 347 unknown words when they were made known to the lexicon was detected at 98.5 % for both taggers ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In the second experiment we tagged the same text in the same way but with the small lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Out of 5,970 words of the text , 2,215 were unknown to the small lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The results of this tagging are summarised in Table  ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The accuracy of the taggers on the 2,215 unknown words when they were made known to the lexicon was much lower than in the previous experiment -- 90.3 % for the Xerox tagger and 91.5 % for"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Naturally , the performance of the guessers was also lower than in the previous experiment plus the fact that many `` semi-closed '' class adverbs like `` however '' , `` instead '' , etc ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": ", were missing in the small lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The accuracy of the tagging on unknown words dropped by about 5 % in general ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The best results on unknown words were again obtained on the cascading guesser ( 86 % - 87.45 % ) and"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Two types of mis-taggings caused by the guessers occured ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The first type is when guessers provided broader POS - classes for unknown words and the tagger had difficulties with the disambiguation of such broader classes ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This is especially the case with the `` ing '' words which , in general , can act as nouns , adjectives and gerunds and only direct lexicalization can restrict the search space , as in the case with the word `` going '' which cannot be an adjective but only a noun and a gerund ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The second type of mis-tagging was caused by wrong assignments of POS s by the guesser ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Usually this is the case with irregular words like , for example , `` cattle '' which was wrongly guessed as a singular noun ( NN ) but in fact is a plural noun ( NNS ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We presented a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for words unknown to the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This technique does not require specially prepared training data and uses for training the lexicon and word frequencies collected from a raw corpus ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Using these training data three types of guessing rules are learned : prefix morphological rules , suffix morphological rules and ending-guessing rules ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "To select best performing guessing rule-sets we suggested an evaluation methodology , which is solely dedicated to the performance of part-of-speech guessers ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Evaluation of tagging accuracy on unknown words using texts unseen by the guessers and the taggers at the training phase showed that tagging with the automatically induced cascading guesser was consistently more accurate than previously quoted results known to the author ( 85 % ) ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The cascading guesser outperformed the guesser supplied with the Xerox tagger by about 8 - 9 % and the guesser supplied with"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Tagging accuracy on unknown words using the cascading guesser was detected at 90 - 92 % when tagging with the full-fledged lexicon and 86 - 88 % when tagging with the closed-class and short word lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "When the unknown words were made known to the lexicon the accuracy of tagging was detected at 96 - 98 % and 90 - 92 % respectively ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This makes the accuracy drop caused by the cascading guesser to be less than 6 % in general ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Another important conclusion from the evaluation experiments is that the morphological guessing rules do improve the guessing performance ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Since they are more accurate than ending-guessing rules they are applied before ending-guessing rules and improve the precision of the guessings by about 5 % ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This , actually , results in about 2 % higher accuracy of tagging on unknown words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The acquired guessing rules employed in our cascading guesser are , in fact , of a standard nature and in that form or another are used in other POS - guessers ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "There are , however , a few points which make the rule-sets acquired by the presented here technique more accurate :"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "the learning of such rules is done from the lexicon rather than tagged corpus , because the guesser 's task is akin to the lexicon lookup ;"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "there is a well-tuned statistical scoring procedure which accounts for rule features and frequency distribution ;"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "there is an empirical way to determine an optimum collection of rules , since acquired rules are subject to rigorous direct evaluation in terms of precision , recall and coverage ;"}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "rules are applied cascadingly using the most accurate rules first ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "One of the most important issues in the induction of guessing rule-sets is the choice right data for training ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In our approach , guessing rules are extracted from the lexicon and the actual corpus frequencies of word-usage then allow for discrimination between rules which are no longer productive ( but have left their imprint on the basic lexicon ) and rules that are productive in real-life texts ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Thus the major factor in the learning process is the lexicon ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Since guessing rules are meant to capture general language regularities the lexicon should be as general as possible ( list all possible POS s for a word ) and as large as possible ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The corresponding corpus should include most of the words from the lexicon and be large enough to obtain reliable estimates of word-frequency distribution ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Our experiments with the lexicon and word frequencies derived from the Brown Corpus , which can be considered as a general model of English , resulted in guessing rule-sets which proved to be domain and corpus independent , producing similar results on test texts of different origin ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Although in general the performance of the cascading guesser is only 6 % worse than the lookup of a general language lexicon there is room for improvement ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "First , in the extraction of the morphological rules we did not attempt to model non-concatenative cases ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In English , however , since most of letter mutations occur in the last letter of the main word it is possible to account for it ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "So our next goal is to extract morphological rules with one letter mutations at the end ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This would account for cases like `` try - tries '' , `` reduce - reducing '' , `` advise - advisable '' ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "We expect it to increase the coverage of thesuffix morphological rules and hence contribute to the overall guessing accuracy ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Another avenue for improvement is to provide the guessing rules with the probabilities of emission of POS s from their resulting POS - classes ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "This information can be compiled automatically and also might improve the accuracy of tagging unknown words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The described rule acquisition and evaluation methods are implemented as a modular set of C ++ and AWK tools , and the guesser is easily extendable to sub-language specific regularities and retrainable to new tag-sets and other languages , provided that these languages have affixational morphology ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Both the software and the produced guessing rule-sets are available by contacting the author ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Words unknown to the lexicon present a substantial problem to part-of-speech tagging ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible parts-of-speech for unknown words ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules ."}
 {"title": "Unsupervised Learning of Word-Category Guessing Rules", "sentence": "The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Tagging by means of a Hidden Markov Model ( HMM ) is widely recognised as an effective technique for assigning parts of speech to a corpus in a robust and efficient manner ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "An attractive feature of the technique is that the algorithm itself is independent of the ( natural ) language to which it is applied ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "All of the `` knowledge engineering '' is localised in the choice of tagset and the method of training ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Typically , training makes use of a manually tagged corpus , or an untagged corpus with some initial bootstrapping probabilities ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Some attention has been given to how to make such techniques effective ; for example  suggest ways of training trigram taggers , and  and  consider the amount and quality of the seeding data needed to construct an accurate tagger ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In training a tagger for a given language , a major part of the knowledge engineering required can therefore be localised in the choice of the tagset ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The design of an appropriate tagset is subject to both external and internal criteria ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The external criterion is that the tagset must be capable of making the linguistic ( for example , syntactic or morphological ) distinctions required in the output corpora ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Tagsets used in the past have included varying amounts of detail ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For example , the Penn treebank tagset  omits a number of the distinctions which are made in the LOB and Brown tagsets on which it is based  ,  in cases where the surface form of the words allows the distinctions to be recovered if they are needed ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Thus , the auxiliary verbs be , do and have have the same tags as other verbs in Penn , but are each separated out in the LOB tagset ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "A second design criterion on tagsets is the internal one of making the tagging as effective as possible ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "As an example , one of the most common errors made by taggers with the LOB and Brown tagsets is mistagging a word as a subordinating conjunction ( CS ) rather than a preposition ( IN ) , or vice-versa  ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "A higher level of syntactic analysis indicating the phrasal structure would be required to predict which tag is correct , and this information is not available to fixed-context taggers ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The Penn treebank therefore uses a single tag for both cases , leaving the resolution - if required - to some other process ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Similarly , most tagsets do not distinguish transitive and intransitive verbs , since taggers which use a context of only two or three words will generally not be able to make the right predictions ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Distinctions of this sort are usually found only in corpora such as Susanne which are parsed as well as tagged ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The problem of tagset design becomes particularly important for highly inflected languages , such as Greek or Hungarian ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "If all of the syntactic variations which are realised in the inflectional system were represented in the tagset , there would be a huge number of tags , and it would be practically impossible to implement or train a simple tagger ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Note in passing that this may not as serious a problem as it first appears ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "If the language is very highly inflected , it may be be possible to do all ( or a large part ) of the work of a tagger with a word-by-word morphological analysis instead ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Nevertheless , there are many languages which have enough ambiguity that tagging is useful , but a rich enough tagset that the criteria on which it is designed must be given careful consideration ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In this paper , I report two experiments which address the internal design criterion , by looking at how tagging accuracy varies as the tagset is modified , in English , French and Swedish ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Although the choice of language was dictated by the corpora which were available , they represent three different degrees of complexity in their inflectional systems ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "English has a very limited system , marking little more than plurality on nouns and a restricted range of verb properties ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "French has a little more complexity , with gender , number and person marked , while Swedish has more detailed marking for gender , number , definiteness and case ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "As a subsidiary issue , we will also look at how the tagger performs on unknown words , i.e. ones not seen in the training data ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The usual approach here is to hypothesise all tags in the tagset for an unknown word , other than ones where all the words that may have the tag can be enumerated in advance ( closed class tags ) ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "HMM taggers often perform poorly on unknown words ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Alternative tagsets were derived by taking the initial tagset for each corpus ( from manual tagging of the corpus ) and condensing sets of tags which represent a grammatical distinction such as gender into single tags ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The changes were then applied to the training corpus ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "This allows us to effectively produce a corpus tagged according to a different scheme without having to manually re-tag the corpus ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The changes in the tagsets were motivated purely by grammatical considerations , and did not take the errors actually observed into account ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In general what we will look at in the results is how the tagging accuracy changes as the size of the tagset changes ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "This is a deliberately naive approach , and it is adopted with the goal of continuing in the relatively `` knowledge-free '' tradition of work in HMM tagging ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The aim of the experiment is to determine , crudely , whether a bigger tagset is better than a smaller one , or whether external criteria requiring human intervention should be used to choose the best tagset ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The results for the three languages turn out to be quite different , and the general conclusion ( which is the overall contribution of the paper ) will be that the external criterion should be the one to dominate tagset design : there is a limit to how knowledge-free we can be ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "As a preliminary to this work , note that it is hard to reason about the effect of changing the tagset ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "It can be argued that a smaller tagset should improve tagging accuracy , since it puts less of a burden on the tagger to make fine distinctions ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In information-theoretic terms , the number of decisions required is smaller , and hence the tagger need contribute less information to make the decisions ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "A smaller tagset may also mean that more words have only one possible tag and so can be handled trivially ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Conversely , more detail in the tagset may help the tagger when the properties of two adjacent words give support to the choice of tag for both of them ; that is , the transitions between tags contribute the information the tagger needs ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For example , if determiners and nouns are marked for number , then the tagger can effectively model agreement in simple noun phrases , by having a higher probability for a singular determiner followed by a singular noun that it does for a singular determiner followed by a plural noun ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Theory on its own does not help much in deciding which point of view should dominate ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Two experiments were conducted on three corpora : 300 k words of Swedish text from the ECI Multilingual CD-ROM , and 100 k words each of English and French from a corpus of International Telecommunications Union text ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In the first experiment the whole of each corpus was used to train the model , and a small sample from the same text was used as test data ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For the second experiment , 95 % of the corpus was used in training and the remainder in testing ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The importance of the second test is that it includes unknown words , which are difficult to tag ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The tagsets were progressively modified , by textually substituting simplified tags for the original ones and e e-running the training and test procedures using the modified corpora ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The changes to the tagset are listed below ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In the results that follow , we will identify tagset that include a given distinction with an uppercase letter and ones that do not with a lowercase letter ; for example G for a tagset that marks gender , and g for one that does not ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Swedish"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The changes made were entirely based on inflections ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "G Gender : masculine , neuter , common gender ( `` UTR '' in the tagset ) ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "N Number : singular , plural ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "D Definiteness : definite , indefinite ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "C Case : nominative , genitive ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "French"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The changes other than V were based on inflections ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "G Gender : masculine , feminine ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "N Number : singular , plural ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "P Person : identified as 1 st to 6th in the tagset ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "V Verbs : treat avoir and etre as being the same as any other verb ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "English"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The changes here are more varied than for the other languages , and generally consisted of removing some of the finer subdivisions of the major classes ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The grouping of some of these changes is admittedly a little ad hoc , and was intended to give a good distribution of tagset sizes ; not all combinations were tried ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "C Reduce specific conjunction classes to a common class , and simplify one adjective class ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "A Simplify noun and adverb classes ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "P Simplify pronoun classes ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "N Number : all singular / plural distinctions removed ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "V Use the same class for have , do and be as for other verbs ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The sizes of the resulting tagsets and the degree of ambiguity in the corpora which resulted appear below ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Accuracy figures quoted here are for ambiguous and unknown words only , and therefore factor out effects due to the varying degree of ambiguity as the tagset changes ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In fact , this is a rather approximate way of accounting for ambiguity , since it does not take the length of ambiguous sequences into account , and the accuracy is likely to deteriorate more on long sequences of ambiguous words than on short ones ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The tests were run using Good-Turing correction to the probability estimates ; that is , rather than estimating the probability of the transition from a tag i to a tag j as the count of transition from i to j in the training corpus divided by the total frequency of tag i , one was added to the count of all transitions , and the total tag frequencies adjusted correspondingly ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The purpose in using this correction is to correct for corpora which might not provide enough training data ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "On the largest tagsets , the correction was found to give a very slight reduction in the accuracy for Swedish , and to improve the French and English accuracies by about 1.5 % , suggesting that it is indeed needed ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The first experiment , with no unknown words , gave accuracies on ambiguous words of 91 - 93 % for Swedish , 94 - 97 % for French and 85 - 90 % for English ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The results for English are surprisingly low ( for example , on the Penn treebank , the tagger gives an accuracy of 95 - 96 % ) , and may be due to long sequences of ambiguous words ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The results appear in table  ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The figures include the degree of ambiguity , that is , the number of words in the corpus for which more than one tag was hypothesised ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The accuracy is plotted against the size of the tagset in figures  -  , where the numbers on the points correspond to the index of tagsets listed ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Summarising the patterns :"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Swedish"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Larger tagset generally gives higher accuracy ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The results are quite widely spread ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "French"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Clustered , with an accuracy on all tagsets which do not mark gender of around 96 % - 96.5 % ; when gender is marked 94 % - 94.5 % ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "English"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Larger tagset tends to give larger accuracy , though with less of a spread than for Swedish ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The sizes of the tagsets ranged from approximately 80 - 200 tags for Swedish , 35 - 90 for French , and 70 - 160 for English ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "As discussed above , it is not clear what would happen with larger tagsets , but some experiments based on the Susanne corpus and using tagsets ranging from 236 to 425 tags suggest that the trend to higher accuracy continues with even bigger tagsets ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In the second experiment , the test corpora included `` unknown '' words , which had not been seen during training , and for which the tagger hypothesises all open-class tags ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Two results are interesting to look at here : the accuracy on the unknown words , and the accuracy on words which were ambiguous but were found in the training corpus ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The results , in outline , are :"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Swedish"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Similar results on known words to first experiment ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For unknown words , smaller tagsets give higher accuracy ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "French"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For ambiguous words , the pattern and accuracy were similar to first experiment ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For unknown words , the pattern of accuracies was again similar , with tagsets that do not include gender giving accuracies of 51 % - 52 % , and those which do giving 45 % - 46 % ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "English"}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Ambiguous words gave similar results to the first test ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Unknown words show a weak tendency to give higher accuracy on smaller tagsets ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Typical accuracies on ambiguous words were 90 - 92 % , 93 - 97 % and 83 - 88 % for Swedish , French and English respectively , with the corresponding accuracies on unknown words being 25 - 50 % , 45 - 52 % and 44 - 58 % ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Table  lists the results , giving the tagset size , the degree of ambiguity and the accuracies on known ambiguous and unknown words ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The ambiguous word accuracy is plotted in figures  -  ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "What seems to come out from these results is that there is not a consistent relationship between the size of the tagsets and the tagging accuracy ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The most common pattern was for a larger tagset to give higher accuracy , but there were notable exceptions in French ( where gender marking was the key factor ) , in Swedish unknown words ( which show the reverse trend ) and in English unknown words ( which show no very clear trend at all ) ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "This seems to fit quite well with the difficulties that were suggested above in reasoning about the effect of tagset size ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The main conclusion of this paper is therefore that the knowledge engineering component of setting up a tagger should concentrate on optimising the tagset for external criteria , and that the internal criterion of tagset size does not show sufficient generality to be taken into account without prior knowledge of properties of the language ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Perhaps this is not too surprising , but it is useful to have an experimental confirmation that the linguistics matters rather than the engineering ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "One final observation about the experiments : the accuracy on unknown words was very low in all of the tests , and was particularly bad in Swedish ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The tagger used in the experiments took a very simple-minded approach to unknown words ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "An alternative that is often used is to limit the possible tags using a simple morphological analysis or some other examination of the surface form of the word ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For example , in a variant of the English tagger which was not used in these experiments , a module which reduces the range of possible tags based on testing for only seven surface characteristics such as capitalisation and word endings improved the unknown word accuracy by 15 - 20 ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "The results above show that if it were not for unknown words , there might be some argument for favouring larger tagsets , since they have some tendency to give a higher accuracy ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "A tentative experiment on the contribution of using morphological or surface analysis in French and Swedish was therefore carried out ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Firstly , in both languages , the unknown words from the second experiment were looked up in the lexicon trained from the full corpus to see what tags they might have ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For Swedish , 96 % of the unknown words came from inflected classes , and had a single tag ; for French the figure was about 60 % ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In both cases , very few of the unknown words ( less than 1 % ) had more than one tag ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "This provides some hope that an inflectional analysis might should help considerably with unknown words ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "For confirmation , the list of French unknown words was given to a French grammarian , who predicted that it would be possible to make a good guess at the correct tag from the morphology for around 70 % of the words , and could narrow down the possible tags to two or three for about a further 25 % ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "However , further research is needed to determine how realistic these estimates turn out to be ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "We have shown how a simple experiment in changing the tagset shows that the relationship between tagset size and accuracy is a weak one and is not consistent against languages ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "This seems to go against the `` folklore '' of the tagging community , where smaller tagsets are often held to be better for obtaining good accuracy ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "I have suggested that what is important is to choose the tagset required for the application , rather than to optimise it for the tagger ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "A follow-up to this work might be to apply similar tests in other languages to provide a further confirmation of the results , and to see if language families which similar characteristics can be identified ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "A further conclusion might be that when a corpus is being tagged by hand , a large tagset should be used , since it can always be reduced to a smaller one if the application demands it ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Perhaps the major factor we have to set against this is the danger of introducing more human errors into the manual tagging process , by increasing the cognitive load on the human annotators ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "An experiment designed to explore the relationship between tagging accuracy and the nature of the tagset is described , using corpora in English , French and Swedish ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "In particular , the question of internal versus external criteria for tagset design is considered , with the general conclusion that external ( linguistic ) criteria should be followed ."}
 {"title": "Tagset Design and Inflected Languages", "sentence": "Some problems associated with tagging unknown words in inflected languages are briefly considered ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In this paper we are concerned with the syntactic analysis phase of a natural language understanding system ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Ordinarily , the input of such a system is a sequence of words ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "However , following"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Parsing uncertain input might be necessary in case of ill-formed textual input , or in case of speech input ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For example , if a natural language understanding system is interfaced with a speech recognition component , chances are that this compenent is uncertain about the actual string of words that has been uttered , and thus produces a word lattice of the most promising hypotheses , rather than a single sequence of words ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "FSA of course generalizes such word lattices ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "As another example , certain techniques to deal with ill-formed input can be characterized as finite state transducers  ; the composition of an input string with such a finite state transducer results in a FSA that can then be input for syntactic parsing ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Such an approach allows for the treatment of missing , extraneous , interchanged or misused words  , ,  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Such techniques might be of use both in the case of written and spoken language input ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In the latter case another possible application concerns the treatment of phenomena such as repairs  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Note that we allow the input to be a full FSA ( possibly including cycles , etc. ) since some of the above-mentioned techniques indeed result in cycles ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Whereas an ordinary word-graph always defines a finite language , a FSA of course can easily define an infinite number of sentences ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Cycles might emerge to treat unknown sequences of words , i.e. sentences with unknown parts of unknown lengths  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "As suggested by an ACL reviewer , one could also try to model haplology phenomena ( such as the 's in English sentences like ` The chef at Joe 's hat ' , where ` Joe 's ' is the name of a restaurant ) using a finite state transducer ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In a straightforward approach this would also lead to a finite-state automaton with cycles ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "It can be shown that the computation of the intersection of a FSA and a CFG requires only a minimal generalization of existing parsing algorithms ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "We simply replace the usual string positions with the names of the states in the FSA ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "It is also straightforward to show that the complexity of this process is cubic in the number of states of the FSA ( in the case of ordinary parsing the number of states equals n + 1 )  ,  ( assuming the right-hand-sides of grammar rules have at most two categories ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In this paper we investigate whether the same techniques can be applied in case the grammar is a constraint-based grammar rather than a CFG ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For specificity we will take the grammar to be a Definite Clause Grammar ( DCG )  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A DCG is a simple example of a family of constraint-based grammar formalisms that are widely used in natural language analysis ( and generation ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The main findings of this paper can be extended to other members of that family of constraint-based grammar formalisms ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The calculation of the intersection of a CFG and a FSA is very simple  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The ( context-free ) grammar defining this intersection is simply constructed by keeping track of the state names in the non-terminal category symbols ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For each rule  there are rules  , for all  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore for each transition  we have a rule  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Thus the intersection of a FSA and a CFG is a CFG that exactly derives all parse-trees ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Such a grammar might be called the parse-forest grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Although this construction shows that the intersection of a FSA and a CFG is itself a CFG , it is not of practical interest ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The reason is that this construction typically yields an enormous amount of rules that are ` useless ' ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In fact the ( possibly enormously large ) parse forest grammar might define an empty language ( if the intersection was empty ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Luckily ` ordinary ' recognizers / parsers for CFG can be easily generalized to construct this intersection yielding ( in typical cases ) a much smaller grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Checking whether the intersection is empty or not is then usually very simple as well : only in the latter case will the parser terminate succesfully ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "To illustrate how a parser can be generalized to accept a FSA as input we present a simple top-down parser ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A context-free grammar is represented as a definite-clause specification as follows ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "We do not wish to define the sets of terminal and non-terminal symbols explicitly , these can be understood from the rules that are defined using the relation rule/2 , and where symbols of the rhs are prefixed with ` - ' in the case of terminals and ` + ' in the case of non-terminals ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The relation top/1 defines the start symbol ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The language  is defined as :"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In order to illustrate how ordinary parsers can be used to compute the intersection of a FSA and a CFG consider first the definite-clause specification of a top-down parser ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This parser runs in polynomial time if implemented using Earley deduction or XOLDT resolution  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "It is assumed that the input string is represented by the trans/3 predicate ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The predicate side _ effect is used to construct the parse forest grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The predicate always succeeds , and as a side-effect asserts that its argument is a rule of the parse forest grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For the sentence ` a a b b ' we obtain the parse forest grammar :"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The reader easily verifies that indeed this grammar generates ( a isomorphism of ) the single parse tree of this example , assuming of course that the start symbol for this parse-forest grammar is  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In the parse-forest grammar , complex symbols are non-terminals , atomic symbols are terminals ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Next consider the definite clause specification of a FSA ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "We define the transition relation using the relation trans/3 ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For start states , the relation start/1 should hold , and for final states the relation final/1 should hold ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Thus the following FSA , defining the regular language  ( i.e. an even number of a 's followed by at least one b ) is given as :"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Interestingly , nothing needs to be changed to use the same parser for the computation of the intersection of a FSA and a CFG ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "If our input ` sentence ' now is the definition of trans/3 as given above , we obtain the following parse forest grammar ( where the start symbol is"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Thus , even though we now use the same parser for an infinite set of input sentences ( represented by the FSA ) the parser still is able to come up with a parse forest grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A possible derivation for this grammar constructs the following ( abbreviated ) parse tree in figure  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Note that the construction of"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In this section we want to generalize the ideas described above for CFG to DCG ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "First note that the problem of calculating the intersection of a DCG and a FSA can be solved trivially by a generalization of the construction by  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "However , if we use that method we will end up ( typically ) with an enormously large forest grammar that is not even guaranteed to contain solutions ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Therefore , we are interested in methods that only generate a small subset of this ; e.g. if the intersection is empty we want an empty parse-forest grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The straightforward approach is to generalize existing recognition algorithms ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The same techniques that are used for calculating the intersection of a FSA and a CFG can be applied in the case of DCGs ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In order to compute the intersection of a DCG and a FSA we assume that FSA are represented as before ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "DCGs are represented using the same notation we used for context-free grammars , but now of course the category symbols can be first-order terms of arbitrary complexity ( note that without loss of generality we don't take into account DCGs having external actions defined in curly braces ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "But if we use existing techniques for parsing DCGs , then we are also confronted with an undecidability problem : the recognition problem for DCGs is undecidable  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A fortiori the problem of deciding whether the intersection of a FSA and a DCG is empty or not is undecidable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This undecidability result is usually circumvented by considering subsets of DCGs which can be recognized effectively ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For example , we can restrict the attention to DCGs of which the context-free skeleton does not contain cycles ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Recognition for such ` off-line parsable ' grammars is decidable  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Most existing constraint-based parsing algorithms will terminate for grammars that exhibit the property that for each string there is only a finite number of possible derivations ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Note that off-line parsability is one possible way of ensuring that this is the case ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This observation is not very helpful in establishing insights concerning interesting subclasses of DCGs for which termination can be guaranteed ( in the case of FSA input ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The reason is that there are now two sources of recursion : in the DCG and in the FSA ( cycles ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "As we saw earlier : even for CFG it holds that there can be an infinite number of analyses for a given FSA ( but in the CFG this of course does not imply undecidability ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "I now show that the question whether the intersection of a FSA and an off-line parsable DCG is empty is undecidable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A yes-no problem is undecidable   if there is no algorithm that takes as its input an instance of the problem and determines whether the answer to that instance is ` yes ' or ` no ' ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "An instance of a problem consists of a particular choice of the parameters of that problem ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "I use Post 's Correspondence Problem ( PCP ) as a well-known undecidable problem ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "I show that if the above mentioned intersection problem were decidable , then we could solve the PCP too ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The following definition and example of a PCP are taken from  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "An instance of PCP consists of two lists ,  and  of strings over some alphabet  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This instance has a solution if there is any sequence of integers  , with  , such that ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The sequence  is a solution to this instance of PCP ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "As an example , assume that  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore , let  and  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A solution to this instance of PCP is the sequence 2,1,1,3 ( obtaining the sequence 101111110 ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For an illustration , cf. figure  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Clearly there are PCP 's that do not have a solution ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Assume again that  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore let  and  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Clearly this PCP does not have a solution ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In general , however , the problem whether some PCP has a solution or not is not decidable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This result is proved by  by showing that the halting problem for Turing Machines can be encoded as an instance of Post 's Correspondence Problem ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "First I give a simple algorithm to encode any instance of a PCP as a pair , consisting of a FSA and an off-line parsable DCG , in such a way that the question whether there is a solution to this PCP is equivalent to the question whether the intersection of this FSA and DCG is empty ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Encoding of PCP"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For each  ( k the length of lists A and B ) define a DCG rule ( the i-th member of A is  , and the i-th member of B is  ) :"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore , there is a rule"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore , there is a rule"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Also , s is the start category of the DCG ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Finally , the FSA consists of a single state q which is both the start state and the final state , and a single transition  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This FSA generates  ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Observe that the DCG is off-line parsable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The underlying idea of the algorithm is really very simple ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For each pair of strings from the lists A and B there will be one lexical entry ( deriving the terminal x ) where these strings are represented by a difference-list encoding ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore there is a general combination rule that simply concatenates A-strings and concatenates B-strings ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Finally the rule for s states that in order to construct a succesful top category the A and B lists must match ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The resulting DCG , FSA pair for the example PCP is given in figure  :"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Proposition"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The question whether the intersection of a FSA and an off-line parsable DCG is empty is undecidable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Proof"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Suppose the problem was decidable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In that case there would exist an algorithm for solving the problem ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This algorithm could then be used to solve the PCP , because a PCP  has a solution if and only if its encoding given above as a FSA and an off-line parsable DCG is not empty ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The PCP problem however is known to be undecidable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Hence the intersection question is undecidable too ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The following approaches towards the undecidability problem can be taken :"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "limit the power of the FSA"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "limit the power of the DCG"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "compromise completeness"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "compromise soundness"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "These approaches are discussed now in turn ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Limit the FSA"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Rather than assuming the input for parsing is a FSA in its full generality , we might assume that the input is an ordinary word graph ( a FSA without cycles ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Thus the techniques for robust processing that give rise to such cycles cannot be used ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "One example is the processing of an unknown sequence of words , e.g. in case there is noise in the input and it is not clear how many words have been uttered during this noise ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "It is not clear to me right now what we loose ( in practical terms ) if we give up such cycles ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Note that it is easy to verify that the question whether the intersection of a word-graph and an off-line parsable DCG is empty or not is decidable since it reduces to checking whether the DCG derives one of a finite number of strings ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Limit the DCG"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Another approach is to limit the size of the categories that are being employed ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This is the GPSG and F-TAG approach ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In that case we are not longer dealing with DCGs but rather with CFGs ( which have been shown to be insufficient in general for the description of natural languages ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Compromise Completeness"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Completeness in this context means : the parse forest grammar contains all possible parses ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "It is possible to compromise here , in such a way that the parser is guaranteed to terminate , but sometimes misses a few parse-trees ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For example , if we assume that each edge in the FSA is associated with a probability it is possible to define a threshold such that each partial result that is derived has a probability higher than the threshold ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Thus , it is still possible to have cycles in the FSA , but anytime the cycle is ` used ' the probability decreases and if too many cycles are encountered the threshold will cut off that derivation ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Of course this implies that sometimes the intersection is considered empty by this procedure whereas in fact the intersection is not ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "For any threshold it is the case that the intersection problem of off-line parsable DCGs and FSA is decidable ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Compromise Soundness"}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Soundness in this context should be understood as the property that all parse trees in the parse forest grammar are valid parse trees ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A possible way to ensure termination is to remove all constraints from the DCG and parse according to this context-free skeleton ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "The resulting parse-forest grammar will be too general most of the times ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "A practical variation can be conceived as follows ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "From the DCG we take its context-free skeleton ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This skeleton is obtained by removing the constraints from each of the grammar rules ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Then we compute the intersection of the skeleton with the input FSA ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This results in a parse forest grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Finally , we add the corresponding constraints from the DCG to the grammar rules of the parse forest grammar ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "This has the advantage that the result is still sound and complete , although the size of the parse forest grammar is not optimal ( as a consequence it is not guaranteed that the parse forest grammar contains a parse tree ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Of course it is possible to experiment with different ways of taking the context-free skeleton ( including as much information as possible / useful ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "I would like to thank Gosse Bouma , Mark-Jan Nederhof and John Nerbonne for comments on this paper ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore the paper benefitted from remarks made by the anonymous ACL reviewers ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": ""}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems , in which parsing takes a word lattice as input ( rather than a word string ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore , certain techniques for robust parsing can be modelled as finite state transducers ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In this paper we investigate how we can generalize this approach for unification grammars ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "It is shown that existing parsing algorithms can be easily extended for FSA inputs ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "However , we also show that the termination properties change drastically : we show that it is undecidable whether the intersection of a FSA and a DCG is empty ( even if the DCG is off-line parsable ) ."}
 {"title": "The intersection of Finite State Automata and Definite Clause Grammars", "sentence": "Furthermore we discuss approaches to cope with the problem ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Recently various methods for automatically constructing a thesaurus ( hierarchically clustering words ) based on corpus data have been proposed  ,  , ,  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The realization of such an automatic construction method would make it possible to"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "save the cost of constructing a thesaurus by hand ,"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "do away with subjectivity inherent in a hand made thesaurus , and"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "make it easier to adapt a natural language processing system to a new domain ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In this paper , we propose a new method for automatic construction of thesauri ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Specifically , we view the problem of automatically clustering words as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns ( in general , any set of words ) and a partition of a set of verbs ( in general , any set of words ) , and propose an estimation algorithm using simulated annealing with an energy function based on the Minimum Description Length ( MDL ) Principle ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "As a strategy of statistical estimation MDL is guaranteed to be near optimal ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We empirically evaluated the effectiveness of our method ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In particular , we compared the performance of an MDL-based simulated annealing algorithm in hierarchical word clustering against that of one based on the Maximum Likelihood Estimator ( MLE , for short ) ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We found that the MDL-based method performs better than the MLE-based method ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We also evaluated our method by conducting pp-attachment disambiguation experiments using a thesaurus automatically constructed by it and found that disambiguation results can be improved ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Since some words never occur in a corpus , and thus cannot be reliably classified by a method solely based on corpus data , we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We conducted some experiments in order to test the effectiveness of this strategy ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the coverage of our disambiguation method , while maintaining high accuracy ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "A method of constructing a thesaurus based on corpus data usually consists of the following three steps :"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Extract co-occurrence data ( e.g. case frame data , adjacency data ) from a corpus ,"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Starting from a single class ( or each word composing its own class ) , divide ( or merge ) word classes based on the co-occurrence data using some similarity ( distance ) measure ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( The former approach is called ` divisive , ' the latter ` agglomerative ' )"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Repeat step  until some stopping condition is met , to construct a thesaurus ( tree ) ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The method we propose here consists of the same three steps ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Suppose available to us are data like those in Figure  , which are frequency data ( co-occurrence data ) between verbs and their objects extracted from a corpus ( step  ) ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We then view the problem of clustering words as that of estimating a probabilistic model ( representing probability distribution ) that generates such data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We assume that the target model can be defined in the following way ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "First , we define a noun partition  over a given set of nouns  and a verb partion  over a given set of verbs  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "A noun partition is any set  satisfying  ,  and  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "A verb partition  is defined analogously ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In this paper , we call a member of a noun partition a ` noun cluster , ' and a member of a verb partition a ` verb cluster ' ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We refer to a member of the Cartesian product of a noun partition and a verb partition (  ) simply as a ` cluster ' ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We then define a probabilistic model ( a joint distribution ) , written  , where random variable  assumes a value from a fixed noun partition  , and  a value from a fixed verb partition  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Within a given cluster , we assume that each element is generated with equal probability , i.e. ,"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Figure  shows two example models which might have given rise to the data in Figure  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In this paper , we assume that the observed data are generated by a model belonging to the class of models just described , and select a model which best explains the data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "As a result of this , we obtain both noun clusters and verb clusters ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "This problem setting is based on the intuitive assumption that similar words occur in the same context with roughly equal likelihood , as is made explicit in equation  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Thus selecting a model which best explains the given data is equivalent to finding the most appropriate classification of words based on their co-occurrence ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We now turn to the question of what strategy ( or criterion ) we should employ for estimating the best model ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our choice is the MDL ( Minimum Description Length ) principle  ,  ,  ,  ,  , a well-known principle of data compression and statistical estimation from information theory ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "MDL stipulates that the best probability model for given data is that model which requires the least code length for encoding of the model itself , as well as the given data relative to it ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We refer to the code length for the model as the ` model description length ' and that for the data the ` data description length ' ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We apply MDL to the problem of estimating a model consisting of a pair of partitions as described above ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In this context , a model with less clusters , such as Model 2 in Figure  , tends to be simpler ( in terms of the number of parameters ) , but also tends to have a poorer fit to the data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In contrast , a model with more clusters , such as Model 1 in Figure  , is more complex , but tends to have a better fit to the data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Thus , there is a trade-off relationship between the simplicity of clustering ( a model ) and the goodness of fit to the data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The model description length quantifies the simplicity ( complexity ) of a model , and the data description length quantifies the fit to the data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "According to MDL , the model which minimizes the sum total of the two types of description lengths should be selected ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In what follows , we will describe in detail how the description length is to be calculated in our current context , as well as our simulated annealing algorithm based on MDL ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We will now describe how the description length for a model is calculated ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Recall that each model is specified by the Cartesian product of a noun partition and a verb partition , and a number of parameters for them ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Here we let  denote the size of the noun partition , and  the size of the verb partition ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Then , there are  free parameters in a model ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Given a model M and data S , its total description length L ( M ) is computed as the sum of the model description length  , the description length of its parameters  , and the data description length  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( We often refer to  as the model description length ) ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Namely ,"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We employ the ` binary noun clustering method , ' in which  is fixed at  and we are to decide whether  or  , which is then to be applied recursively to the clusters thus obtained ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "This is as if we view the nouns as entities and the verbs as features and cluster the entities based on their features ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Since there are  subsets of the set of nouns  , and for each ` binary ' noun partition we have two different subsets ( a special case of which is when one subset is  and the other the empty set  ) , the number of possible binary noun partitions is  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Thus for each binary noun partition we need  bits ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Hence  is calculated as"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "is calculated by"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "where | S | denotes the input data size , and  is the number of ( free ) parameters in the model ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "It is known that using   bits to describe each of the parameters will ( approximately ) minimize the description length  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Finally ,  is calculated by"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "where  denotes the observed frequency of the noun verb pair  , and  the estimated probability of  , which is calculated as follows"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "where  denotes the observed frequency of the noun verb pairs belonging to cluster  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "With the description length of a model defined in the above manner , we wish to select a model having the minimum description length and output it as the result of clustering ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Since the model description length  is the same for each model , in practice we only need to calculate and compare  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The description lengths for the data in Figure  using the two models in Figure  are shown in Table  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( Table  shows some values needed for the calculation of the description length for Model 1 . ) These calculations indicate that according to MDL , Model 1 should be selected over Model 2 ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We could in principle calculate the description length for each model and select a model with the minimum description length , if computation time were of no concern ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "However , since the number of probabilistic models under consideration is exponential , this is not feasible in practice ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We employ the ` simulated annealing technique ' to deal with this problem ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Figure  shows our ( divisive ) clustering algorithm ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Although there have been many methods of word clustering proposed to date , their objectives seem to vary ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In Table  we exhibit a simple comparison between our work and related work ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Perhaps the method proposed by  is the most relevant in our context ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In  , they proposed a method of ` soft clustering , ' namely , each word can belong to a number of distinct classes with certain probabilities ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Soft clustering has several desirable properties ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "For example , word sense ambiguities in input data can be treated in a unified manner ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Here , we restrict our attention on ` hard clustering ' ( i.e. , each word must belong to exactly one class ) , in part because we are interested in comparing the thesauri constructed by our method with existing hand-made thesauri ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( Note that a hand made thesaurus is based on hard clustering . )"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In this section , we elaborate on the merits of our method ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In statistical natural language processing , usually the number of parameters in a probabilistic model to be estimated is very large , and therefore such a model is difficult to estimate with a reasonable data size that is available in practice ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( This problem is usually referred to as the ` data sparseness problem ' . )"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We could smooth the estimated probabilities using an existing smoothing technique  ,  , then calculate some similarity measure using the smoothed probabilities , and then cluster words according to it ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "There is no guarantee , however , that the employed smoothing method is in any way consistent with the clustering method used subsequently ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our method based on MDL resolves this issue in a unified fashion ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "By employing models that embody the assumption that words belonging to a same cluster occur in the same context with equal likelihood , our method achieves the smoothing effect as a side effect of the clustering process , where the domains of smoothing coincide with the clusters obtained by clustering ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Thus , the coarseness or fineness of clustering also determines the degree of smoothing ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "All of these effects fall out naturally as a corollary of the imperative of ` best possible estimation , ' the original motivation behind the MDL principle ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In our simulated annealing algorithm , we could alternatively employ the Maximum Likelihood Estimator ( MLE ) as criterion for the best probabilistic model , instead of MDL ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "MLE , as its name suggests , selects a model which maximizes the likelihood of the data , that is ,  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "This is equivalent to minimizing the ` data description length ' as defined in Section 3 , i.e.  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We can see easily that MDL generalizes MLE , in that it also takes into account the complexity of the model itself ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In the presence of models with varying complexity , MLE tends to overfit the data , and output a model that is too complex and tailored to fit the specifics of the input data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "If we employ MLE as criterion in our simulated annealing algorithm , it will result in selecting a very fine model with many small clusters , most of which will have probabilities estimated as zero ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Thus , in contrast to employing MDL , it will not have the effect of smoothing at all ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Purely as a method of estimation as well , the superiority of MDL over MLE is supported by convincing theoretical findings  ,  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "For instance , the speed of convergence of the models selected by MDL to the true model is known to be near optimal ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( The models selected by MDL converge to the true model approximately at the rate of 1 / s where s is the number of parameters in the true model , whereas for MLE the rate is 1 / t , where t is the size of the domain , or in our context , the total number of elements of  . )"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "` Consistency ' is another desirable property of MDL , which is not shared by MLE ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "That is , the number of parameters in the models selected by MDL converge to that of the true model  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Both of these properties of MDL are empirically verified in our present context , as will be shown in the next section ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In particular , we have compared the performance of employing an MDL-based simulated annealing against that of one based on MLE in word clustering ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We describe our experimental results in this section ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We compared the performance of employing MDL as a criterion in our simulated annealing algorithm , against that of employing MLE by simulation experiments ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We artificially constructed a true model of word co-occurrence , and then generated data according to its distribution ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We then used the data to estimate a model ( clustering words ) , and measured the KL distance between the true model and the estimated model ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( The algorithm used for MLE was the same as that shown in Figure  , except the ` data description length ' replaces the ( total ) description length ' in Step 2 . )"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Figure  plots the relation between the number of obtained noun clusters ( leaf nodes in the obtained thesaurus tree ) versus the input data size , averaged over 10 trials ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( The number of noun clusters in the true model is 4 . )"}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Figure   plots the KL distance versus the data size , also averaged over the same 10 trials ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The results indicate that MDL converges to the true model faster than MLE ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Also , MLE tends to select a model overfitting the data , while MDL tends to select a model which is simple and yet fits the data reasonably well ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We conducted the same simulation experiment for some other models and found the same tendencies ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "( Figure   and Figure   show the analogous results when the number of noun clusters in the true model is 2 ) ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We conclude that it is better to employ MDL than MLE in word clustering ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We extracted roughly 180,000 case frames from the bracketed WSJ ( Wall Street Journal ) corpus of the Penn Tree Bank  as co-occurrence data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We then constructed a number of thesauri based on these data , using our method ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Figure  shows an example thesaurus for the 20 most frequently occurred nouns in the data , constructed based on their appearances as subject and object of roughly 2000 verbs ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The obtained thesaurus seems to agree with human intuition to some degree ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "For example , ` million ' and ` billion ' are classified in one noun cluster , and ` stock ' and ` share ' are classified together ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Not all of the noun clusters , however , seem to be meaningful in the useful sense ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "This is probably because the data size we had was not large enough ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "This general tendency is also observed in another example thesaurus obtained by our method , shown in Figure  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Pragmatically speaking , however , whether the obtained thesaurus agrees with our intuition in itself is only of secondary concern , since the main purpose is to use the constructed thesaurus to help improve on a disambiguation task ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We also evaluated our method by using a constructed thesaurus in a pp-attachment disambiguation experiment ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We used as training data the same 180,000 case frames in Experiment 1 ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We also extracted as our test data 172  patterns from the data in the same corpus , which is not used in the training data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "For the 150 words that appear in the position of  , we constructed a thesaurus based on the co-occurrences between heads and slot values of the frames in the training data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "This is because in our disambiguation test we only need a thesaurus consisting of these 150 words ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We then applied the learning method proposed in  to learn case frame patterns with the constructed thesaurus as input using the same training data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "That is , we used it to learn the conditional distributions  ,  , where  and  vary over the internal nodes in a certain ` cut ' in the thesaurus tree ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Table  shows some example case frame patterns obtained by this method , and Figure  shows the leaf nodes dominated by the internal nodes appearing in the case frame patterns of Table  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We then compare  and  , which are estimated based on the case frame patterns , to determine the attachment site of  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "More specifically , if the former is larger than the latter , we attach it to verb , and if the latter is larger than the former , we attach it to  , and otherwise ( including when both are 0 ) , we conclude that we cannot make a decision ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Table  shows the results of our pp-attachment disambiguation experiment in terms of ` coverage ' and ` accuracy ' ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Here ` coverage ' refers to the proportion ( in percentage ) of the test patterns on which the disambiguation method could make a decision ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "` Base Line ' refers to the method of always attaching  to  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "` Word-Based , ' ` MLE-Thesaurus , ' and ` MDL-Thesaurus ' respectively stand for using word-based estimates , using a thesaurus constructed by employing MLE , and using a thesaurus constructed by our method ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Note that the coverage of ` MDL-Thesaurus ' significantly outperformed that of ` Word-Based , ' while basically maintaining high accuracy ( though it drops somewhat ) , indicating that using an automatically constructed thesaurus can improve disambiguation results in terms of coverage ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We also tested the method proposed in  of learning case frames patterns using an existing thesaurus ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In particular , we used this method with WordNet  and using the same training data , and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We show the result of this experiment as ` WordNet ' in Table  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We can see that in terms of ` coverage , ' ` WordNet ' outperforms ` MDL-Thesaurus , ' but in terms of ` accuracy , ' ` MDL-Thesaurus ' outperforms ` WordNet ' ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "These results can be interpreted as follows ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "An automatically constructed thesaurus is more domain dependent and captures the domain dependent features better , and thus using it achieves high accuracy ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "On the other hand , since training data we had available is insufficient , its coverage is smaller than that of a hand made thesaurus ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In practice , it makes sense to combine both types of thesauri ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "More specifically , an automatically constructed thesaurus can be used within its coverage , and outside its coverage , a hand made thesaurus can be used ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Given the current state of the word clustering technique ( namely , it requires data size that is usually not available , and it tends to be computationally demanding ) , this strategy is practical ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We show the result of this combined method as ` MDL-Thesaurus + WordNet ' in Table  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our experimental result shows that employing the combined method does increase the coverage of disambiguation ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We also tested ` MDL-Thesaurus + WordNet + LA + Default , ' which stands for using the learned thesaurus and WordNet first , then the lexical association value proposed by  , and finally the default ( i.e. always attaching  to  ) ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our best disambiguation result obtained using this last combined method somewhat improves the accuracy reported in  (  ) ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We have proposed a method of clustering words based on large corpus data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We conclude with the following remarks ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our method of hierarchical clustering of words based on the MDL principle is theoretically sound ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our experimental results show that it is better to employ MDL than MLE as estimation criterion in word clustering ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Using a thesaurus constructed by our method can improve pp-attachment disambiguation results ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "At the current state of the art in statistical natural language processing , it is best to use a combination of an automatically constructed thesaurus and a hand made thesaurus for disambiguation purpose ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "The disambiguation accuracy obtained this way was  ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "In the future , hopefully with larger training data size , we plan to construct larger thesauri as well as to test other clustering algorithms ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We thank Mr. K. Nakamura , Mr. T. Fujita , and Dr. K. Kobayashi of NEC C & C Res. Labs.  for their constant encouragement ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We thank Dr. K. Yamanishi of C & C Res. Labs. for his comments ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We thank Ms. Y. Yamaguchi of NIS for her programming effort ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We address the problem of automatically constructing a thesaurus by clustering words based on corpus data ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus ."}
 {"title": "Clustering Words with the MDL Principle", "sentence": "Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In collaborative consultation dialogues , the consultant and the executing agent collaborate on developing a plan to achieve the executing agent 's domain goal ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Since agents are autonomous and heterogeneous , it is inevitable that conflicts in their beliefs arise during the planning process ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In such cases , collaborative agents should attempt to square away  the conflicts by engaging in collaborative negotiation to determine what should constitute their shared plan of actions and shared beliefs ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Collaborative negotiation differs from non-collaborative negotiation and argumentation mainly in the attitude of the participants , since collaborative agents are not self-centered , but act in a way as to benefit the agents as a group ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Thus , when facing a conflict , a collaborative agent should not automatically reject a belief with which she does not agree ; instead , she should evaluate the belief and the evidence provided to her and adopt the belief if the evidence is convincing ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "On the other hand , if the evaluation indicates that the agent should maintain her original belief , she should attempt to provide sufficient justification to convince the other agent to adopt this belief if the belief is relevant to the task at hand ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This paper presents a model for engaging in collaborative negotiation to resolve conflicts in agents ' beliefs about domain knowledge ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Our model"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "detects conflicts in beliefs and initiates a negotiation subdialogue only when the conflict is relevant to the current task ,"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "selects the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts exist ,"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "selects appropriate evidence to justify the system 's proposed modification of the user 's beliefs , and"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "captures the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , thus enabling the system to handle embedded negotiation subdialogues ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Researchers have studied the analysis and generation of arguments  ,  ,  ,  ,  ,  ; however , agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": ",  formulated an artificial language for modeling collaborative discourse using proposal / acceptance and proposal / rejection sequences ; however , her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "have noted the importance of a cooperative system providing support for its responses ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "They identified strategies that a system can adopt in justifying its beliefs ; however , they did not specify the criteria under which each of these strategies should be selected ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost , inference cost , and cost of memory retrieval ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "However , her model focuses on determining when to include informationally redundant utterances , whereas our model determines whether or not justification is needed for a claim to be convincing and , if so , selects appropriate evidence from the system 's private beliefs to support the claim ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": ",  introduced the idea of utilizing a belief revision mechanism  to predict whether a set of evidence is sufficient to change a user 's existing belief and to generate responses for information retrieval dialogues in a library domain ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "They argued that in the library dialogues they analyzed , `` in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution . ''  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "However , our analysis of naturally-occurring consultation dialogues  ,  shows that in other domains conflict resolution does extend beyond a single exchange of conflicting beliefs ; therefore we employ a recursive model for collaboration that captures extended negotiation and represents the structure of the discourse ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Furthermore , their system deals with a single conflict , while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In addition , we provide a process for selecting among multiple possible pieces of evidence ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Collaborative negotiation occurs when conflicts arise among agents developing a shared plan during collaborative planning ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "A collaborative agent is driven by the goal of developing a plan that best satisfies the interests of all the agents as a group , instead of one that maximizes his own interest ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This results in several distinctive features of collaborative negotiation :"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "A collaborative agent does not insist on winning an argument , and may change his beliefs if another agent presents convincing justification for an opposing belief ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This differentiates collaborative negotiation from argumentation  ,  , ,  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Agents involved in collaborative negotiation are open and honest with one another ; they will not deliberately present false information to other agents , present information in such a way as to mislead the other agents , or strategically hold back information from other agents for later use ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Collaborative agents are interested in others ' beliefs in order to decide whether to revise their own beliefs so as to come to agreement  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Although agents involved in argumentation and non-collaborative negotiation take other agents ' beliefs into consideration , they do so mainly to find weak points in their opponents ' beliefs and attack them to win the argument ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In our earlier work , we built on  and developed a model that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This model views collaborative planning as agent A proposing a set of actions and beliefs to be incorporated into the shared plan being developed , agent B evaluating the proposal to determine whether or not he accepts the proposal and , if not , agent B proposing a set of modifications to A 's original proposal ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The proposed modifications will again be evaluated by A , and if conflicts arise , she may propose modifications to B 's previously proposed modifications , resulting in a recursive process ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "However , our research did not specify , in cases where multiple conflicts arise , how an agent should identify which part of an unaccepted proposal to address or how to select evidence to support the proposed modification ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This paper extends that work by incorporating into the modification process a strategy to determine the aspect of the proposal that the agent will address in her pursuit of conflict resolution , as well as a means of selecting appropriate evidence to justify the need for such modification ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In order to capture the agents ' intentions conveyed by their utterances , our model of collaborative negotiation utilizes an enhanced version of the dialogue model described in  to represent the current status of the interaction ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The enhanced dialogue model has four levels : the domain level which consists of the domain plan being constructed for the user 's later execution , the problem-solving level which contains the actions being performed to construct the domain plan , the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions , and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This paper focuses on the evaluation and modification of proposed beliefs , and details a strategy for engaging in collaborative negotiations ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Our system maintains a set of beliefs about the domain and about the user 's beliefs ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Associated with each belief is a strength that represents the agent 's confidence in holding that belief ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "We model the strength of a belief using endorsements , which are explicit records of factors that affect one 's certainty in a hypothesis  , following ,  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Our endorsements are based on the semantics of the utterance used to convey a belief , the level of expertise of the agent conveying the belief , stereotypical knowledge , etc ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The belief level of the dialogue model consists of mutual beliefs proposed by the agents ' discourse actions ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "When an agent proposes a new belief and gives ( optional ) supporting evidence for it , this set of proposed beliefs is represented as a belief tree , where the belief represented by a child node is intended to support that represented by its parent ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The root nodes of these belief trees ( top-level beliefs ) contribute to problem-solving actions and thus affect the domain plan being developed ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Given a set of newly proposed beliefs , the system must decide whether to accept the proposal or to initiate a negotiation dialogue to resolve conflicts ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The evaluation of proposed beliefs starts at the leaf nodes of the proposed belief trees since acceptance of a piece of proposed evidence may affect acceptance of the parent belief it is intended to support ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The process continues until the top-level proposed beliefs are evaluated ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Conflict resolution strategies are invoked only if the top-level proposed beliefs are not accepted because if collaborative agents agree on a belief relevant to the domain plan being constructed , it is irrelevant whether they agree on the evidence for that belief  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In determining whether to accept a proposed belief or evidential relationship , the evaluator first constructs an evidence set containing the system 's evidence that supports or attacks _bel and the evidence accepted by the system that was proposed by the user as support for _bel ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Each piece of evidence contains a belief _bel  , and an evidential relationship supports(_bel,_bel)."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Following  's weakest link assumption the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The evaluator then employs a simplified version of   to compare the strengths of the evidence that supports and attacks _bel ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If the strength of one set of evidence strongly outweighs that of the other , the decision to accept or reject _bel is easily made ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "However , if the difference in their strengths does not exceed a pre-determined threshold , the evaluator has insufficient information to determine whether to adopt _bel and therefore will initiate an information-sharing subdialogue  to share information with the user so that each of them can knowledgably re-evaluate the user 's original proposal ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If , during information-sharing , the user provides convincing support for a belief whose negation is held by the system , the system may adopt the belief after the re-evaluation process , thus resolving the conflict without negotiation ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "To illustrate the evaluation of proposed beliefs , consider the following utterances :"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Figure  shows the belief and discourse levels of the dialogue model that captures utterances  and  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The belief evaluation process will start with the belief at the leaf node of the proposed belief tree , On-Sabbatical(Smith,next year)) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The system will first gather its evidence pertaining to the belief , which includes"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "a warranted belief that Dr. Smith has postponed his sabbatical until 1997(Postponed-Sabbatical(Smith,1997)) ,"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "a warranted belief that Dr. Smith postponing his sabbatical until 1997 supports the belief that he is not going on sabbatical next year ( supports(Postponed-Sabbatical(Smith,1997),  On-Sabbatical(Smith,next year)) ,"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "a strong belief that Dr. Smith will not be a visitor at IBM next year (  visitor(Smith,IBM,next year)) , and"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "a warranted belief that Dr. Smith not being a visitor at IBM next year supports the belief that he is not going on sabbatical next year ( supports( visitor(Smith,IBM,next year),  On-Sabbatical(Smith,next year)) , perhaps because Dr. Smith has expressed his desire to spend his sabbatical only at IBM ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The belief revision mechanism will then be invoked to determine the system 's belief about On-Sabbatical(Smith,next year) based on the system 's own evidence and the user 's statement ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Since beliefs  and  above constitute a warranted piece of evidence against the proposed belief and beliefs  and  constitute a strong piece of evidence against it , the system will not accept On-Sabbatical(Smith,next year) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The system believes that being on sabbatical implies a faculty member is not teaching any courses ; thus the proposed evidential relationship will be accepted ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "However , the system will not accept the top-level proposed belief ,  Teaches(Smith,AI) , since the system has a prior belief to the contrary ( as expressed in utterance  ) and the only evidence provided by the user was an implication whose antecedent was not accepted ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The collaborative planning principle in  ,  suggests that `` conversants must provide evidence of a detected discrepancy in belief as soon as possible . ''"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Thus , once an agent detects a relevant conflict , she must notify the other agent of the conflict and initiate a negotiation subdialogue to resolve it -- to do otherwise is to fail in her responsibility as a collaborative agent ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "We capture the attempt to resolve a conflict with the problem-solving action Modify-Proposal , whose goal is to modify the proposal to a form that will potentially be accepted by both agents ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "When applied to belief modification , Modify-Proposal has two specializations : Correct-Node , for when a proposed belief is not accepted , and Correct-Relation , for when a proposed evidential relationship is not accepted ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Figure  shows the problem-solving recipes for Correct-Node and its subaction , Modify-Node , that is responsible for the actual modification of the proposal ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The applicability conditions of Correct-Node specify that the action can only be invoked when _s1 believes that _node is not acceptable while _s2 believes that it is ( when _s1 and _s2 disagree about the proposed belief represented by _node ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "However , since this is a collaborative interaction , the actual modification can only be performed when both _s1 and _s2 believe that _node is not acceptable -- that is , the conflict between _s1 and _s2 must have been resolved ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This is captured by the applicability condition and precondition of Modify-Node ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The attempt to satisfy the precondition causes the system to post as a mutual belief to be achieved the belief that _node is not acceptable , leading the system to adopt discourse actions to change _s2 's beliefs , thus initiating a collaborative negotiation subdialogue ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "When multiple conflicts arise between the system and the user regarding the user 's proposal , the system must identify the aspect of the proposal on which it should focus in its pursuit of conflict resolution ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "For example , in the case where Correct-Node is selected as the specialization of Modify-Proposal , the system must determine how the parameter _node in Correct-Node should be instantiated ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The goal of the modification process is to resolve the agents ' conflicts regarding the unaccepted top-level proposed beliefs ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "For each such belief , the system could provide evidence against the belief itself , address the unaccepted evidence proposed by the user to eliminate the user 's justification for the belief , or both ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Since collaborative agents are expected to engage in effective and efficient dialogues , the system should address the unaccepted belief that it predicts will most quickly resolve the top-level conflict ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Therefore , for each unaccepted top-level belief , our process for selecting the focus of modification involves two steps : identifying a candidate foci tree from the proposed belief tree , and selecting a focus from the candidate foci tree using the heuristic `` attack the belief ( s ) that will most likely resolve the conflict about the top-level belief . ''"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "A candidate foci tree contains the pieces of evidence in a proposed belief tree which , if disbelieved by the user , might change the user 's view of the unaccepted top-level proposed belief ( the root node of that belief tree ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "It is identified by performing a depth-first search on the proposed belief tree ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "When a node is visited , both the belief and the evidential relationship between it and its parent are examined ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If both the belief and relationship were accepted by the evaluator , the search on the current branch will terminate , since once the system accepts a belief , it is irrelevant whether it accepts the user 's support for that belief  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Otherwise , this piece of evidence will be included in the candidate foci tree and the system will continue to search through the evidence in the belief tree proposed as support for the unaccepted belief and / or evidential relationship ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Once a candidate foci tree is identified , the system should select the focus of modification based on the likelihood of each choice changing the user 's belief about the top-level belief ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Figure  shows our algorithm for this selection process ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Given an unaccepted belief ( _bel ) and the beliefs proposed to support it , Select-Focus-Modification will annotate _bel with"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "its focus of modification ( _bel focus ) , which contains a set of beliefs ( _bel and / or its descendents ) which , if disbelieved by the user , are predicted to cause him to disbelieve _bel , and"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "the system 's evidence against _bel itself ( _bel s-attack ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Select-Focus-Modification determines whether to attack _bel 's supporting evidence separately , thereby eliminating the user 's reasons for holding _bel , to attack _bel itself , or both ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "However , in evaluating the effectiveness of attacking the proposed evidence for _bel , the system must determine whether or not it is possible to successfully refute a piece of evidence ( i.e. , whether or not the system believes that sufficient evidence is available to convince the user that a piece of proposed evidence is invalid ) , and if so , whether it is more effective to attack the evidence itself or its support ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Thus the algorithm recursively applies itself to the evidence proposed as support for _bel which was not accepted by the system ( step  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In this recursive process , the algorithm annotates each unaccepted belief or evidential relationship proposed to support _bel with its focus of modification ( _bel .focus ) and the system 's evidence against it ( _bel .s-attack ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "bel .focus contains the beliefs selected to be addressed in order to change the user 's belief about _bel  , and its value will be nil if the system predicts that insufficient evidence is available to change the user 's belief about _bel  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Based on the information obtained in step  , Select-Focus-Modification decides whether to attack the evidence proposed to support _bel , or _bel itself ( step  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Its preference is to address the unaccepted evidence , because  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Thus the algorithm first considers whether or not attacking the user 's support for _bel is sufficient to convince him of  _bel ( step  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "It does so by gathering ( in cand-set ) evidence proposed by the user as direct support for _bel but which was not accepted by the system and which the system predicts it can successfully refute ( i.e. , _bel .focus is not nil ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The algorithm then hypothesizes that the user has changed his mind about each belief in cand-set and predicts how this will affect the user 's belief about _bel ( step  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If the user is predicted to accept  _bel under this hypothesis , the algorithm invokes Select-Min-Set to select a minimum subset of cand-set as the unaccepted beliefs that it would actually pursue , and the focus of modification ( _bel.focus ) will be the union of the focus for each of the beliefs in this minimum subset ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If attacking the evidence for _bel does not appear to be sufficient to convince the user of  _bel , the algorithm checks whether directly attacking _bel will accomplish this goal ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If providing evidence directly against _bel is predicted to be successful , then the focus of modification is _bel itself ( step  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If directly attacking _bel is also predicted to fail , the algorithm considers the effect of attacking both _bel and its unaccepted proposed evidence by combining the previous two prediction processes ( step  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If the combined evidence is still predicted to fail , the system does not have sufficient evidence to change the user 's view of _bel ; thus , the focus of modification for _bel is nil ( step  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Notice that steps  and  of the algorithm invoke a function , Predict , that makes use of the belief revision mechanism  discussed in Section  to predict the user 's acceptance or unacceptance of _bel based on the system 's knowledge of the user 's beliefs and the evidence that could be presented to him  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The result of Select-Focus-Modification is a set of user beliefs ( in _bel.focus ) that need to be modified in order to change the user 's belief about the unaccepted top-level belief ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Thus , the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message  ,  , ,  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Research on the quantity of evidence indicates that there is no optimal amount of evidence , but that the use of high-quality evidence is consistent with persuasive effects  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "On the other hand ,  's maxim of quantity specifies that one should not contribute more information than is required ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Thus , it is important that a collaborative agent selects sufficient and effective , but not excessive , evidence to justify an intended mutual belief ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "To convince the user of a belief , _bel , our system selects appropriate justification by identifying beliefs that could be used to support _bel and applying filtering heuristics to them ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The system must first determine whether justification for _bel is needed by predicting whether or not merely informing the user of _bel will be sufficient to convince him of _bel ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If so , no justification will be presented ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If justification is predicted to be necessary , the system will first construct the justification chains that could be used to support _bel ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "For each piece of evidence that could be used to directly support _bel , the system first predicts whether the user will accept the evidence without justification ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If the user is predicted not to accept a piece of evidence ( evid  ) , the system will augment the evidence to be presented to the user by posting evid  as a mutual belief to be achieved , and selecting propositions that could serve as justification for it ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This results in a recursive process that returns a chain of belief justifications that could be used to support _bel ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Once a set of beliefs forming justification chains is identified , the system must then select from this set those belief chains which , when presented to the user , are predicted to convince the user of _bel ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Our system will first construct a singleton set for each such justification chain and select the sets containing justification which , when presented , is predicted to convince the user of _bel ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If no single justification chain is predicted to be sufficient to change the user 's beliefs , new sets will be constructed by combining the single justification chains , and the selection process is repeated ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This will produce a set of possible candidate justification chains , and three heuristics will then be applied to select from among them ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The first heuristic prefers evidence in which the system is most confident since high-quality evidence produces more attitude change than any other evidence form  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Furthermore , the system can better justify a belief in which it has high confidence should the user not accept it ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The second heuristic prefers evidence that is novel to the user , since studies have shown that evidence is most persuasive if it is previously unknown to the hearer ,  ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The third heuristic is based on"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "After the evaluation of the dialogue model in Figure  , Modify-Proposal is invoked because the top-level proposed belief is not accepted ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In selecting the focus of modification , the system will first identify the candidate foci tree and then invoke the Select-Focus-Modification algorithm on the belief at the root node of the candidate foci tree ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The candidate foci tree will be identical to the proposed belief tree in Figure  since both the top-level proposed belief and its proposed evidence were rejected during the evaluation process ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This indicates that the focus of modification could be either  Teaches(Smith,AI) or On-Sabbatical(Smith,next year) ( since the evidential relationship between them was accepted ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "When Select-Focus-Modification is applied to  Teaches(Smith,AI) , the algorithm will first be recursively invoked on On-Sabbatical(Smith,next year) to determine the focus for modifying the child belief ( step  in Figure  ) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Since the system has two pieces of evidence against On-Sabbatical(Smith,next year) ,"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "a warranted piece of evidence containing Postponed-Sabbatical(Smith,1997) and supports(Postponed-Sabbatical(Smith,1997) ,  On-Sabbatical(Smith,next year)) , and"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "a strong piece of evidence containing  visitor(Smith,IBM,next year) and supports( visitor(Smith,IBM,next year) ,  On-Sabbatical(Smith,next year)) , the evidence is predicted to be sufficient to change the user 's belief in On-Sabbatical(Smith,next year) , and hence  Teaches(Smith,AI) ; thus , the focus of modification will be On-Sabbatical(Smith,next year) ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The Correct-Node specialization of Modify-Proposal will be invoked since the focus of modification is a belief , and in order to satisfy the precondition of Modify-Node(Figure ) , MB(S,U, On-Sabbatical(Smith,next year)) will be posted as a mutual belief to be achieved ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Since the user has a warranted belief in On-Sabbatical(Smith,next year) ( indicated by the semantic form of utterance  ) , the system will predict that merely informing the user of the intended mutual belief is not sufficient to change his belief ; therefore it will select justification from the two available pieces of evidence supporting  On-Sabbatical(Smith,next year) presented earlier ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The system will predict that either piece of evidence combined with the proposed mutual belief is sufficient to change the user 's belief ; thus , the filtering heuristics are applied ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The first heuristic will cause the system to select Postponed-Sabbatical(Smith,1997) and supports(Postponed-Sabbatical(Smith,1997) ,  On-Sabbatical(Smith,next year)) as support , since it is the evidence in which the system is more confident ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The system will try to establish the mutual beliefs as an attempt to satisfy the precondition of Modify-Node ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This will cause the system to invoke Inform discourse actions to generate the following utterances :"}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "If the user accepts the system 's utterances,thus satisfying the precondition that the conflict be resolved , Modify-Node can be performed and changes made to the original proposed beliefs ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Otherwise , the user may propose modifications to the system 's proposed modifications , resulting in an embedded negotiation subdialogue ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This paper has presented a computational strategy for engaging in collaborative negotiation to square away conflicts in agents ' beliefs ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "The model captures features specific to collaborative negotiation ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "It also supports effective and efficient dialogues by identifying the focus of modification based on its predicted success in resolving the conflict about the top-level belief and by using heuristics motivated by research in social psychology to select a set of evidence to justify the proposed modification of beliefs ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Furthermore , by capturing collaborative negotiation in a cycle of Propose-Evaluate-Modify actions , the evaluation and modification processes can be applied recursively to capture embedded negotiation subdialogues ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In collaborative planning activities , since the agents are autonomous and heterogeneous , it is inevitable that conflicts arise in their beliefs during the planning process ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "In cases where such conflicts are relevant to the task at hand , the agents should engage in collaborative negotiation as an attempt to square away the discrepancies in their beliefs ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise , and of selecting appropriate evidence to justify the need for such modification ."}
 {"title": "Response Generation in Collaborative Negotiation", "sentence": "Furthermore , by capturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions , our model can successfully handle embedded negotiation subdialogues ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Conversation between two people has a number of characteristics that have yet to be modeled adequately in human-computer dialogue ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Conversation is BIDIRECTIONAL ; there is a two way flow of information between participants ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Information is exchanged by MIXED-INITIATIVE ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Each participant will , on occasion , take the conversational lead ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Conversational partners not only respond to what others say , but feel free to volunteer information that is not requested and sometimes ask questions of their own  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "As INITIATIVE passes back and forth between the discourse participants , we say that CONTROL over the conversation gets transferred from one discourse participant to another ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Why should we , as computational linguists , be interested in factors that contribute to the interactivity of a discourse ?"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "There are both theoretical and practical motivations ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "First , we wish to extend formal accounts of single utterances produced by single speakers to explain multi-participant , multi-utterance discourses ,  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Previous studies of the discourse structure of multi-participant dialogues have often factored out the role of MIXED-INITIATIVE , by allocating control to one participant  ,  , or by assuming a passive listener ,  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Since conversation is a collaborative process  ,  , models of conversation can provide the basis for extending planning theories ,  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "When the situation requires the negotiation of a collaborative plan , these theories must account for the interacting beliefs and intentions of multiple participants ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "From a practical perspective , there is ample evidence that limited mixed-initiative has contributed to lack of system usability ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Many researchers have noted that the absence of mixed-initiative gives rise to two problems with expert systems : They don't allow users to participate in the reasoning process , or to ask the questions they want answered  , ,  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In addition , question answering systems often fail to take account of the system 's role as a conversational partner ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "For example , fragmentary utterances may be interpreted with respect to the previous user input , but what users say is often in reaction to the system 's previous response ,  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In this paper we focus on interactive discourse ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We model mixed-initiative using an utterance type classification and a set of rules for transfer of control between discourse participants that were proposed by  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We evaluate the generality of this analysis by applying the control rules to 4 sets of dialogues , including both advisory dialogues ( ADs ) and task-oriented dialogues ( TODs ) ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We analysed both financial and support ADs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The financial ADs are from the radio talk show `` Harry Gross : Speaking of Your Money '' ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The support ADs resulted from a client phoning an expert to help them diagnose and repair various software faults ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The TODs are about the construction of a plastic water pump in both telephone and keyboard modality ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The application of the control rules to these dialogues lets us derive domain-independent discourse segments with each segment being controlled by one or other discourse participant ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We propose that control segments correspond to different subgoals in the evolving discourse plan ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In addition , we argue that various linguistic devices are necessary for conversational participants to coordinate their contributions to the dialogue and agree on their mutual beliefs with respect to a evolving plan , for example , to agree that a particular subgoal has been achieved ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "A final phenomenon concerns shifts of control and the devices used to achieve this ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Control shifts occur because it is unusual for a single participant to be responsible for coordinating the achievement of the whole discourse plan ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "When a different participant assumes control of a discourse subgoal then a control shift occurs and the participants must have mechanisms for achieving this ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The control framework distinguishes instances in which a control shift is negotiated by the participants and instances where one participant seizes control ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This paper has two objectives :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "To explore the phenomenon of control in relation to ATTENTIONAL STATE  , ,  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We predict shifts of attentional state when shifts in control are negotiated and agreed by all participants , but not when control is seized by one participant without the acceptance of the others ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This should be reflected in different distribution of anaphora in the two cases ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "To test predictions about the distribution of control in different types of dialogues ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Because the TOD 's embody the master-slave assumption  , and control is allocated to the expert , our expectation is that control should be located exclusively with one participant in the TODs in contrast with the ADs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We use the framework for the allocation and transfer of control of  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The analysis is based on a classification of utterances into 4 types ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "These are :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "ASSERTIONS :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Declarative utterances used to state facts ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Yes and No in response to a question were classified as assertions on the basis that they are supplying information ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "COMMANDS :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Utterances intended to instigate action ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Generally imperative form , but could be indirect such as My suggestion would be that you do ...."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "QUESTIONS :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Utterances which are intended to elicit information , including indirect forms such as I was wondering whether I should ..."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "PROMPTS : Utterances which did not express propositional content , such as Yeah , Okay , Uh-huh ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Note that prompts are in direct contrast to the other options that a participant has available at any point in the discourse ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "By indicating that the speaker does not want the floor , prompts function on a number of levels , including the expression of understanding or agreement  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The rules for the allocation of control are based on the utterance type classification and allow a dialogue to be divided into segments that correspond to which speaker is the controller of the segment ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The definition of controller can be seen to correspond to the intuitions behind the term INITIATING CONVERSATIONAL PARTICIPANT ( ICP ) , who is defined as the initiator of a given discourse segment  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The OTHER CONVERSATIONAL PARTICIPANT ( S ) , OCP , may speak some utterances in a segment , but the DISCOURSE SEGMENT PURPOSE , must be the purpose of the ICP ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The control rules place a segment boundary whenever the roles of the participants ( ICP or OCP ) change ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "For example :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": ""}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The boundaries fell into one of three types :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "ABDICATION :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "REPETITION / SUMMARY :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "That would be my recommendation and that will ensure that you get a logically integral set of files ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "INTERRUPTION :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "ABDICATIONS correspond to those cases where the controller produces a prompt as the last utterance of the segment ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The class REPETITION / SUMMARY corresponds to the controller producing a redundant utterance ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The utterance is either an exact repetition of previous propositional content , or a summary that realizes a proposition , P , which could have been inferred from what came before ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Thus orderly control shifts occur when the controller explicitly indicates that s / he wishes to relinquish control ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "What unifies ABDICATIONS and REPETITION / SUMMARIES is that the controller supplies no new propositional content ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The remaining class , INTERRUPTIONS , characterize shifts occurring when the noncontroller displays initiative by seizing control ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This class is more general than other definitions of Interruptions ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "It properly contains cross-speaker interruptions that involve topic shift , similar to the true-interruptions of  , as well as clarification subdialogues ,  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This classification suggests that the transfer of control is often a collaborative phenomenon ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Since a noncontroller ( OCP ) , has the option of seizing control at any juncture in discourse , it would seem that controllers ( ICPs ) , are in control because the noncontroller allows it ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "These observations address problems raised by"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The claim is that shifts of control often do not occur until the controller indicates the end of a discourse segment by abdicating or producing a repetition / summary ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "To determine the relationship between the derived control segments and ATTENTIONAL STATE we looked at the distribution of anaphora with respect to the control segments in the ADs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "All data were analysed statistically by  and all differences cited are significant at the 0.05 level ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We looked at all anaphors ( excluding first and second person ) , and grouped them into 4 classes ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Classes of Anaphors ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "3 RD PERSON :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "it , they , them , their , she , he , her , him , his"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "ONE / SOME ,"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "one of them , one of those , a new one , that one , the other one , some"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "DEICTIC :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Noun phrases , e.g. this , that , this NP , that NP , those NP , these NP"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "EVENT :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Verb Phrases , Sentences , Segments , e.g. this , that , it ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The class DEICTIC refers to deictic references to material introduced by noun phrases , whereas the class EVENT refers to material introduced clausally ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The first phenomenon we noted was that the anaphora distribution indicated that some segments are hierarchically related to others ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This was especially apparent in cases where one discourse participant interrupted briefly , then immediately passed control back to the other ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The following example illustrates the same point ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The control segments as defined would treat both of these cases as composed of 3 different segments ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "But this ignores the fact that utterances  and  have closely related propositional content in the first example , and that the plural pronoun straddles the central subsegment with the same referents being picked out by they and their in the second example ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Thus we allowed for hierarchical segments by treating the interruptions of 2 - 4 as subsegments , and utterances 1 and 5 as related parts of the parent segments ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "All interruptions were treated as embeddings in this way ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "However the relationship of the segment after the interruption to the segment before must be determined on independent grounds such as topic or intentional structure ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Once we extended the control framework to allow for the embedding of interrupts , we coded every anaphor with respect to whether its antecedent lay outside or within the current segment ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "These are labelled X ( cross segment boundary antecedent ) NX ( no cross segment boundary ) , in Figure  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In addition we break these down as to which type of control shift occurred at the previous segment boundary ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We also looked at the distribution of anaphora in the Support ADs and found similar results ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "For both dialogues , the distribution of anaphors varies according to which type of control shift occurred at the previous segment boundary ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "When we look at the different types of anaphora , we find that third person and one anaphors cross boundaries extremely rarely , but the event anaphors and the deictic pronouns demonstrate a different pattern ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "What does this mean ?"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The fact that anaphora is more likely to cross segment boundaries following interruptions than for summaries or abdications is consistent with the control principles ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "With both summaries and abdications the speaker gives an explicit signal that s / he wishes to relinquish control ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In contrast , interruptions are the unprompted attempts of the listener to seize control , often having to do with some ` problem ' with the controller 's utterance ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Therefore , interruptions are much more likely to be within topic ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "But why should deixis and event anaphors behave differently from the other anaphors ?"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Deixis serves to pick out objects that cannot be selected by the use of standard anaphora , i.e. we should expect the referents for deixis to be outside immediate focus and hence more likely to be outside the current segment  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The picture is more complex for event anaphora , which seems to serve a number of different functions in the dialogue ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "It is used to talk about the past events that lead up to the current situation , I did THAT in order to move the place ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "It is also used to refer to sets of propositions of the preceding discourse , Now THAT 'S a little background  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The most prevalent use , however , was to refer to future events or actions , THAT would be the move that I would make - but you have to do IT the same day ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Since the task in the ADs is to develop a plan , speakers use event anaphora as concise references to the plans they have just negotiated and to discuss the status and quality of plans that have been suggested ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Thus the frequent cross-speaker references to future events and actions correspond to phases of plan negotiation  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "More importantly these references are closely related to the control structure ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The example above illustrates the clustering of event anaphora at segment boundaries ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "One discourse participant uses an anaphor to summarize a plan , but when the other participant evaluates this plan there may be a control shift and any reference to the plan will necessarily cross a control boundary ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The distribution of event anaphora bears this out , since 23 / 25 references to future actions are within 2 utterances of a segment boundary ( See the example above ) ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "More significantly every instance of event anaphora crossing a segment boundary occurs when the speaker is talking about future events or actions ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We also looked at the TODs for instances of anaphora being used to describe a future act in the way that we observed in the ADs. However , over the 938 turns in the TODs , there were only 18 instances of event anaphora , because in the main there were few occasions when it was necessary to talk about the plan ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The financial ADs had 45 event anaphors in 474 utterances ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "To explore the relationship of control to planning , we compare the TODs with both types of ADs ( financial and support ) ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We would expect these dialogues to differ in terms of initiative ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In the ADs , the objective is to develop a collaborative plan through a series of conversational exchanges ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Both discourse participants believe that the expert has knowledge about the domain , but only has partial information about the situation ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "They also believe that the advisee must contribute both the problem description and also constraints as to how the problem can be solved ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This information must be exchanged , so that the mutual beliefs necessary to develop the collaborative plan are established in the conversation  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The situation is different in the TODs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Both participants here believe at the outset that the expert has sufficient information about the situation and complete and correct knowledge about how to execute the Task ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Since the apprentice has no need to assert information to change the expert 's beliefs or to ask questions to verify the expert 's beliefs or to issue commands , we should not expect the apprentice to have control ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "S / he is merely present to execute the actions indicated by the knowledgeable participant ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The differences in the beliefs and knowledge states of the participants can be interpreted in the terms of the collaborative planning principles of  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We generalize the principles of INFORMATION QUALITY and PLAN QUALITY , which predict when an interrupt should occur ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "INFORMATION QUALITY :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The listener must believe that the information that the speaker has provided is true , unambiguous and relevant to the mutual goal ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This corresponds to the two rules :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "(A1) TRUTH :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "If the listener believes a fact P and believes that fact to be relevant and either believes that the speaker believes not P or that the speaker does not know P then interrupt ;"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "(A2) AMBIGUITY :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "If the listener believes that the speaker 's assertion is relevant but ambiguous then interrupt ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "PLAN QUALITY :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The listener must believe that the action proposed by the speaker is a part of an adequate plan to achieve the mutual goal and the action must also be comprehensible to the listener ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The two rules to express this are :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "(B1) EFFECTIVENESS :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "If the listener believes P and either believes that P presents an obstacle to the proposed plan or believes that P is part of the proposed plan that has already been satisfied , then interrupt ;"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "(B2) AMBIGUITY :"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "If the listener believes that an assertion about the proposed plan is ambiguous , then interrupt ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "These principles indirectly provide a means to ensure mutual belief ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Since a participant must interrupt if any condition for an interrupt holds , then lack of interruption signals that there is no discrepancy in mutual beliefs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "If there is such a discrepancy , the interruption is a necessary contribution to a collaborative plan , not a distraction from the joint activity ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We compare ADs to TODs with respect to how often control is exchanged by calculating the average number of turns between control shifts ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We also investigate whether control is shared equally between participants and what percentage of control shifts are represented by abdications , interrupts , and summaries for each dialogue type ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "See Figure  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Three things are striking about this data ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "As we predicted , the distribution of control between expert and client is completely different in the ADs and the TODs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The expert has control for around 90 % of utterances in the TODs whereas control is shared almost equally in the ADs. Secondly , contrary to our expectations , we did find some instances of shifts in the TODs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Thirdly , the distribution of interruptions and summaries differs across dialogue types ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "How can the collaborative planning principles highlight the differences we observe ?"}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "There seem to be two reasons why shifts occur in the TODs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "First , many interruptions in the TODs result from the apprentice seizing control just to indicate that there is a temporary problem and that plan execution should be delayed ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Second , control was exchanged when the execution of the task started to go awry ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The problem with the physical situation indicates to the apprentice that the relevant beliefs are no longer shared ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The Instructor is not in possession of critical information such as the current state of the apprentice 's pump ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This necessitates an information exchange to resynchronize mutual beliefs , so that the rest of the plan may be successfully executed ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "However , since control is explicitly allocated to the instructor in TODs , there is no reason for that participant to believe that the other has any contribution to make ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Thus there are fewer attempts by the instructor to coordinate activity , such as by using summaries to synchronize mutual beliefs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Therefore , if the apprentice needs to make a contribution , s / he must do so via interruption , explaining why there are many more interruptions in these dialogues ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In addition , the majority of Interruptions ( 73 % ) are initiated by apprentices , in contrast to the ADs in which only 29 % are produced by the Clients ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Summaries are more frequent in ADs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In the ADs both participants believe that a plan cannot be constructed without contributions from both of them ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Abdications and summaries are devices which allow these contributions to be coordinated and participants use these devices to explicitly set up opportunities for one another to make a contribution , and to ensure mutual beliefs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The increased frequency of summaries in the ADs may result from the fact that the participants start with discrepant mutual beliefs about the situation and that establishing and maintaining mutual beliefs is a key part of the ADs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "It has often been stated that discourse is an inherently collaborative process and that this is manifested in certain phenomena , e.g. the use of anaphora and cue words  ,  ,  by which the speaker makes aspects of the discourse structure explicit ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We found shifts of attentional state when shifts in control are negotiated and agreed by all participants , but not when control is seized by one participant without the acceptance of the others ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This was reflected in different distribution of anaphora in the two cases ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Furthermore we found that not all types of anaphora behaved in the same way ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Event anaphora clustered at segment boundaries when it was used to refer to preceding segments and was more likely to cross segment boundaries because of its function in talking about the proposed plan ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We also found that control was distributed and exchanged differently in the ADs and TODs ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "These results provide support for the control rules ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In our analysis we argued for hierarchical organization of the control segments on the basis of specific examples of interruptions ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We also believe that there are other levels of structure in discourse that are not captured by the control rules , e.g. control shifts do not always correspond with task boundaries ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "There can be topic shifts without change of initiation , change of control without a topic shift  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The relationship of cue words , intonational contour  and the use of modal subordination  to the segments derived from the control rules is a topic for future research ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "A more controversial question concerns rhetorical relations and the extent to which these are detected and used by listeners  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": ",  ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "One category of rhetorical relation he describes is that of ELABORATION , in which a speaker repeats the propositional content of a previous utterance ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": ""}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Abdications , repetitions and summaries all add no new information and function to signal to the listener that the speaker has nothing further to say right now ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The listener certainly must recognize this fact ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Summaries appear to have an additional function of synchronization , by allowing both participants to agree on what propositions are mutually believed at that point in the discussion ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Thus this work highlights aspects of collaboration in discourse , but should be formally integrated with research on collaborative planning  ,  , particularly with respect to the relation between control shifts and the coordination of plans ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "Conversation between two people is usually of MIXED-INITIATIVE , with CONTROL over the conversation being transferred from one person to another ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We apply a set of rules for the transfer of control to 4 sets of dialogues consisting of a total of 1862 turns ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The application of the control rules lets us derive domain-independent discourse structures ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The derived structures indicate that initiative plays a role in the structuring of discourse ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "In order to explore the relationship of control and initiative to discourse processes like centering , we analyze the distribution of four different classes of anaphora for two data sets ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "This distribution indicates that some control segments are hierarchically related to others ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "The analysis suggests that discourse participants often mutually agree to a change of topic ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "We also compared initiative in Task Oriented and Advice Giving dialogues and found that both allocation of control and the manner in which control is transferred is radically different for the two dialogue types ."}
 {"title": "Mixed Initiative in Dialogue : An Investigation into Discourse Segmentation", "sentence": "These differences can be explained in terms of collaborative planning principles ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Following the work of , for example ,  ,  and  , it has become widely accepted that semantic interpretation in human sentence processing can occur before sentence boundaries and even before clausal boundaries ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It is less widely accepted that there is a need for incremental interpretation in computational applications ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In the 1970 s and early 1980 s several computational implementations motivated the use of incremental interpretation as a way of dealing with structural and lexical ambiguity ( a survey is given in  ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A sentence such as the following has 4862 different syntactic parses due solely to attachment ambiguity  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Although some of the parses can be ruled out using structural preferences during parsing ( such as Late Closure or Minimal Attachment  ) , extraction of the correct set of plausible readings requires use of real world knowledge ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Incremental interpretation allows on-line semantic filtering , i.e. parses of initial fragments which have an implausible or anomalous interpretation are rejected , thereby preventing ambiguities from multiplying as the parse proceeds ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "However , on-line semantic filtering for sentence processing does have drawbacks ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Firstly , for sentence processing using a serial architecture ( rather than one in which syntactic and semantic processing is performed in parallel ) , the savings in computation obtained from on-line filtering have to be balanced against the additional costs of performing semantic computations for parses of fragments which would eventually be ruled out anyway from purely syntactic considerations ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Moreover , there are now relatively sophisticated ways of packing ambiguities during parsing ( e.g. by the use of graph-structured stacks and packed parse forests  ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Secondly , the task of judging plausibility or anomaly according to context and real world knowledge is a difficult problem , except in some very limited domains ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In contrast , statistical techniques using lexeme co-occurrence provide a relatively simple mechanism which can imitate semantic filtering in many cases ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , instead of judging bank as a financial institution as more plausible than bank as a riverbank in the noun phrase the rich bank , we can compare the number of co-occurrences of the lexemes rich and bank  ( = riverbank ) versus rich and bank  ( = financial institution ) in a semantically analysed corpus ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Cases where statistical techniques seem less appropriate are where plausibility is affected by local context ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , consider the ambiguous sentence ,"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "in the two contexts"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Such cases involve reasoning with an interpretation in its immediate context , as opposed to purely judging the likelihood of a particular linguistic expression in a given application domain ( see e.g.  for discussion ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Although the usefulness of on-line semantic filtering during the processing of complete sentences is debatable , filtering has a more plausible role to play in interactive , real-time environments , such as interactive spell checkers ( see e.g.  for arguments for incremental parsing in such environments ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Here the choice is between whether or not to have semantic filtering at all , rather than whether to do it on-line , or at the end of the sentence ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The concentration in early literature on using incremental interpretation for semantic filtering has perhaps distracted from some other applications which provide less controversial applications ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "We will consider two in detail here : graphical interfaces , and dialogue ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The Foundations for Intelligent Graphics Project ( FIG ) considered various ways in which natural language input could be used within computer aided design systems ( the particular application studied was computer aided kitchen design , where users would not necessarily be professional designers ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Incremental interpretation was considered to be useful in enabling immediate visual feedback ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Visual feedback could be used to provide confirmation ( for example , by highlighting an object referred to by a successful definite description ) , or it could be used to give the user an improved chance of achieving successful reference ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , if sets of possible referents for a definite noun phrase are highlighted during word by word processing then the user knows how much or how little information is required for successful reference ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Human dialogue , in particular , task oriented dialogue is characterised by a large numbers of self-repairs   ,   , such as hesitations , insertions , and replacements ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It is also common to find interruptions requesting extra clarification , or disagreements before the end of a sentence ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It is even possible for sentences started by one dialogue participant to be finished by another ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Applications involving the understanding of dialogues include information extraction from conversational databases , or computer monitoring of conversations ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It also may be useful to include some features of human dialogue in man-machine dialogue ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , interruptions can be used for early signalling of errors and ambiguities ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Let us first consider some examples of self-repair ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Insertions add extra information , usually modifiers e.g."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Replacements correct pieces of information e.g."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In some cases information from the corrected material is incorporated into the final message ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , consider :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In  , the corrected material the three main sources of data come provides the antecedent for the pronoun they ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In  the corrected material tells us that the man is both old and has a wife ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In  , the pronoun he is bound by the quantifier every boy ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For a system to understand dialogues involving self-repairs such as those in  would seem to require either an ability to interpret incrementally , or the use of a grammar which includes self repair as a syntactic construction akin to non-constituent coordination ( the relationship between coordination and self-correction is noted by  ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For a system to generate self repairs might also require incremental interpretation , assuming a process where the system performs on-line monitoring of its output ( akin to"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It has been suggested that generation of self repairs is useful in cases where there are severe time constraints , or where there is rapidly changing background information  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A more compelling argument for incremental interpretation is provided by considering dialogues involving interruptions ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Consider the following dialogue from the TRAINS corpus  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This requires interpretation by speaker B before the end of A 's sentence to allow objection to the apposition , the engine at Avon , engine E ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "An example of the potential use of interruptions in human computer interaction is the following :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In this example , interpretation must not only be before the end of the sentence , but before a constituent boundary ( the verb phrase in the user 's command has not yet been completed ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In this section we shall briefly review work on providing semantic representations ( e.g. lambda expressions ) word by word ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Traditional layered models of sentence processing first build a full syntax tree for a sentence , and then extract a semantic representation from this ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "To adapt this to an incremental perspective , we need to be able to provide syntactic structures ( of some sort ) for fragments of sentences , and be able to extract semantic representations from these ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "One possibility , which has been explored mainly within the Categorial Grammar tradition  is to provide a grammar which can treat most if not all initial fragments as constituents ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "They then have full syntax trees from which the semantics can be calculated ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "However , an alternative possibility is to directly link the partial syntax trees which can be formed for non-constituents with functional semantic representations ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , a fragment missing a noun phrase such as John likes can be associated with a semantics which is a function from entities to truth values ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Hence , the partial syntax tree given in Fig.  ,"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "can be associated with a semantic representation ,  x. likes ( john , x ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Both Categorial approaches to incremental interpretation and approaches which use partial syntax trees get into difficulty in cases of left recursion ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Consider the sentence fragment , Mary thinks John ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A possible partial syntax tree is provided by Fig.  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "However , this is not the only possible partial tree ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In fact there are infinitely many different trees possible ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The completed sentence may have an arbitrarily large number of intermediate nodes between the lower s node and the lower np ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , John could be embedded within a gerund e.g. Mary thinks John leaving here was a mistake , and this in turn could be embedded e.g. Mary thinks John leaving here being a mistake is surprising ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "John could also be embedded within a sentence which has a sentence modifier requiring its own s node e.g. Mary thinks John will go home probably , and this can be further embedded e.g. Mary thinks John will go home probably because he is tired ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The problem of there being an arbitrary number of different partial trees for a particular fragment is reflected in most current approaches to incremental interpretation being either incomplete , or not fully word by word ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , incomplete parsers have been proposed by  and  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": ""}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": ""}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A complete incremental parser , which is not fully word by word , was proposed by  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This is based on arc-eager left-corner parsing  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "To enable complete , fully word by word parsing requires a way of encoding an infinite number of partial trees ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "There are several possibilities ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The first is to use a language describing trees where we can express the fact that John is dominated by the s node , but do not have to specify what it is immediately dominated by ( e.g. D-Theory )  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Semantic representations could be formed word by word by extracting ` default ' syntax trees ( by strengthening dominance links into immediated dominance links wherever possible ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A second possibility is to factor out recursive structures from a grammar ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "show how this can be done for a phrase structure grammar ( creating an equivalent Tree Adjoining Grammar  ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The parser for the resulting grammar allows linear parsing for an ( infinitely ) parallel system , with the absorption of each word performed in constant time ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "At each choice point , there are only a finite number of possible new partial TAG trees ( the TAG trees represents the possibly infinite number of trees which can be formed using adjunction ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It should again be possible to extract ` default ' semantic values , by taking the semantics from the TAG tree ( i.e. by assuming that there are to be no adjunctions ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A somewhat similar system has recently been proposed by  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The third possibility is suggested by considering the semantic representations which are appropriate during a word by word parse ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Although there are any number of different partial trees for the fragment Mary thinks John , the semantics of the fragment can be represented using just two lambda expressions :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Consider the first ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The lambda abstraction ( over a functional item of type e  t ) can be thought of as a way of encoding an infinite set of partial semantic ( tree ) structures ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , the eventual semantic structure may embed john at any depth e.g."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The second expression ( a functional item over type e  t and t  t ) , allows for eventual structures where the main sentence is embedded e.g."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This third possibility is therefore to provide a syntactic correlate of lambda expressions ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In practice , however , provided we are only interested in mapping from a string of words to a semantic representation , and don't need explicit syntax trees to be constructed , we can merely use the types of the ` syntactic lambda expressions ' , rather than the expressions themselves ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This is essentially the approach taken in  in order to provide complete , word by word , incremental interpretation using simple lexicalised grammars , such as a lexicalised version of formal dependency grammar and simple categorial grammar ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In processing the sentence Mary introduced John to Susan , a word-by-word approach such as  provides the following logical forms after the corresponding sentence fragments are absorbed :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Each input level representation is appropriate for the meaning of an incomplete sentence , being either a proposition or a function into a proposition ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In  it is argued that the incrementally derived meanings are not judged for plausibility directly , but instead are first turned into existentially quantified propositions ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , instead of judging the plausibility of  , we judge the plausibility of  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This is just the proposition Mary introduced something to something using a generalized quantifier notation of the form Quantifier ( Variable , Restrictor , Body ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Although the lambda expressions are built up monotonically , word by word , the propositions formed from them may need to be retracted , along with all the resulting inferences ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , Mary introduced something to something is inappropriate if the final sentence is Mary introduced noone to anybody ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A rough algorithm is as follows :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Parse a new word , Word"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Form a new lambda expression by combining the lambda expression formed after parsing Word  with the lexical semantics for Word"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Form a proposition , P  , by existentially quantifying over the lambda abstracted variables ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Assert P  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "If P  does not entail P  retract P  and all conclusions made from it ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Judge the plausibility of P  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "If implausible block this derivation ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It is worth noting that the need for retraction is not due to a failure to extract the correct ` least commitment ' proposition from the semantic content of the fragment Mary introduced ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This is due to the fact that it is possible to find pairs of possible continuations which are the negation of each other ( e.g. Mary introduced noone to anybody and Mary introduced someone to somebody ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The only proposition compatible with both a proposition , p , and its negation ,  p is the trivial proposition , T ( see"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "So far we have only considered semantic representations which do not involve quantifiers ( except for the existential quantifier introduced by the mechanism above ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In sentences with two or more quantifiers , there is generally an ambiguity concerning which quantifier has wider scope ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , in sentence  below the preferred reading is for the same kid to have climbed every tree ( i.e. the universal quantifier is within the scope of the existential ) whereas in sentence  the preferred reading is where the universal quantifier has scope over the existential ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Scope preferences sometimes seem to be established before the end of a sentence ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , in sentence  below , there seems a preference for an outer scope reading for the first quantifier as soon as we interpret child ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In  the preference , by the time we get to e.g. grammar , is for an inner scope reading for the first quantifier ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This intuitive evidence can be backed up by considering garden path effects with quantifier scope ambiguities ( called jungle paths by  ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The original examples , such as the following ,"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "showed that preferences for a particular scope are established and are overturned ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "To show that preferences are sometimes established before the end of a sentence , and before a potential sentence end , we need to show garden path effects in examples such as the following :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Most psycholinguistic experimentation has been concerned with which scope preferences are made , rather than the point at which the preferences are established  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Given the intuitive evidence , our hypothesis is that scope preferences can sometimes be established early , before the end of a sentence ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This leaves open the possibility that in other cases , where the scoping information is not particularly of interest to the hearer , preferences are determined late , if at all ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Dealing with quantifiers incrementally is a rather similar problem to dealing with fragments of trees incrementally ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Just as it is impossible to predict the level of embedding of a noun phrase such as John from the fragment Mary thinks John , it is also impossible to predict the scope of a quantifier in a fragment with respect to the arbitrarily large number of quantifiers which might appear later in the sentence ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Again the problem can be avoided by a form of packing ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A particularly simple way of doing this is to use unscoped logical forms where quantifiers are left in situ ( similar to the representations used by  , or to Quasi Logical Form  ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , the fragment Every man gives a book can be given the following representation :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Each quantified term consists of a quantifier , a variable and a restrictor , but no body ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "To convert lambda expressions to unscoped propositions , we replace an occurrence of each argument with an empty existential quantifier term ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In this case we obtain :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Scoped propositions can then be obtained by using an outside-in quantifier scoping algorithm  , or an inside-out algorithm with a free variable constraint  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The propositions formed can then be judged for plausibility ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "To imitate jungle path phenomena , these plausibility judgements need to feed back into the scoping procedure for the next fragment ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , if every man is taken to be scoped outside a book after processing the fragment Every man gave a book , then this preference should be preserved when determining the scope for the full sentence Every man gave a book to a child ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Thus instead of doing all quantifier scoping at the end of the sentence , each new quantifier is scoped relative to the existing quantifiers ( and operators such as negation , intensional verbs etc ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A preliminary implementation achieves this by annotating the semantic representations with node names , and recording which quantifiers are ` discharged ' at which nodes , and in which order ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Dynamic semantics adopts the view that `` the meaning of a sentence does not lie in its truth conditions , but rather in the way in which it changes ( the representation of ) the information of the interpreter ''  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "At first glance such a view seems ideally suited to incremental interpretation ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Indeed ,"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Putting these two quotes together is , however , misleading , since it suggests a more direct mapping between incremental semantics and dynamic semantics than is actually possible ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In an incremental semantics , we would expect the information state of an interpreter to be updated word by word ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In contrast , in dynamic semantics , the order in which states are updated is determined by semantic structure , not by left-to-right order ( see e.g.  for discussion ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , in Dynamic Predicate Logic  , states are threaded from the antecedent of a conditional into the consequent , and from a restrictor of a quantifier into the body ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Thus , in interpreting ,"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "the input state for evaluation of John will buy it right away is the output state from the antecedent a car impresses him ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In this case the threading through semantic structure is in the opposite order to the order in which the two clauses appear in the sentence ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Some intuitive justification for the direction of threading in dynamic semantics is provided by considering appropriate orders for evaluation of propositions against a database : the natural order in which to evaluate a conditional is first to add the antecedent , and then see if the consequent can be proven ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It is only at the sentence level in simple narrative texts that the presentation order and the natural order of evaluation necessarily coincide ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The ordering of anaphors and their antecedents is often used informally to justify left-to-right threading or threading through semantic structure ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "However , threading from left-to-right disallows examples of optional cataphora , as in example  , and examples of compulsory cataphora as in :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Similarly , threading from the antecedents of conditionals into the consequent fails for examples such as :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It is also possible to get sentences with ` donkey ' readings , but where the indefinite is in the consequent :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This sentence seems to get a reading where we are not talking about a particular student ( an outer existential ) , or about a typical student ( a generic reading ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Moreover , as noted by  , the use of any kind of ordered threading will tend to fail for Bach-Peters sentences , such as :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For this kind of example , it is still possible to use a standard dynamic semantics , but only if there is some prior level of reference resolution which reorders the antecedents and anaphors appropriately ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , if  is converted into the ` donkey ' sentence :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "When we consider threading of possible worlds , as in Update Semantics  , the need to distinguish between the order of evaluation and the order of presentation becomes more clear cut ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Consider trying to perform threading in left-to-right order during interpretation of the sentence , John left if Mary left ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "After processing the proposition John left the set of worlds is refined down to those worlds in which John left ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Now consider processing if Mary left ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Here we want to reintroduce some worlds , those in which neither Mary or John left ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "However , this is not allowed by Update Semantics which is eliminative : each new piece of information can only further refine the set of worlds ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It is worth noting that the difficulties in trying to combine eliminative semantics with left-to-right threading apply to constraint-based semantics as well as to Update Semantics ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "uses incremental refinement of sets of possible referents ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "For example , the effect of processing the rabbit in the noun phrase the rabbit in the hat is to provide a set of all rabbits ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The processing of in refines this set to rabbits which are in something ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Finally , processing of the hat refines the set to rabbits which are in a hat ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "However , now consider processing the rabbit in none of the boxes ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "By the time the rabbit in has been processed , the only rabbits remaining in consideration are rabbits which are in something ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This incorrectly rules out the possibility of the noun phrase referring to a rabbit which is in nothing at all ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The case is actually a parallel to the earlier example of Mary introduced someone to something being inappropriate if the final sentence is Mary introduced noone to anybody ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Although this discussion has argued that it is not possible to thread the states which are used by a dynamic or eliminative semantics from left to right , word by word , this should not be taken as an argument against the use of such a semantics in incremental interpretation ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "What is required is a slightly more indirect approach ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In the present implementation , semantic structures ( akin to logical forms ) are built word by word , and each structure is then evaluated independently using a dynamic semantics ( with threading performed according to the structure of the logical form ) ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "At present there is a limited implementation , which performs a mapping from sentence fragments to fully scoped logical representations ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "To illustrate its operation , consider the following discourse :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "We assume that the first sentence has been processed , and concentrate on processing the fragment ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The implementation consists of five modules :"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A word-by-word incremental parser for a lexicalised version of dependency grammar  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "This takes fragments of sentences and maps them to unscoped logical forms ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A module which replaces lambda abstracted variables with existential quantifiers in situ ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "A pronoun coindexing procedure which replaces pronoun variables with a variable from the same sentence , or from the preceding context ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "An outside-in quantifier scoping algorithm based on  ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "An ` evaluation ' procedure based on  , which takes a logical form containing free variables ( such as the w in the LF above ) , and evaluates it using a dynamic semantics in the context given by the preceding sentences ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The output is a new logical form representing the context as a whole , with all variables correctly bound ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "At present , the coverage of module  is limited , and module  is a naive coindexing procedure which allows a pronoun to be coindexed with any quantified variable or proper noun in the context or the current sentence ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "The paper described some potential applications of incremental interpretation ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "It then described the series of steps required in mapping from initial fragments of sentences to propositions which can be judged for plausibility ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Finally , it argued that the apparently close relationship between the states used in incremental semantics and dynamic semantics fails to hold below the sentence level , and briefly presented a more indirect way of using dynamic semantics in incremental interpretation ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Why should computers interpret language incrementally ?"}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In recent years psycholinguistic evidence for incremental interpretation has become more and more compelling , suggesting that humans perform semantic interpretation before constituent boundaries , possibly word by word ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "However , possible computational applications have received less attention ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "In this paper we consider various potential applications , in particular graphical interaction and dialogue ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "We then review the theoretical and computational tools available for mapping from fragments of sentences to fully scoped semantic representations ."}
 {"title": "Incremental Interpretation : Applications , Theory , and Relationship to Dynamic Semantics", "sentence": "Finally , we tease apart the relationship between dynamic semantics and incremental interpretation ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "A discourse strategy is a strategy for communicating with another agent ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Agents make strategy choices via decisions about when to talk , when to let the other agent talk , what to say , and how to say it ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "One choice a conversational agent must make is whether an utterance should include some relevant , but optional , information in what is communicated ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "For example , consider  :"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The speaker made a strategic choice in  to include  since she could have simply said  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "What determines the speaker 's choice ?"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Existing dialogue systems have two modes for dealing with optional information :"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "include all optional information that is not already known to the hearer ;"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "include no optional information  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "But these modes are simply the extremes of possibility and to my knowledge , no previous work has proposed any principles for when to include optional information , or any way of testing the proposed principles to see how they are affected by the conversants and their processing abilities , by the task , by the communication channel , or by the domain ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This paper presents a new experimental method for determining whether a discourse strategy is effective and presents experimental results on a strategy for supporting deliberation ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The method is based on earlier simulation work by ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Section  outlines hypotheses about the factors that affect which strategies are effective ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Section  presents a new method for testing the role of the hypothesized factors ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The experimental results in section  show that effective strategies to support deliberation are determined by both cognitive and task variables ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Deliberation is the process by which an agent decides what to believe and what to do ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "One strategy that supports deliberation is the Explicit-Warrant strategy , as in  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The WARRANT in  can be used by the hearer in deliberating whether to ACCEPT or REJECT the speaker 's PROPOSAL in  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "An analysis of proposals in a corpus of 55 problem-solving dialogues shows that communicating agents don't always include warrants in a proposal , and suggest a number of hypotheses about which factors affect their decision ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Consider a situation in which an agent A wants an agent B to accept a proposal P ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If B is a ` helpful ' agent ( nonautonomous ) , B will accept A 's proposal without a warrant ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Alternatively , if B deliberates whether to accept P , but B knows of no competing options , then P will be the best option whether or not A tells B the warrant for P ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Since a warrant makes the dialogue longer , the Explicit-Warrant strategy might be inefficient whenever either of these situations hold ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Now consider a situation where B is an autonomous agent  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "B always deliberates every proposal and B probably knows of options which compete with proposal P. Then B cannot decide whether to accept P without a warrant ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Supposedly agent A should omit a warrant is if it is already believed by B , so that the speaker in  would not have said It 's shorter if she believed that the hearer knew that the Walnut St. route was shorter ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "However , consider  , said in discussing which Indian restaurant to go to for lunch :"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The warrant in  was included despite the fact that it was common knowledge among the conversants ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Its inclusion violates the rule of Don't tell people facts that they already know ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Clearly the rule does not hold ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "These already-known warrants are a type of INFORMATIONALLY REDUNDANT UTTERANCE , henceforth IRU , which are surprisingly frequent in naturally-occurring dialogue  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "A Warrant IRU such as that in  suggests that B 's cognitive limitations may be a factor in what A chooses to say , so that even if B knows a warrant for adopting A 's proposal , what is critical is whether the warrant is salient for B , i.e. whether the warrant is already accessible in B 's working memory ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If the warrant is not already salient , then B must either infer or retrieve the warrant information or obtain it from an external source in order to evaluate A 's proposal ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Thus A 's strategy choice may depend on A 's model of B 's attentional state , as well as the costs of retrieval and inference as opposed to communication ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In other words , A may decide that it is easier to just say the warrant rather than require B to infer or retrieve it ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Finally , the task determines whether there are penalties for leaving a warrant implicit and relying on B to infer or retrieve it ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Some tasks require that two agents agree on the reasons for adopting a proposal , e.g. in order to ensure robustness in situations of environmental change ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Other tasks , such as a management / union negotiation , only require the agents to agree on the actions to be carried out and each agent can have its own reasons for wanting those actions to be done without affecting success in the task ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Figure  summarizes these hypotheses by proposing a hypothetical decision tree for an agent 's choice of whether to use the Explicit-Warrant strategy ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The choice is hypothesized to depend on cognitive properties of B , e.g. what B knows , B 's attentional state , and B 's processing capabilities , as well as properties of the task and the communication channel ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "To my knowledge , all previous work on dialogue has simply assumed that an agent should never tell an agent facts that the other agent already knows ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The hypotheses in figure  seem completely plausible , but the relationship of cognitive effort to dialogue behavior has never been explored ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Given these hypotheses , what is required is a way to test the hypothesized relationship of task and cognitive factors to effective discourse strategies ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Section  describes a new method for testing hypotheses about effective discourse strategies in dialogue ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Design-World is an experimental environment for testing the relationship between discourse strategies , task parameters and agents ' cognitive capabilities , similar to the single agent TileWorld simulation environment ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Design-World agents can be parametrized as to discourse strategy , and the effects of this strategy can be measured against a range of cognitive and task parameters ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This paper compares the Explicit-Warrant strategy to the All-Implicit strategy as strategies for supporting deliberation ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Other strategies tested in Design-World are presented elsewhere  , ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The Design-World task requires two agents to carry out a dialogue in order to negotiate an agreement on the design of the floor plan of a two room house  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The DESIGN-HOUSE plan requires the agents to agree on how to DESIGN-ROOM - 1 and DESIGN-ROOM - 2 ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Both agents know what the DESIGN-HOUSE plan requires and start out with a set of furniture pieces that can be used to design each room ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "To negotiate an agreement , each agent carries out means-end reasoning about the furniture pieces that they have that can be used in the floor plan ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Means-end reasoning generates OPTIONS - these options are the content of PROPOSALS to the other agent to PUT a piece of furniture into one of the rooms ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Dialogue  illustrates agents ' communication for part of designing room - 1 , including both the artificial language that the agents communicate with and a gloss generated from that language in italics :"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "On receiving a proposal , an agent deliberates whether to ACCEPT or REJECT the proposal  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "As potential warrants to support deliberation , and to provide a way of objectively evaluating agents ' performance , each piece of furniture has a score ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The score propositions for all the pieces of furniture are stored in both agents ' memories at the beginning of the dialogue ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Agents REJECT a proposal if deliberation leads them to believe that they know of a better option or if they believe the preconditions for the proposal do not hold ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The content of rejections is determined by the COLLABORATIVE PLANNING PRINCIPLES , abstracted from analyzing four different types of problem solving dialogues ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "For example , in  Kim rejects the proposal in  , and gives as her reason that option - 56 is a counter-proposal ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Proposals  and  are inferred to be implicitly ACCEPTED because they are not rejected ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If a proposal is ACCEPTED , either implicitly or explicitly , then the option that was the content of the proposal becomes a mutual intention that contributes to the final design plan ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "A potential final design plan negotiated via a dialogue is shown in figure  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The Design-World experiments reported here compare the All-Implicit strategy with the Explicit-Warrant strategy ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Agents are parametrized for different discourse strategies by placing different expansions of discourse plans in their plan libraries ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Discourse plans are plans for PROPOSAL , REJECTION , ACCEPTANCE , CLARIFICATION , OPENING and CLOSING ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The only variations discussed here are variations in the expansions of PROPOSALS ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The All-Implicit strategy is an expansion of a discourse plan to make a PROPOSAL , in which a PROPOSAL decomposes trivially to the communicative act of PROPOSE ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In dialogue  , both Design-World agents communicate using the All-Implicit strategy , and the proposals are shown in utterances  ,  , and  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The All-Implicit strategy never includes warrants in proposals , leaving it up to the other agent to retrieve them from memory ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The Explicit-Warrant strategy expands the PROPOSAL discourse act to be a WARRANT followed by a PROPOSE utterance ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Since agents already know the point values for pieces of furniture , warrants are always IRUs in the experiments here ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "For example ,  is a WARRANT for the proposal in  : The names of agents who use the Explicit-Warrant strategy are a numbered version of the string `` IEI '' to help the experimenter keep track of the simulation data files ; IEI stands for Implicit acceptance , Explicit warrant , Implicit opening and closing ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The fact that the green rug is worth 56 points supports deliberation about whether to adopt the intention of putting the green rug in the study ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The Explicit-Warrant strategy models naturally occurring examples such as those in  because the points information used by the hearer to deliberate whether to accept or reject the proposal is already mutually believed ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Section  introduced a range of factors motivated by the corpus analysis that were hypothesized to determine when Explicit-Warrant is an effective strategy ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This section discusses how Design-World supports the parametrization of these factors ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The agent architecture for deliberation and means-end reasoning is based on the IRMA architecture , also used in the TileWorld simulation environment  , with the addition of a model of limited Attention / Working memory , AWM ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "includes a fuller discussion of the Design-World deliberation and means-end reasoning mechanism and the underlying mechanisms assumed in collaborative planning ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "We hypothesized that a warrant must be SALIENT for both agents ( as shown by example  ) ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In Design-World , salience is modeled by AWM model , adapted from  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "While the AWM model is extremely simple ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "AWM consists of a three dimensional space in which propositions acquired from perceiving the world are stored in chronological sequence according to the location of a moving memory pointer ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The sequence of memory loci used for storage constitutes a random walk through memory with each loci a short distance from the previous one ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If items are encountered multiple times , they are stored multiple times  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "When an agent retrieves items from memory , search starts from the current pointer location and spreads out in a spherical fashion ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Search is restricted to a particular search radius : radius is defined in Hamming distance ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "For example if the current memory pointer loci is  , the loci distance 1 away would be  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The actual locations are calculated modulo the memory size ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The limit on the search radius defines the capacity of attention / working memory and hence defines which stored beliefs and intentions are SALIENT ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The radius of the search sphere in the AWM model is used as the parameter for Design-World agents ' resource-bound on attentional capacity ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In the experiments below , memory is 16x16x16 and the radius parameter varies between 1 and 16 , where AWM of 1 gives severely attention limited agents and AWM of 16 means that everything an agent knows is accessible ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This parameter lets us distinguish between an agent 's ability to access all the information stored in its memory , and the effort involved in doing so ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The advantages of the AWM model is that it was shown to reproduce , in simulation , many results on human memory and learning ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Because search starts from the current pointer location , items that have been stored most recently are more likely to be retrieved , predicting recency effects  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Because items that are stored in multiple locations are more likely to be retrieved , the model predicts frequency effects  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Because items are stored in chronological sequence , the model produces natural associativity effects  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Because deliberation and means-end reasoning can only operate on salient beliefs , limited attention produces a concomitant inferential limitation , i.e. if a belief is not salient it cannot be used in deliberation or means-end-reasoning ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This means that mistakes that agents make in their planning process have a plausible cognitive basis ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Agents can both fail to access a belief that would allow them to produce an optimal plan , as well as make a mistake in planning if a belief about how the world has changed as a result of planning is not salient ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Depending on the preceding discourse , and the agent 's attentional capacity , the propositions that an agent knows may or may not be salient when a proposal is made ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Another hypothetical factor was the relative cost of retrieval and communication ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "AWM also gives us a way to measure the number of retrievals from memory in terms of the number of locations searched to find a proposition ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The amount of effort required for each retrieval step is a parameter , as is the cost of each inference step and the cost of each communicated message ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "These cost parameters support modeling various cognitive architectures , e.g. varying the cost of retrieval models different assumptions about memory ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "For example , if retrieval is free then all items in working memory are instantly accessible , as they would be if they were stored in registers with fast parallel access ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If AWM is set to 16 , but retrieval isn't free , the model approximates slow spreading activation that is quite effortful , yet the agent still has the ability to access all of memory , given enough time ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If AWM is set lower than 16 and retrieval isn't free , then we model slow spreading activation with a timeout when effort exceeds a certain amount , so that an agent does not have the ability to access all of memory ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "It does not make sense to fix absolute values for the retrieval , inference and communication cost parameters in relation to human processing ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "However , Design-World supports exploring issues about the relative costs of various processes ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "These relative costs might vary depending on the language that the agents are communicating with , properties of the communication channel , how smart the agents are , how much time they have , and what the demands of the task are  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Below we vary the relative cost of communication and retrieval ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Finally , we hypothesized that the Explicit-Warrant strategy may be beneficial if the relationship between the warrant and the proposal must be mutually believed ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Thus the definition of success for the task is a Design-World parameter : the Standard task does not require a shared warrant , whereas the Zero NonMatching Beliefs task gives a zero score to any negotiated plan without agreed-upon warrants ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "To evaluate PERFORMANCE , we compare the Explicit-Warrant strategy with the All-Implicit strategy in situations where we vary the task requirements , agents ' attentional capacity , and the cost of retrieval , inference and communication ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Evaluation of the resulting DESIGN-HOUSE plan is parametrized by"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "COMMCOST : cost of sending a message ;"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "INFCOST : cost of inference ; and"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "RETCOST : cost of retrieval from memory :"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "PERFORMANCE =  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "RAW SCORE is task specific : in the Standard task we simply summarize the point values of the furniture pieces in each PUT-ACT in the final Design , while in the Zero NonMatching Beliefs task , agents get no points for a plan unless they agree on the reasons underlying each action that contributes to the plan ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The way PERFORMANCE is defined reflects the fact that agents are meant to collaborate on the task ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The costs that are deducted from the RAW SCORE are the costs for both agents ' communication , inference , and retrieval ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Thus PERFORMANCE is a measure of LEAST COLLABORATIVE EFFORT ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Since the parameters for cognitive effort are fixed while discourse strategy and AWM settings are varied , we can directly test the benefits of different discourse strategies under different assumptions about cognitive effort and the cognitive demands of the task ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This is impossible to do with corpus analysis alone ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "We simulate 100 dialogues at each parameter setting for each strategy ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Differences in performance distributions are evaluated for significance over the 100 dialogues using the Kolmogorov-Smirnov ( KS ) two sample test  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "A strategy A is BENEFICIAL as compared to a strategy B , for a set of fixed parameter settings , if the difference in distributions using the Kolmogorov-Smirnov two sample test is significant at p # LT .05 , in the positive direction , for two or more AWM settings ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "A strategy is DETRIMENTAL if the differences go in the negative direction ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Strategies may be neither BENEFICIAL or DETRIMENTAL , as there may be no difference between two strategies ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This section discusses the results of comparing the Explicit-Warrant discourse strategy with the All-Implicit discourse strategy to determine when each strategy is BENEFICIAL ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "We test 4 factors outlined in figure  : when the warrant is salient or not , when the warrant is required for the task or not , when the costs of retrieval and communication vary , and when retrieval is indeterminate ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Differences in performance between the Explicit-Warrant strategy and the All-Implicit strategy are shown via a DIFFERENCE PLOT such as figure  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In figure  performance differences are plotted on the Y-axis and AWM settings are shown on the X-axis ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If the plot is above the dotted line for 2 or more AWM settings , then the Explicit-Warrant strategy may be BENEFICIAL depending on whether the differences are significant by the KS test ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Each point represents the difference in the means of 100 runs of each strategy at a particular AWM setting ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "These plots summarize the results of 1800 simulated dialogues : 100 for each AWM setting for each strategy ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Dialogues in which one or both agents use the Explicit-Warrant strategy are more efficient when retrieval has a cost ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Figure  shows that the Explicit-Warrant strategy is DETRIMENTAL at AWM of 3,4,5 for the Standard task , in comparison with the All-Implicit strategy , if retrieval from memory is free ( KS 3,4,5 # GT .19 , p # LT .05 ) ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This is because making the warrant salient displaces information about other pieces of furniture when agents are attention-limited ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In the Standard task , agents aren't required to share beliefs about the value of a proposal , so remembering what pieces they have is more important than remembering their value ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "However , figure  shows that Explicit-Warrant is beneficial when retrieval is one tenth the cost of communication and inference ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "By AWM values of 3 , performance with Explicit-Warrant is better than All-Implicit because the beliefs necessary for deliberation are made salient with each proposal ( KS for AWM of 3 and above # GT .23 , p # LT .01 ) ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "At AWM parameter settings of 16 , where agents have the ability to search all their beliefs for warrants , the saving in processing time is substantial ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Again at the lowest AWM settings , the strategy is not beneficial because it displaces information about other pieces from AWM ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "However in figure  , in contrast with figure  , retrieval has an associated cost ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Thus the savings in retrieval balance out with the loss of raw score so that the strategy is not DETRIMENTAL ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Other experiments show that even when the relative cost of retrieval is .0001 , that Explicit-Warrant is still beneficial at AWM settings of 11 and 16 ( KS for 11,16 # GT .23 , p # LT .01 ) ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If we change the relative costs of the different processes in the situation , we change whether a strategy is beneficial ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Figure  shows that if communication cost is 10 , and inference and retrieval are free , then the Explicit-Warrant strategy is DETRIMENTAL ( KS for AWM 1 to 5 # GT .23 , p # LT .01 ) ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This is because the Explicit-Warrant strategy increases the number of utterances required to perform the task ; it doubles the number of messages in every proposal ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If communication is expensive compared to retrieval , communication cost can dominate the other benefits ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If we change the definition of success in the task , we change whether a strategy is beneficial ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "When the task is Zero-Nonmatching-Beliefs , the Explicit-Warrant strategy is beneficial even if retrieval is free ( KS # GT .23 for AWM from 2 to 11 , p # LT .01 ) The warrant information that is redundantly provided is exactly the information that is needed in order to achieve matching beliefs about the warrants for intended actions ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The strategy virtually guarantees that the agents will agree on the reasons for carrying out a particular course of action ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The fact that retrieval is indeterminate produces this effect ; a similar result is obtained when warrants are required and retrieval costs something ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "To my great surprise , the beneficial effect of Explicit-Warrant for the Zero-NonMatching-Beliefs task is so robust that even if communication cost is 10 and retrieval and inference are free , Explicit-Warrant is better than All-Implicit at AWM of 3  11 ( KS # GT .23 , p # LT .01 ) ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "See figure  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In other words , even when every extra WARRANT message incurs a penalty of 10 points , if the task is Zero-NonMatching-Beliefs , agents using Explicit-Warrant do better ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Contrast figure  with the Standard task and same cost parameters in  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "These result suggests that including warrants is highly effective when agents must agree on a specific warrant , if they are attention-limited to any extent ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "This paper has discussed an instance of a general problem in the design of conversational agents : when to include optional information ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "We presented and tested a number of hypotheses about the factors that contribute to the decision of when to include a warrant in a proposal ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "We showed that warrants are useful when the task requires agreement on the warrant , when the warrant is not currently salient , when retrieval of the warrant is indeterminate , or when retrieval has some associated cost , and that warrants hinder performance if communication is costly and if the warrant can displace information that is needed to complete the task , e.g. when AWM is very limited and warrants are not required to be shared ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The method used here is a new experimental methodology for computational linguistics that supports testing hypotheses about beneficial discourse strategies ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The Design-World environment is based on a cognitive model of limited attention and supports experiments on the interaction of discourse strategies with agents ' cognitive limitations ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The use of the method and the focus of this work are novel : previous work has focused on determining underlying mechanisms for cooperative strategies rather than on investigating when a strategy is effective ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "To my knowledge , no previous work on dialogue has ever argued that conversational agents ' resource limits are a major factor in determining effective conversational strategies in collaboration ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The results presented here suggest that cooperative strategies cannot be defined in the abstract , but cooperation arises from the interaction of two agents in dialogue ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "If one agent has limited working memory , then the other agent can make the dialogue go more smoothly by adopting a strategy that makes deliberative premises salient ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "In other words , strategies are cooperative for certain conversational partners , under particular task definitions , for particular communication situations ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Here we compared two discourse strategies : All-Implicit and Explicit-Warrant ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Explicit-Warrant is a type of discourse strategy called an Attention strategy in  because its main function is to manipulate agents ' attentional state ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Elsewhere we show that"}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "some IRU strategies are only beneficial when inferential complexity is higher than in the Standard Task ,  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "IRUs that make inferences explicit can help inference limited agents perform as well as logically omniscient ones  ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Although much work remains to be done , there is reason to believe that these results are domain independent ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The simplicity of the Design-World task means that its structure is a subcomponent of many other tasks ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The model of limited resources is cognitively based , but the cost parameters support modeling different agent architectures , and we explored the effects of different cost parameters ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The Explicit-Warrant strategy is based on simple relationships between different facts which we would expect to occur in any domain , i.e. the fact that some belief can be used as a WARRANT for accepting a proposal should occur in almost any task ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Future work should extend these results , showing that a ` cooperative strategy ' need not always be ` cooperative ' , and investigate additional factors that determine when strategies are effective ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "A discourse strategy is a strategy for communicating with another agent ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "Designing effective dialogue systems requires designing agents that can choose among discourse strategies ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "We claim that the design of effective strategies must take cognitive factors into account , propose a new method for testing the hypothesized factors , and present experimental results on an effective strategy for supporting deliberation ."}
 {"title": "Discourse and Deliberation : Testing a Collaborative Strategy", "sentence": "The proposed method of computational dialogue simulation provides a new empirical basis for computational linguistics ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In natural language processing filtering is used to weed out those search paths that are redundant , i.e. , are not going to be used in the proof tree corresponding to the natural language expression to be generated or parsed ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Filter optimization often comprises an extension of a specific processing strategy such that it exploits specific knowledge about grammars and / or the computational task ( s ) that one is using them for ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "At the same time it often remains unclear how these optimizations relate to each other and what they actually mean ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In this paper I show how starting from a definite clause characterization of filtering derived automatically from a logic grammar using Magic compilation , filter optimizations can be performed in a processor independent and logically clean fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Magic ( templates ) is a general compilation technique for efficient bottom-up evaluation of logic programs developed in the deductive database community  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Given a logic program , Magic produces a new program in which the filtering as normally resulting from top-down evaluation is explicitly characterized through , so-called , magic predicates , which produce variable bindings for filtering when evaluated bottom-up ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The original rules of the program are extended such that these bindings can be made effective ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "As a result of the definite clause characterization of filtering , Magic brings filtering into the logic underlying the grammar ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "I discuss two filter optimizations ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "These optimizations are direction independent in the sense that they are useful for both generation and parsing ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "For expository reasons , though , they are presented merely on the basis of examples of generation ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Magic compilation does not limit the information that can be used for filtering ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This can lead to nontermination as the tree fragments enumerated in bottom-up evaluation of magic compiled grammars are connected  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "More specifically , 'magic generation ' falls prey to non-termination in the face of head recursion , i.e. , the generation analog of left recursion in parsing ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This necessitates a dynamic processing strategy , i.e. , memoization , extended with an abstraction function like , e.g. , restriction  , to weaken filtering and a subsumption check to discard redundant results ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It is shown that for a large class of grammars the subsumption check which often influences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic predicates derived for a particular grammar after applying an abstraction function in an off-line fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Unfolding can be used to eliminate superfluous filtering steps ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Given an off-line optimization of the order in which the right-hand side categories in the rules of a logic grammar are processed  the resulting processing behavior can be considered a generalization of the head corner generation approach  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Without the need to rely on notions such as semantic head and chain rule , a head corner behavior can be mimicked in a strict bottom-up fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Many approaches focus on exploiting specific knowledge about grammars and / or the computational task ( s ) that one is using them for by making filtering explicit and extending the processing strategy such that this information can be made effective ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In generation , examples of such extended processing strategies are head corner generation with its semantic linking  or bottom-up ( Earley ) generation with a semantic filter  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Even though these approaches often accomplish considerable improvements with respect to efficiency or termination behavior , it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "By bringing filtering into the logic underlying the grammar it is possible to show in a perspicuous and logically clean way how and why filtering can be optimized in a particular fashion and how various approaches relate to each other ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Magic makes filtering explicit through characterizing it as definite clauses ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Intuitively understood , filtering is reversed as binding information that normally becomes available as a result of top-down evaluation is derived by bottom-up evaluation of the definite clause characterization of filtering ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The following is the basic Magic algorithm taken from  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Let P be a program and  a query on the program ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "We construct a new program  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Initially  is empty ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Create a new predicate magic_p for each predicate p in P ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The arity is that of p ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "For each rule in P , add the modified version of the rule to  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "If rule r has head , say , p (  ) , the modified version is obtained by adding the literal  to the body ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "For each rule r in P with head , say , p (  ) , and for each literal  in its body , add a magic rule to  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The head is  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The body contains the literal magic_p (  ) , and all the literals that precede  in the rule ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Create a seed fact magic_q (  ) from the query ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "To illustrate the algorithm I zoom in on the application of the above algorithm to one particular grammar rule ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Suppose the original grammar rule looks as follows :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Step  of the algorithm results in the following modified version of the original grammar rule :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "A magic literal is added to the right-hand side of the rule which 'guards ' the application of the rule ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This does not change the semantics of the original grammar as it merely serves as a way to incorporate the relevant bindings derived with the magic predicates to avoid redundant applications of a rule ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Corresponding to the first right-hand side literal in the original rule step  derives the following magic rule :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It is used to derive from the guard for the original rule a guard for the rules defining the first right-hand side literal ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The second right-hand side literal in the original rule leads to the following magic rule :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Finally , step  of the algorithm ensures that a seed is created ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Assuming that the original rule is defining the start category , the query corresponding to the generation of the s `` John buys Mary a book '' leads to the following seed :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The seed constitutes a representation of the initial bindings provided by the query that is used by the magic predicates to derive guards ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Note that the creation of the seed can be delayed until run-time , i.e. , the grammar does not need to be recompiled for every possible query ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Magic compilation is illustrated on the basis of the simple logic grammar extract in figure  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This grammar has been optimized automatically for generation  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The right-hand sides of the rules are reordered such that a simple left-to-right evaluation order constitutes the optimal evaluation order ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "With this grammar a simple top-down generation strategy does not terminate as a result of the head recursion in rule  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It is necessary to use memoization extended with an abstraction function and a subsumption check ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Strict bottom-up generation is not attractive either as it is extremely inefficient :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "One is forced to generate all possible natural language expressions licensed by the grammar and subsequently check them against the start category ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It is possible to make the process more efficient through excluding specific lexical entries with a semantic filter ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The use of such a semantic filter in bottom-up evaluation requires the grammar to obey the semantic monotonicity constraint in order to ensure completeness  ( see below ) ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The 'magic - compiled grammar ' in figure  is the result of applying the algorithm in the previous section to the head-recursive example grammar and subsequently performing two optimizations  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "All ( calls to ) magic predicates corresponding to lexical entries are removed ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Furthermore , data-flow analysis is used to fine-tune the magic predicates for the specific processing task at hand , i.e. , generation ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Given a user-specified abstract query , i.e. , a specification of the intended input  those arguments which are not bound and which therefore serve no filtering purpose are removed ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The modified versions of the original rules in the grammar are adapted accordingly ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The effect of taking data-flow into account can be observed by comparing the rules for magic_vp and magic_np in the previous section with rule  and  in figure  , respectively ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Figure  shows the results from generation of the sentence `` John buys Mary a book '' ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In the case of this example the seed looks as follows :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The facts , i.e. , passive edges / items , in figure  resulted from semi-naive bottom-up evaluation  which constitutes a dynamic bottom-up evaluation , where repeated derivation of facts from the same earlier derived facts ( as in naive evaluation ;  ) is blocked ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "( Active edges are not memoized ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": ") The figure consist of two tree structures ( connected through dotted lines ) of which the left one corresponds to the filtering part of the derivation ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The filtering tree is reversed and derives magic facts starting from the seed in a bottom-up fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The tree on the right is the proof tree for the example sentence which is built up as a result of unifying in the derived magic facts when applying a particular rule ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "E.g. , in order to derive fact  , magic fact  is unified with the magic literal in the modified version of rule  ( in addition to the facts  and  ) ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This , however , is not represented in order to keep the figure clear ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Dotted lines are used to represent when 'normal ' facts are combined with magic facts to derive new magic facts ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "As can be reconstructed from the numbering of the facts in figure  the resulting processing behavior is identical to the behavior that would result from Earley generation as in  except that the different filtering steps are performed in a bottom-up fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In order to obtain a generator similar to the bottom-up generator as described in  the compilation process can be modified such that only lexical entries are extended with magic literals ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Just like in case of"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "As a result of characterizing filtering by a definite clause representation Magic brings filtering inside of the logic underlying the grammar ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This allows it to be optimized in a processor independent and logically clean fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "I discuss two possible filter optimizations based on a program transformation technique called unfolding  also referred to as partial execution  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Just like top-down evaluation of the original grammar bottom-up evaluation of its magic compiled version falls prey to non-termination in the face of head recursion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It is however possible to eliminate the subsumption check through fine-tuning the magic predicates derived for a particular grammar in an off-line fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In order to illustrate how the magic predicates can be adapted such that the subsumption check can be eliminated it is necessary to take a closer look at the relation between the magic predicates and the facts they derive ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In figure  the relation between the magic predicates for the example grammar is represented by an unfolding tree  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This , however , is not an ordinary unfolding tree as it is constructed on the basis of an abstract seed , i.e. , a seed adorned with a specification of which arguments are to be considered bound ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Note that an abstract seed can be derived from the user-specified abstract query ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Only the magic part of the abstract unfolding tree is represented ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The abstract unfolding tree in figure  clearly shows why there exists the need for subsumption checking :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Rule  in figure  produces infinitely many magic_vp facts ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This 'cyclic ' magic rule is derived from the head-recursive vp rule in the example grammar ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "There is however no reason to keep this rule in the magic-compiled grammar ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It influences neither the efficiency of processing with the grammar nor the completeness of the evaluation process ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Finding these types of cycles in the magic part of the compiled grammar is in general undecidable ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It is possible though to ` trim ' the magic predicates by applying an abstraction function ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "As a result of the explicit representation of filtering we do not need to postpone abstraction until run-time , but can trim the magic predicates off-line ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "One can consider this as bringing abstraction into the logic as the definite clause representation of filtering is weakened such that only a mild form of connectedness results which does not affect completeness  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Consider the following magic rule :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This is the rule that is derived from the head-recursive vp rule when the partially specified subcategorization list is considered as filtering information ( cf. , fn. 1 ) ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The rule builds up infinitely large subcategorization lists of which eventually only one is to be matched against the subcategorization list of , e.g. , the lexical entry for `` buys '' ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Though this rule is not cyclic , it becomes cyclic upon off-line abstraction :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Through trimming this magic rule , e.g. , given a bounded term depth  or a restrictor  , constructing an abstract unfolding tree reveals the fact that a cycle results from the magic rule ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This information can then be used to discard the culprit ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Removing the direct or indirect cycles from the magic part of the compiled grammar does eliminate the necessity of subsumption checking in many cases ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "However , consider the magic rules  and  in figure  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Rule  is more general than rule  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Without subsumption checking this leads to spurious ambiguity :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Both rules produce a magic fact with which a subject np can be built ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "A possible solution to this problem is to couple magic rules with the modified version of the original grammar rule that instigated it ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "To accomplish this I propose a technique that can be considered the off-line variant of an indexing technique described in  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The indexing technique is illustrated on the basis of the running example :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Rule  in figure  is coupled to the modified version of the original s rule that instigated it , i.e. , rule  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Both rules receive an index :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The modified versions of the rules defining nps are adapted such that they percolate up the index of the guarding magic fact that licensed its application ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This is illustrated on the basis of the adapted version of rule  :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "As is illustrated in section  this allows the avoidance of spurious ambiguities in the absence of subsumption check in case of the example grammar ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Unfolding can also be used to collapse filtering steps ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "As becomes apparent upon closer investigation of the abstract unfolding tree in figure  the magic predicates magic_sentence , magic_s and magic_vp provide virtually identical variable bindings to guard bottom-up application of the modified versions of the original grammar rules ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Unfolding can be used to reduce the number of magic facts that are produced during processing ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "E.g. , in figure  the magic_s rule :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "can be eliminated by unfolding the magic_s literal in the modified s rule :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This results in the following new rule which uses the seed for filtering directly without the need for an intermediate filtering step :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Note that the unfolding of the magic_s literal leads to the instantiation of the argument VFORM to finite ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "As a result of the fact that there are no other magic_s literals in the remainder of the magic-compiled grammar the magic_s rule can be discarded ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This filter optimization is reminiscent of computing the deterministic closure over the magic part of a compiled grammar  at compile time ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Performing this optimization throughout the magic part of the grammar in figure  not only leads to a more succinct grammar , but brings about a different processing behavior ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Generation with the resulting grammar can be compared best with head corner generation  ( see next section ) ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "After cycle removal , incorporating relevant indexing and the collapsing of redundant magic predicates the magic-compiled grammar from figure  looks as displayed in figure  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Figure  shows the chart resulting from generation of the sentence `` John buys Mary a book '' ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The seed is identical to the one used for the example in the previous section ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The facts in the chart resulted from not-so-naive bottom-up evaluation : semi-naive evaluation without subsumption checking  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The resulting processing behavior is similar to the behavior that would result from head corner generation except that the different filtering steps are performed in a bottom-up fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The head corner approach jumps top-down from pivot to pivot in order to satisfy its assumptions concerning the flow of semantic information , i.e. , semantic chaining , and subsequently generates starting from the semantic head in a bottom-up fashion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In the example , the seed is used without any delay to apply the base case of the vp-procedure , thereby jumping over all intermediate chain and non-chain rules ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In this respect the initial reordering of rule  which led to rule  in the final grammar in figure  is crucial ( see section  ) ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "To which extent it is useful to collapse magic predicates using unfolding depends on whether the grammar has been optimized through reordering the right-hand sides of the rules in the grammar as discussed in section  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "If the s rule in the running example is not optimized , the resulting processing behavior would not have fallen out so nicely :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "In this case it leads either to an intermediate filtering step for the non-chaining sentence rule or to the addition of the literal corresponding to the subject np to all chain and non-chain rules along the path to the semantic head ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Even when cycles are removed from the magic part of a compiled grammar and indexing is used to avoid spurious ambiguities as discussed in the previous section , subsumption checking can not always be eliminated ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The grammar must be finitely ambiguous , i.e. , fulfill the off-line parsability constraint  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Furthermore , the grammar is required to obey what I refer to as the dependency constraint :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "When a particular right-hand side literal can not be evaluated deterministically , the results of its evaluation must uniquely determine the remainder of the right-hand side of the rule in which it appears ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Figure  gives a schematic example of a grammar that does not obey the dependency constraint ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Given a derived fact or seed magic_cat_1 ( property_1 ) bottom-up evaluation of the abstract grammar in ifigure  leads to spurious ambiguity ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "There are two possible solutions for cat_2 as a result of the fact that the filtering resulting from the magic literal in rule  is too unspecific ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This is not problematic as long as this nondeterminism will eventually disappear , e.g. , by combining these solutions with the solutions to cat_3 ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The problem arises as a result of the fact that these solutions lead to identical filters for the evaluation of the cat_3 literal , i.e. , the solutions to cat_2 do not uniquely determine cat_3 ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Also with respect to the dependency constraint an optimization of the rules in the grammar is important ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Through reordering the right-hand sides of the rules in the grammar the amount of nondeterminism can be drastically reduced as shown in  ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "This way of following the intended semantic dependencies the dependency constraint is satisfied automatically for a large class of grammars ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Magic evaluation constitutes an interesting combination of the advantages of top-down and bottom-up evaluation ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "It allows bottom-up filtering that achieves a goal-directedness which corresponds to dynamic top-down evaluation with abstraction and subsumption checking ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "For a large class of grammars in effect identical operations can be performed off-line thereby allowing for more efficient processing ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Furthermore , it enables a reduction of the number of edges that need to be stored through unfolding magic predicates ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The presented research was sponsored by Teilprojekt B 4 `` From Constraints to Rules :"}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Efficient Compilation of HPSG Grammars '' of the Sonderforschungsbereich 340 of the Deutsche Forschungsgemeinschaft ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The author wishes to thank Dale Gerdemann , Mark Johnson , Thilo Goetz and the anonymous reviewers for valuable comments and discussion ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Of course , the author is responsible for all remaining errors ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness ."}
 {"title": "Magic for Filter Optimization in Dynamic Bottom-up Processing", "sentence": "Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Many theories of semantic interpretation use  - term manipulation to compositionally compute the meaning of a sentence ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "These theories are usually implemented in a language such as Prolog that can simulate  - term operations with first-order unification ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "However , there are cases in which this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , Combinatory Categorial Grammar ( CCG )  is a theory of syntax and semantic interpretation that has the attractive characteristic of handling many coordination constructs that other theories cannot ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "While many aspects of CCG semantics can be reasonably simulated in first-order unification , the simulation breaks down on some of the most interesting cases that CCG can theoretically handle ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The problem in general , and for CCG in particular , is that the implementation language does not have sufficient expressive power to allow a more direct encoding ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The solution given in this paper is to show how advances in logic programming allow the implementation of semantic theories in a very direct and natural way , using CCG as a case study ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "We begin by briefly illustrating why first-order unification is inadequate for some coordination constructs , and then review two proposed solutions ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The sentence in  usually has the logical form ( LF ) in  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "CCG is one of several theories in which  gets derived by raising John to be the LF  , where P is a predicate that takes a NP as an argument to return a sentence ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Likewise , Bill gets the LF  , and coordination results in the following LF for John and Bill :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "When  is applied to the predicate ,  will result after  - reduction ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "However , under first-order unification , this needs to simulated by having the variable x in  unify both with Bill and John , and this is not possible ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "See  and  for a thorough discussion ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "suggests that the way to overcome this problem is to use explicit  - terms and encode  - reduction to perform the needed reduction ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , the logical form in  would be produced , where  is the representation of  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This would then be reduced by the clauses for apply to result in  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For this small example , writing such an apply predicate is not difficult ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "However , as the semantic terms become more complex , it is no trivial matter to write  - reduction that will correctly handle variable capture ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Also , if at some point it was desired to determine if the semantic forms of two different sentences were the same , a predicate would be needed to compare two lambda forms for  - equivalence , which again is not a simple task ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Essentially , the logic variable X is meant to be interpreted as a bound variable , which requires an additional layer of programming ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "proposes a solution within first-order unification that can handle not only sentence  , but also more complex examples with determiners ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The method used is to introduce spurious bindings that subsequently get removed ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , the semantics of  would be  , which would then get simplified to  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "While this pushes first-order unification beyond what it had been previously shown capable of , there are two disadvantages to this technique :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For every possible category that can be conjoined , a separate lexical entry for and is required , and"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "As the conjoinable categories become more complex , the and entries become correspondingly more complex and greatly obscure the theoretical background of the grammar formalism ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by Prolog , but instead needs to be implemented by additional programming ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "While theoretically possible , it becomes quite problematic to actually implement ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The solution given in this paper is to use a higher-order logic programming language , -Prolog , that already implements these concepts , called `` abstract syntax '' in  and `` higher-order abstract syntax '' in  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This allows a natural and elegant implementation of the grammatical theory , with only one lexical entry for and ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics  ,  ,  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "CCG is a grammatical formalism in which there is a one-to-one correspondence between the rules of composition at the level of syntax and logical form ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Each word is ( perhaps ambiguously ) assigned a category and LF , and when the syntactical operations assign a new category to a constituent , the corresponding semantic operations produce a new LF for that constituent as well ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The CCG rules  shown in Figure  are implemented in the system described in this paper ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Each of the three operations have both a forward and backward variant ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "As an illustration of how the semantic rules can be simulated in first-order unification , consider the derivation of the constituent harry found , where harry has the category np with LF harry ' and found is a transitive verb of category  with LF"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "In the CCG formalism , the derivation is as follows : harry gets raised with the # GT T rule , and then forward composed by the # GT B rule with found , and the result is a category of type s / np with LF  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "In section  it will be seen how the use of abstract syntax allows this to be expressed directly ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "In first-order unification , it is simulated as shown in Figure  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The final CCG rule to be considered is the coordination rule that specifies that only like categories can coordinate :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This is actually a schema for a family of rules , collectively called `` generalized coordination '' , since the semantic rule is different for each case ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , if X is a unary function , then the semantic rule is  , and if the functions have two arguments , then the rule is  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , when processing  , rule  would be used with :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "with the result"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "which is  - equivalent to  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "-Prolog is a logic programming language based on higher-order hereditary Harrop formulae  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "It differs from Prolog in that first-order terms and unification are replaced with simply-typed  - terms and higher-order unification , respectively ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "It also permits universal quantification and implication in the goals of clauses ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The crucial aspect for this paper is that together these features permits the usage of abstract syntax to express the logical forms terms computed by CCG ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The built-in  - term manipulation is used as a `` meta-language '' in which the `` object-language '' of CCG logical forms is expressed , and variables in the object-language are mapped to variables in the meta-language ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The -Prolog code fragment shown in Figure  declares how the CCG logical forms are represented ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Each CCG LF is represented as an untyped  - term , namely type tm. abs represents object-level abstraction  by the meta-level expression ( abs N ) , where N is a meta-level function of type  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "A meta-level  - abstraction  is written  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Thus , if walked ' has type  , then  is a -Prolog ( meta-level ) function with type  , and  is the object-level representation , with type tm ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The LF for found shown in  would be represented as  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "app encodes application , and so in the derivation of harry found , the type-raised harry has the -Prolog value  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The second part of Figure  shows declares how quantifiers are represented , which are required since the sentences to be processed may have determiners ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "forall and exists are encoded similarly to abstraction , in that they take a functional argument and so object-level binding of variables by quantifiers is handled by meta-level  - abstraction ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "and  are simple constructors for implication and conjunction , to be used with forall and exists respectively , in the typical manner  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , the sentence `` every man found a bone '' has as a possible LF  , with the -Prolog representation  :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Figure  illustrates how directly the CCG operations can be encoded ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "o is the type of a meta-level proposition , and so the intended usage of apply is to take three arguments of type tm , where the first should be an object-level  - abstraction , and set the third equal to the application of the first to the second ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Thus , for the query"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "R unifies with the  function  , S with harry ' and M with ( R S ) , the meta-level application of R to S , which by the built-in  - reduction is  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "In other words , object-level function application is handled simply by the meta-level function application ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Function composition is similar ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Consider again the derivation of harry found by type-raising and forward composition ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "harry would get type-raised by the raise clause to produce  , and then composed with found , with the result shown in the following query :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "At this point a further  - reduction is needed ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Note however this is not at all the same problem of writing a  - reducer in Prolog ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Instead it is a simple matter of using the meta-level  - reduction to eliminate  - redexes to produce the final result  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "We won't show the complete declaration of the  - reducer , but the key clause is simply :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Thus , using the abstract syntax capabilities of -Prolog , we can have a direct implementation of the underlying linguistic formalism , in stark contrast to the first-order simulation shown in Figure  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "A primary goal of abstract-syntax is to support recursion through abstractions with bound variables ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This leads to the interpretation of a bound variable as a `` scoped constant '' - it acts like a constant that is not visible from the top of the term , but which becomes visible during the descent through the abstraction ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "See  for a discussion of how this may be used for evaluation of functional programs by `` pushing '' the evaluation through abstractions to reduce redexes that are not at the top-level ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This technique is also used in the  - reducer briefly mentioned at the end of the previous section , and a similar technique will be used here to implement coordination by recursively descending through the two arguments to be coordinated ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Before describing the implementation of coordination , it is first necessary to mention how CCG categories are represented in the -Prolog code ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "As shown in Figure  , cat is declared to be a primitive type , and np , s , conj , noun are the categories used in this implementation ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "fs and bs are declared to be constructors for forward and backward slash ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , the CCG category for a transitive verb  would be represented as  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Also , the predicate atomic-type is declared to be true for the four atomic categories ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This will be used in the implementation of coordination as a test for termination of the recursion ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The implementation of coordination crucially uses the capability of -Prolog for universal quantification in the goal of a clause ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "pi is the meta-level operator for  , and  is written as  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The operational semantics for -Prolog state that  is provable if and only if [ c / x ] G is provable , where c is a new variable of the same type as x that does not otherwise occur in the current signature ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "In other words , c is a scoped constant and the current signature gets expanded with c for the proof of [ c / x ] G. Since c is meant to be treated as a generic placeholder for any arbitrary x of the proper type , c must not appear in any terms instantiated for logic variables during the proof of [ c / x ] G ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The significance of this restriction will be illustrated shortly ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The code for coordination is shown in Figure  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The four arguments to coord are a category and three terms that are the object-level LF representations of constituents of that category ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The last argument will result from the coordination of the second and third arguments ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Consider again the earlier problematic example  of coordination ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Recall that after john is type-raised , its LF will be  and similarly for bill ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "They will both have the category  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Thus , to obtain the LF for John and Bill , the following query would be made :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This will match with the first clause for coord , with"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "A instantiated to ( bs np s )"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "B to s"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "R to"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "S to"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "and T a logic variable waiting instantiation ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Then , after the meta-level  - reduction using the new scoped constant c , the following goal is called :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "where N = ( T c ) ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Since s is an atomic type , the third coord clause matches with"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "B instantiated to s"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "R to"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "S to"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "N to"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Since N = ( T c ) , higher-order unification is used by -Prolog to instantiate T by extracting c from N with the result ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "and so M from the original query is"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Note that since c is a scoped constant arising from the proof of an universal quantification , the instantiation"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "is prohibited , along with the other extractions that do not remove c from the body of the abstraction ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This use of universal quantification to extract out c from a term containing c in this case gives the same result as a direct implementation of the rule for cooordination of unary functions  would ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "However , this same process of recursive descent via scoped constants will work for any member of the conj rule family ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , the following query"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "corresponds to rule  ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Note also that the use of the same bound variable names obj and sub causes no difficulty since the use of scoped-constants , meta-level  - reduction , and higher-order unification is used to access and manipulate the inner terms ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Also , whereas  requires careful consideration of handling of determiners with coordination , here such sentences are handled just like any others ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , the sentence `` Mary gave every dog a bone and some policeman a flower '' results in the LF :"}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Thus , `` generalized coordination '' , instead of being a family of separate rules , can be expressed as a single rule on recursive descent through logical forms ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "also discusses `` generalized composition '' , and it may well be that a similar implementation is possible for that family of rules as well ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "We have shown how higher-order logic programming can be used to elegantly implement the semantic theory of CCG , including the previously difficult case of its handling of coordination constructs ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "The techniques used here should allow similar advantages for a variety of such theories ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "An argument can be made that the approach taken here relies on a formalism that entails implementation issues that are more difficult than for the other solutions and inherently not as efficient ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "However , the implementation issues , although more complex , are also well-understood and it can be expected that future work will bring further improvements ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "For example , it is a straightforward matter to transform the -Prolog code into a logic called   which requires only a restricted form of unification that is decidable in linear time and space ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Also , the declarative nature of -Prolog programs opens up the possibility for applications of program transformations such as partial evaluation ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This work is supported by ARO grant DAAL03 - 89 - 0031 , DARPA grant N00014 - 90-J - 1863 , and ARO grant DAAH04 - 94-G - 0426 ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "I would like to thank Aravind Joshi , Dale Miller , Jong Park , and Mark Steedman for valuable discussions and comments on earlier drafts ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "Many theories of semantic interpretation use  - term manipulation to compositionally compute the meaning of a sentence ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "These theories are usually implemented in a language such as Prolog that can simulate  - term operations with first-order unification ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "However , for some interesting cases , such as a Combinatory Categorial Grammar account of coordination constructs , this can only be done by obscuring the underlying linguistic theory with the `` tricks '' needed for implementation ."}
 {"title": "Using Higher-Order Logic Programming for Semantic Interpretation of Coordinate Constructs", "sentence": "This paper shows how the use of abstract syntax permitted by higher-order logic programming allows an elegant implementation of the semantics of Combinatory Categorial Grammar , including its handling of coordination constructs ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence , and before the end of phrasal constituents  ,  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "There is also recent evidence suggesting that , during speech processing , partial interpretations can be built extremely rapidly , even before words are completed  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "There are also potential computational applications for incremental interpretation , including early parse filtering using statistics based on logical form plausibility , and interpretation of fragments of dialogues ( a survey is provided by  , henceforth referred to as"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In the current computational and psycholinguistic literature there are two main approaches to the incremental construction of logical forms ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "One approach is to use a grammar with ` non-standard ' constituency , so that an initial fragment of a sentence , such as John likes , can be treated as a constituent , and hence be assigned a type and a semantics ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This approach is exemplified by Combinatory Categorial Grammar , CCG  , which takes a basic CG with just application , and adds various new ways of combining elements together ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser , working from left to right along the sentence ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The alternative approach , exemplified by the work of  , and  is to associate a semantics directly with the partial structures formed during a top-down or left-corner parse ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "For example , a syntax tree missing a noun phrase , such as the following"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "can be given a semantics as a function from entities to truth values i.e.  x. likes ( john , x ) , without having to say that John likes is a constituent ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Neither approach is without problems ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "If a grammar is augmented with operations which are powerful enough to make most initial fragments constituents , then there may be unwanted interactions with the rest of the grammar ( examples of this in the case of CCG and the Lambek Calculus are given in Section  ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The addition of extra operations also means that , for any given reading of a sentence there will generally be many different possible derivations ( so-called ` spurious ' ambiguity ) , making simple parsing strategies such as shift-reduce highly inefficient ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The limitations of the parsing approaches become evident when we consider grammars with left recursion ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In such cases a simple top-down parser will be incomplete , and a left corner parser will resort to buffering the input ( so won't be fully word-by-word ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": ""}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This has a small number of possible semantic representations ( the exact number depending upon the grammar ) e.g."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The second representation is appropriate if the sentence finishes with a sentential modifier ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The third allows there to be a verb phrase modifier ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "If the semantic representation is to be read off syntactic structure , then the parser must provide a single syntax tree ( possibly with empty nodes ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However , there are actually any number of such syntax trees corresponding to , for example , the first semantic representation , since the np and the s can be arbitrarily far apart ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The following tree is suitable for the sentence Mary thinks John shaves but not for e.g. Mary thinks John coming here was a mistake ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "or Description Theory  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "One further possibility is to choose a single syntax tree , and to use destructive tree operations later in the parse ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The approach which we will adopt here is based on ,  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Partial syntax trees can be regarded as performing two main roles ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The second is to provide a basis for a semantic representation ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The first role can be captured using syntactic types , where each type corresponds to a potentially infinite number of partial syntax trees ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The second role can be captured by the parser constructing semantic representations directly ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The general processing model therefore consists of transitions of the form :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This provides a state-transition or dynamic model of processing , with each state being a pair of a syntactic type and a semantic value ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The main difference between our approach and that of  ,  is that it is based on a more expressive grammar formalism , Applicative Categorial Grammar , as opposed to Lexicalised Dependency Grammar ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Applicative Categorial Grammars allow categories to have arguments which are themselves functions ( e.g. very can be treated as a function of a function , and given the type (n/n)/(n/n) when used as an adjectival modifier ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions , and in providing one kind of robust parsing : the parser never fails until the last word , since there could always be a final word which is a function over all the constituents formed so far ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However , there is a corresponding problem of far greater non-determinism , with even unambiguous words allowing many possible transitions ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "It therefore becomes crucial to either perform some kind of ambiguity packing , or language tuning ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This will be discussed in the final section of the paper ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Applicative Categorial Grammar is the most basic form of Categorial Grammar , with just a single combination rule corresponding to function application ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "It was first applied to linguistic description by"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Although it is still used for linguistic description  , it has been somewhat overshadowed in recent years by HPSG  , and by Lambek Categorial Grammars  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "It is therefore worth giving some brief indications of how it fits in with these developments ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The first directed Applicative CG was proposed by  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Functional types included a list of arguments to the left , and a list of arguments to the right ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Translating  , we obtain the following category for a ditransitive verb such as put :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The list of arguments to the left are gathered under the feature , l , and those to the right , an np and a pp in that order , under the feature r ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": ""}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The result was a system which comes very close to the formalised dependency grammars of  and  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The only real difference is that"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "For example , an adverb such as slowly could be given the type"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "An unfortunate aspect of"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Hence , arguments with functional types had to correspond to single lexical items : there was no way to form the type np  s for a non-lexical verb phrase such as likes Mary ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Rather than adapting the Application Rule to allow functions to be applied to one argument at a time ,  ) adopted a ` Curried ' notation , and this has been adopted by most CGs since ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "To represent a function which requires an np on the left , and an np and a pp to the right , there is a choice of the following three types using Curried notation :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Most CGs either choose the third of these ( to give a vp structure ) , or include a rule of Associativity which means that the types are interchangeable ( in the Lambek Calculus , Associativity is a consequence of the calculus , rather than being specified separately ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The main impetus to change Applicative CG came from the work of  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": ""}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Function composition enables a function to be applied to its argument , even if that argument is incomplete e.g."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This allows peripheral extraction , where the ` gap ' is at the start or the end of e.g. a relative clause ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Variants of the composition rule were proposed in order to deal with non-peripheral extraction , but this led to unwanted effects elsewhere in the grammar  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Subsequent treatments of non-peripheral extraction based on the Lambek Calculus ( where standard composition is built in : it is a rule which can be proven from the calculus ) have either introduced an alternative to the forward and backward slashes i.e. / and  for normal args ,  for wh-args  , or have introduced so called modal operators on the wh-argument  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Both techniques can be thought of as marking the wh-arguments as requiring special treatment , and therefore do not lead to unwanted effects elsewhere in the grammar ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However , there are problems with having just composition , the most basic of the non-applicative operations ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In CGs which contain functions of functions ( such as very , or slowly ) , the addition of composition adds both new analyses of sentences , and new strings to the language ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This is due to the fact that composition can be used to form a function , which can then be used as an argument to a function of a function ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "For example , if the two types , n / n and n / n are composed to give the type n / n , then this can be modified by an adjectival modifier of type ( n / n ) / ( n / n ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Thus , the noun very old dilapidated car can get the unacceptable bracketing , [ [ very [ old dilapidated ] ] car ] ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Associative CGs with Composition , or the Lambek Calculus also allow strings such as boy with the to be given the type n / n predicting very boy with the car to be an acceptable noun ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Although individual examples might be possible to rule out using appropriate features , it is difficult to see how to do this in general whilst retaining a calculus suitable for incremental interpretation ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "If wh-arguments need to be treated specially anyway ( to deal with non-peripheral extraction ) , and if composition as a general rule is problematic , this suggests we should perhaps return to grammars which use just Application as a general operation , but have a special treatment for wh-arguments ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Using the non-Curried notation of"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "For example , the category appropriate for relative clauses with a noun phrase gap would be :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "It is then possible to specify operations which act as purely applicative operations with respect to the left and right arguments lists , but more like composition with respect to the wh-list ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This is very similar to the way in which wh-movement is dealt with in GPSG  and HPSG , where wh-arguments are treated using slash mechanisms or feature inheritance principles which correspond closely to function composition ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Given that our arguments have produced a categorial grammar which looks very similar to HPSG , why not use HPSG rather than Applicative CG ?"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The main reason is that Applicative CG is a much simpler formalism , which can be given a very simple syntax semantics interface , with function application in syntax mapping to function application in semantics  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This in turn makes it relatively easy to provide proofs of soundness and completeness for an incremental parsing algorithm ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Ultimately , some of the techniques developed here should be able to be extended to more complex formalisms such as HPSG ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In this section we define a grammar similar to"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However , unlike"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The resulting grammar is equivalent to AB Categorial Grammar plus associativity ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The categories of the grammar are defined as follows :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "If X is a syntactic type ( e.g. s , np ) , then  is a category ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "If X is a syntactic type , and L and R are lists of categories , then  is a category ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Application to the right is defined by the rule :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Application to the left is defined by the rule :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The basic grammar provides some spurious derivations , since sentences such as John likes Mary can be bracketed as either ( ( John likes ) Mary ) or ( John ( likes Mary ) ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However , we will see that these spurious derivations do not translate into spurious ambiguity in the parser , which maps from strings of words directly to semantic representations ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Most parsers which work left to right along an input string can be described in terms of state transitions i.e. by rules which say how the current parsing state ( e.g. a stack of categories , or a chart ) can be transformed by the next word into a new state ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Here this will be made particularly explicit , with the parser described in terms of just two rules which take a state , a new word and create a new state ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "There are two unusual features ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Firstly , there is nothing equivalent to a stack mechanism : at all times the state is characterised by a single syntactic type , and a single semantic value , not by some stack of semantic values or syntax trees which are waiting to be connected together ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Secondly , all transitions between states occur on the input of a new word : there are no ` empty ' transitions ( such as the reduce step of a shift-reduce parser ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The two rules , which are given in Figure  , are difficult to understand in their most general form ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Here we will work upto the rules gradually , by considering which kinds of rules we might need in particular instances ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Consider the following pairing of sentence fragments with their simplest possible CG type :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Now consider taking each type as a description of the state that the parser is in after absorbing the fragment ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "We obtain a sequence of transitions as follows :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "If an embedded sentence such as John likes Sue is a mapping from an s / s to an s , this suggests that it might be possible to treat all sentences as mapping from some category expecting an s to that category i.e. from X / s to X ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Similarly , all noun phrases might be treated as mappings from an X / np to an X ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Now consider individual transitions ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The simplest of these is where the type of argument expected by the state is matched by the next word i.e."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This can be generalised to the following rule , which is similar to Function Application in standard CG"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "A similar transition occurs for likes ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Here an np  s was expected , but likes only provides part of this : it requires an np to the right to form an np  s ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Thus after likes is absorbed the state category will need to expect an np ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The rule required is similar to Function Composition in CG i.e."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Considering this informally in terms of tree structures , what is happening is the replacement of an empty node in a partial tree by a second partial tree i.e."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The two rules specified so far need to be further generalised to allow for the case where a lexical item has more than one argument ( e.g. if we replace likes by a di-transitive such as gives or a tri-transitive such as bets ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This is relatively trivial using a non-curried notation similar to that used for AACG ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "What we obtain is the single rule of State-Application , which corresponds to application when the list of arguments , R  , is empty , to function composition when R  is of length one , and to n-ary composition when R  is of length n ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The only change needed from AACG notation is the inclusion of an extra feature list , the h list , which stores information about which arguments are waiting for a head ( the reasons for this will be explained later ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The lexicon is identical to that for a standard AACG , except for having h-lists which are always set to empty ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Now consider the first transition ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Here a sentence was expected , but what was encountered was a noun phrase , John ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The appropriate rule in CG notation would be :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This rule states that if looking for a Y and get a Z then look for a Y which is missing a Z ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In tree structure terms we have :"}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The rule of State-Prediction is obtained by further generalising to allow the lexical item to have missing arguments , and for the expected argument to have missing arguments ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "State-Application and State-Prediction together provide the basis of a sound and complete parser ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Parsing of sentences is achieved by starting in a state expecting a sentence , and applying the rules non-deterministically as each word is input ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "A successful parse is achieved if the final state expects no more arguments ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "As an example , reconsider the string John likes Sue ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The sequence of transitions corresponding to John likes Sue being a sentence , is given in Figure  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The transition on encountering John is deterministic : State-Application cannot apply , and State-Prediction can only be instantiated one way ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The result is a new state expecting an argument which , given an np could give an s i.e. an np  s ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The transition on input of likes is non-deterministic ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "State-Application can apply , as in Figure  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However , State-Prediction can also apply , and can be instantiated in four ways ( these correspond to different ways of cutting up the left and right subcategorisation lists of the lexical entry , likes , i.e. as  np  or  np  ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "One possibility corresponds to the prediction of an s  s modifier , a second to the prediction of an ( np  s )  ( np  s ) modifier ( i.e. a verb phrase modifier ) , a third to there being a function which takes the subject and the verb as separate arguments , and the fourth corresponds to there being a function which requires an s / np argument ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The second of these is perhaps the most interesting , and is given in Figure  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "It is the choice of this particular transition at this point which allows verb phrase modification , and hence , assuming the next word is Sue , an implicit bracketing of the string fragment as ( John ( likes Sue ) ) ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Note that if State-Application is chosen , or the first of the State-Prediction possibilities , the fragment John likes Sue retains a flat structure ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "If there is to be no modification of the verb phrase , no verb phrase structure is introduced ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This relates to there being no spurious ambiguity : each choice of transition has semantic consequences ; each choice affects whether a particular part of the semantics is to be modified or not ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Finally , it is worth noting why it is necessary to use h-lists ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "These are needed to distinguish between cases of real functional arguments ( of functions of functions ) , and functions formed by State-Prediction ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Consider the following trees , where the np  s node is empty ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Both trees have the same syntactic type , however in the first case we want to allow for there to be an s  s modifier of the lower s , but not in the second ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The headed list distinguishes between the two cases , with only the first having an np on its headed list , allowing prediction of an s modifier ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "When we consider full sentence processing , as opposed to incremental processing , the use of lexicalised grammars has a major advantage over the use of more standard rule based grammars ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In processing a sentence using a lexicalised formalism we do not have to look at the grammar as a whole , but only at the grammatical information indexed by each of the words ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Thus increases in the size of a grammar don't necessarily effect efficiency of processing , provided the increase in size is due to the addition of new words , rather than increased lexical ambiguity ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Once the full set of possible lexical entries for a sentence is collected , they can , if required , then be converted back into a set of phrase structure rules ( which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar ) , before being parsing with a standard algorithm such as  's ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In incremental parsing we cannot predict which words will appear in the sentence , so cannot use the same technique ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However , if we are to base a parser on the rules given above , it would seem that we gain further ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Instead of grammatical information being localised to the sentence as a whole , it is localised to a particular word in its particular context : there is no need to consider a pp as a start of a sentence if it occurs at the end , even if there is a verb with an entry which allows for a subject pp ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "However there is a major problem ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "As we noted in the last paragraph , it is the nature of parsing incrementally that we don't know what words are to come next ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "But here the parser doesn't even use the information that the words are to come from a lexicon for a particular language ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "For example , given an input of 3 nps , the parser will happily create a state expecting 3 nps to the left ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This might be a likely state for say a head final language , but an unlikely state for a language such as English ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Note that incremental interpretation will be of no use here , since the semantic representation should be no more or less plausible in the different languages ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "In practical terms , a naive interactive parallel Prolog implementation on a current workstation fails to be interactive in a real sense after about 8 words ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "What seems to be needed is some kind of language tuning ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This could be in the nature of fixed restrictions to the rules e.g. for English we might rule out uses of prediction when a noun phrase is encountered , and two already exist on the left list ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "A more appealing alternative is to base the tuning on statistical methods ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "This could be achieved by running the parser over corpora to provide probabilities of particular transitions given particular words ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "These transitions would capture the likelihood of a word having a particular part of speech , and the probability of a particular transition being performed with that part of speech ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories  ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "Unlike a simple Markov process , there are a potentially infinite number of states , so there is inevitably a problem of sparse data ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "It is therefore necessary to make various generalisations over the states , for example by ignoring the R  lists ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The full processing model can then be either serial , exploring the most highly ranked transitions first ( but allowing backtracking if the semantic plausibility of the current interpretation drops too low ) , or ranked parallel , exploring just the n paths ranked highest according to the transition probabilities and semantic plausibility ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The paper has presented a method for providing interpretations word by word for basic Categorial Grammar ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The final section contrasted parsing with lexicalised and rule based grammars , and argued that statistical language tuning is particularly suitable for incremental , lexicalised parsing strategies ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The parser does not require fragments of sentences to form constituents , and thereby avoids problems of spurious ambiguity ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG , Dependency Grammar and the Lambek Calculus ."}
 {"title": "Incremental Interpretation of Categorial Grammar", "sentence": "It also includes a discussion of some of the issues which arise when parsing lexicalised grammars , and the possibilities for using statistical techniques for tuning to particular languages ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Correctly determining number is a difficult problem when translating from Japanese to English ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "This is because in Japanese , noun phrases are not normally marked with respect to number ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Japanese nouns have no equivalent to the English singular and plural forms and verbs do not inflect to agree with the number of the subject  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In addition , there is no grammatical marking of countability ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In order to generate English correctly , it is necessary to know whether a given noun phrase is countable or uncountable and , if countable , whether it is singular or plural ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Deciding this is a problem even for humans translating from Japanese to English , but they have their own knowledge of both languages to draw on ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "A machine translation system needs to have this knowledge codified in some way ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As generating articles and number is only important when the rest of the sentence has been correctly generated , there has not been a lot of research devoted to it ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Recently ,  have proposed a method of determining the referentiality property and number of nouns in Japanese sentences for machine translation into English , but the research has not yet been extended to include the actual English generation ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "This paper describes a method that extracts information relevant to countability and number from the Japanese text and combines it with knowledge about countability and number in English ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "First countability in English is discussed at the noun phrase and then the noun level ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As a noun phrase 's countability in English is affected by its referential property ( generic , referential or ascriptive ) we present a method of determining the referential use of Japanese noun phrases ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Next the process of actually determining noun phrase countability and number is described ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "This is followed by some examples of sentences translated by the proposed method and a discussion of the results ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The processing described in this paper has been implemented in NTT Communication Science Laboratories ' experimental machine translation system ALT-J / E  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Along with new processing for the generation of articles , which is not discussed in detail in this paper , it improved the percentage of noun phrases with correctly generated determiners and number from 65 % to 73 % ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We adopt the definition of countability in English given in  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "A countable noun phrase is defined as follows :"}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "If the head constituent of an NP falls within the scope of a denumerator it is countable ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "If the head constituent of an NP is plural it is countable ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Where `` the phrase ` falls within the scope [ or domain ] of a denumerator ' means ` is denumerated ' by it ; i. e the NP reference is quantified by the denumerator as a number of discrete entities ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "'' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Not all nouns in English can become the head of a countable noun phrase ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In particular , noun phrases whose heads fall within the scope of a denumerator ( ` denumerated ' noun phrases ) must be headed by a noun that has both singular and plural forms ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Nouns that do not have both forms , like equipment or scissors , require a classifier to be used ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The classifier becomes the head of a countable noun phrase with the original noun attached as the complement of a prepositional phrase headed by of : a pair of scissors , a piece of equipment ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Whether a noun can be used to head a countable noun phrase or not depends both on how it is interpreted , and on its inherent countability preference ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Noun countability preferences are discussed in the next section ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "A noun 's countability preference determines how it will behave in different environments ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We classify nouns into seven countability preferences , five major and two minor , as described below ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The two most basic types are ` fully countable ' and ` uncountable ' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Fully countable nouns , such as knife have both singular and plural forms , and cannot be used with determiners such as much ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Uncountable nouns , such as furniture , have no plural form , and can be used with much ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Between these two extremes there are a vast number of nouns , such as cake , that can be used in both countable and uncountable noun phrases ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "They have both singular and plural forms , and can also be used with much ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Whether such nouns will be used countably or uncountably depends on whether their referent is being thought of as made up of discrete units or not ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As it is not always possible to explicitly determine this when translating from Japanese to English , we divide these nouns into two groups : ` strongly countable ' , those that are more often used to refer to discrete entities , such as cake , and ` weakly countable ' , those that are more often used to refer to unbounded referents , such as beer ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The last major type of countability preference is ` pluralia tanta ' : nouns that only have a plural form , such as scissors ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "They can neither be denumerated nor modified by much ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We further subdivide pluralia tanta into two types , those that can use the classifier pair to be denumerated , such as a pair of scissors and those that can't , such as clothes ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "` pair ' pluralia tanta have a singular form when used as modifiers ( a scissor movement ) ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Pluralia tanta such as clothes , use the plural form even as modifiers ( a clothes horse ) , and need a countable word of similar meaning to be substituted when they are denumerated : a garment , a suit ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The two minor types are subsets of fully countable and uncountable nouns respectively ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Unless explicitly indicated , they will be treated the same as their supersets ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "` Collective ' nouns share all the properties of fully countable nouns ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In addition they can have singular or plural verb agreement with the singular form of the noun : The government has / have decided ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "` Semi-countable ' nouns share the properties of uncountable nouns , except that they can be modified directly by a / an ; for example a knowledge [ of Japanese ] ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Examples of the information about countability and number stored in the Japanese to English noun transfer dictionary are given in table  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The information about noun countability preferences cannot be found in standard dictionaries and must be entered by an English native speaker ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Some tests to help determine a given noun 's countability preferences are described in  , which discusses the use of noun countability preferences in Japanese to English machine translation ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The first stage in generating the countability and number of a translated English noun phrase is to determine its referentiality ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We distinguish three kinds of referentiality : ` generic ' , ` referential ' and ` ascriptive ' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We call noun phrases used to make general statements about a class generic ; for example Mammoths are extinct ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The way generic noun phrases are expressed in English is described in Section  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Referential noun phrases are ones that refer to some specific referent ; for example ` Two dogs chase a cat ' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Their number and countability are ideally determined by the properties of the referent ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Ascriptive noun phrases are used to ascribe a property to something ; for example ` Hathi is an elephant ' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "They normally have the same number and countability as the noun phrase whose property they are describing ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The process of determining the referentiality of a noun phrase is shown in Figure  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The tests are processed in the order shown ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As far as possible , simple criteria that can be implemented using the dictionary have been chosen ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "For example , Test  `` if a NP is modified by ` aimed at , for ... ' then it is ` generic '' ' is applied as part of translating NP 1-muke into `` for NP 1 '' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The transfer dictionary includes the information that in this case , NP 1 should be generic ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Tests  a  show two more heuristic methods for determining whether a noun phrase has generic reference ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In Test  , if the predicate is marked in the dictionary as one that only applies to classes as a whole , such as evolve or be extinct , then the sentence is taken to be generic ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In Test  , ALT-J / E 's semantic hierarchy is used to test whether a sentence is generic or not ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "For example in ` Mammoths are animals ' , ` mammoth ' has the semantic category ANIMAL so the sentence is judged to be stating a fact true of all mammoths and is thus generic ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "A generic noun phrase ( with a countable head noun ) can generally be expressed in three ways  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We call these GEN ` a ' , where the noun phrase is indefinite : A mammoth is a mammal ; GEN ` the ' , where the noun phrase is definite : The mammoth is a mammal ; and GEN  , where there is no article : Mammoths are mammals ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Uncountable nouns and pluralia tanta can only be expressed by GEN  ( eg : Furniture is expensive ) ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "They cannot take GEN ` a ' because they cannot be modified by a ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "They do not take GEN ` the ' , because then the noun phrase would normally be interpreted as having definite reference ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Nouns that can be either countable or uncountable also only take GEN  : ` Cake is delicious / Cakes are delicious ' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "These combinations are shown in Table  , noun phrases that can not be used to show generic reference are marked ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The use all three kinds of generic noun phrases is not acceptable in some contexts , for example * a mammoth evolved ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Sometimes a noun phrase can be ambiguous , for example ` I like the elephant ' , where the speaker could like a particular elephant , or all elephants ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Because the use of GEN  is acceptable in all contexts , ALT-J / E generates all generic noun phrases as such , that is as bare noun phrases ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The number of the noun phrase is then determined by the countability preference of the noun phrase heading it ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Fully countable nouns and pluralia tanta will be plural , all others are singular ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The following discussion deals only with referential and ascriptive noun phrases as generic noun phrases were discussed in Section  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The definitions of noun phrase countability given in Section  , while useful for analyzing English , are not sufficient for translating from Japanese to English ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "This is because in many cases it is impossible to tell from the Japanese form or syntactic shape whether a translated noun phrase will fall within the scope of a denumerator or not ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Japanese has no equivalent to a / an and does not distinguish between countable and uncountable quantifiers such as many / much and little / few ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Therefore to determine countability and generate number we need to use a combination of information from the Japanese original sentence , and default information from the Japanese to English transfer dictionary ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As much as possible , detailed information is entered in the transfer dictionaries to allow the translation process itself to be made simple ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The process of determining a noun phrase 's countability and number is shown in Figure  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The process is carried out during the transfer stage so information is available from both the Japanese original and the selected English translation ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "To make the task of determining countability and number simpler , we define combinations of different countabilities for nouns with different countability preferences that we can use in the dictionaries ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The effects of the four most common types on the five major noun countability preferences are shown in Table  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Noun phrases modified by Japanese / English pairs that are translated as denumerators we call denumerated ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "For example a noun modified by onoono-no `` each '' is denumerated - singular , while one modified by ryouhou-no `` both '' is denumerated - plural ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Uncountable and pluralia tantum nouns in denumerated environments are translated as the prepositional complement of a classifier ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "A default classifier is stored stored in the dictionary for uncountable nouns and pluralia tanta ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Ascriptive noun phrases whose subject is countable will also be denumerated ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The two ` mass ' environments shown in Table  are used to show the countability of nouns that can be either countable or uncountable ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Weakly countable nouns will only be countable if used with a denumerator ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Strongly countable nouns will be countable and plural in such mass - countable environments as the object of collect ( vt ) : ` I collect cakes ' , and uncountable and singular in mass - uncountable environments such as ` I ate too much cake ' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In fact , both ` I collect cake ' and ` I ate too many cakes ' are possible ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As Japanese does not distinguish between the two the system must make the best choice it can , in the same way a human translator would have to ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The rules have been implemented to generate the translation that has the widest application , for example generating ` I ate too much cake ' , which is true whether the speaker only ate part or all of one cake or if they ate many cakes , rather than ` I ate too many cakes ' which is only true if the speaker ate many cakes ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Sometimes the choice of the English translation of a modifier will depend on the countability of the noun phrase ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "For example , kazukazu-no and takusan-no can all be translated as `` many '' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "kazukazu-no implies that it 's modificant is made up of discrete entities , so the noun phrase it modifies should be translated as denumerated - plural ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "takusan-no does not carry this nuance so ALT-J / E will translate a noun phrase modified by it as mass - uncountable , and takusan-no as many if the head is countable and much otherwise ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Rules that translate the nouns with different noun countability preferences into other combinations of countable and uncountable are also possible ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "For example , sometimes even fully countable nouns can be used in uncountable noun phrases ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "If an elephant is referred to not as an individual elephant but as a source of meat , then it will be expressed in an uncountable noun phrase : ` I ate a slice of elephant ' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "To generate this the following rule is used : `` nouns quantified with the classifier kire `` slice '' will be generated as the prepositional complement of slice , they will be singular with no article unless they are pluralia tanta , when they will be plural with no article '' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Note that countable indefinite singular noun phrases without a determiner will have a / an generated ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Countable indefinite plural noun phrases and uncountable noun phrases may have some generated ; a full discussion of this is outside the scope of this article ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "This processing described above has been implemented in ALT-J / E. It was tested , together with new processing to generate articles , on a specially constructed set of test sentences , and on a collection of newspaper articles ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The results are summarized in Table  ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In the newspaper articles tested , there were an average of 7.0 noun phrases in each sentence ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "For a sentence to be judged as correct all the noun phrases must be correct ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The introduction of the proposed method improved the percentage of correct sentences from 5 % to 12 ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Some examples of translations before and after the introduction of the new processing are given below ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The translations before the proposed processing was implemented are marked OLD , the translations produced by ALT-J / E using the proposed processing are marked NEW ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "In  , the noun phrase headed by otona `` adult '' is judged to be prescriptive , as it is the complement of the copular naru `` become '' ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Therefore the proposed method translates it with the same number as the subject ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "zetumetu `` die out '' , is entered in the lexicon as a verb whose subject must be generic ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "manmosu `` mammoth '' is fully countable so the generic noun phrase is translated as a bare plural ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The old version recognizes that a denumerated noun phrase headed by an uncountable noun tofu requires a classifier but does not generate the correct structure neither does it generate a classifier for the pluralia tanta scissors ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The version using the proposed method does ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As the subject of the copula that is countable it 's complement is judged to be denumerated by the proposed method ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "As the complement is headed by an uncountable noun it must be embedded in the prepositional complement of a classifier ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "There are three main problems still remaining ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The first is that currently the rules for determining the noun phrase referentiality are insufficiently fine ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We estimate that if referentiality could be determined 100 % correctly then the percentage of noun phrases with correctly generated articles and number could be improved to 96 % in the test set we studied ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The remaining 4 % require knowledge from outside the sentence being translated ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The biggest problem is noun phrases requiring world knowledge that cannot be expressed as a dictionary default ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "These noun phrases cannot be generated correctly by the purely heuristic methods proposed here ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The last problem is noun phrases whose countability and number can be deduced from information in other sentences ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "We would like to extend our method to use this information in the future ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "The quality of the English in a Japanese to English Machine Translation system can be improved by the method proposed in this paper ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "This method uses the information available in the original Japanese sentence along with information about English countability at both the noun phrase and noun level that can be stored in Japanese to English transfer dictionaries ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Incorporating this method into the machine translation system ALT-J / E helped to improve the percentage of noun phrases with correctly generated articles and number from 65 % to 73 % ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "This paper presents a heuristic method that uses information in the Japanese text along with knowledge of English countability and number stored in transfer dictionaries to determine the countability and number of English noun phrases ."}
 {"title": "Countability and Number in Japanese-to-English Machine Translation", "sentence": "Incorporating this method into the machine translation system ALT-J / E , helped to raise the percentage of noun phrases generated with correct use of articles and number from 65 % to 73 ."}
 {"title": "", "sentence": "In applications such as speech recognition , handwriting recognition , and spelling correction , performance is limited by the quality of the language model utilized  ,  , ,  ."}
 {"title": "", "sentence": "However , static language modeling performance has remained basically unchanged since the advent of n-gram language models forty years ago  ."}
 {"title": "", "sentence": "Yet , n-gram language models can only capture dependencies within an n-word window , where currently the largest practical n for natural language is three , and many dependencies in natural language occur beyond a three-word window ."}
 {"title": "", "sentence": "In addition , n-gram models are extremely large , thus making them difficult to implement efficiently in memory-constrained applications ."}
 {"title": "", "sentence": "An appealing alternative is grammar-based language models ."}
 {"title": "", "sentence": "Language models expressed as a probabilistic grammar tend to be more compact than n-gram language models , and have the ability to model long-distance dependencies  , ,  ."}
 {"title": "", "sentence": "However , to date there has been little success in constructing grammar-based language models competitive with n-gram models in problems of any magnitude ."}
 {"title": "", "sentence": "In this paper , we describe a corpus-based induction algorithm for probabilistic context-free grammars that outperforms n-gram models and the Inside-Outside algorithm  in medium-sized domains ."}
 {"title": "", "sentence": "This result marks the first time a grammar-based language model has surpassed n-gram modeling in a task of at least moderate size ."}
 {"title": "", "sentence": "The algorithm employs a greedy heuristic search within a"}
 {"title": "", "sentence": "Grammar induction can be framed as a search problem , and has been framed as such almost without exception in past research  ."}
 {"title": "", "sentence": "The search space is taken to be some class of grammars ; for example , in our work we search within the space of probabilistic context-free grammars ."}
 {"title": "", "sentence": "The objective function is taken to be some measure dependent on the training data ; one generally wants to find a grammar that in some sense accurately models the training data ."}
 {"title": "", "sentence": "Most work in language modeling , including n-gram models and the Inside-Outside algorithm , falls under the maximum-likelihood paradigm , where one takes the objective function to be the likelihood of the training data given the grammar ."}
 {"title": "", "sentence": "However , the optimal grammar under this objective function is one which generates only strings in the training data and no other strings ."}
 {"title": "", "sentence": "Such grammars are poor language models , as they overfit the training data and do not model the language at large ."}
 {"title": "", "sentence": "In n-gram models and the Inside-Outside algorithm , this issue is evaded by bounding the size and form of the grammars considered , so that the `` optimal '' grammar cannot be expressed ."}
 {"title": "", "sentence": "However , in our work we do not wish to limit the size of the grammars considered ."}
 {"title": "", "sentence": "The basic shortcoming of the maximum-likelihood objective function is that it does not encompass the compelling intuition behind Occam 's Razor , that simpler ( or smaller ) grammars are preferable over complex ( or larger ) grammars ."}
 {"title": "", "sentence": "A factor in the objective function that favors smaller grammars over large can prevent the objective function from preferring grammars that overfit the training data ."}
 {"title": "", "sentence": "presents a"}
 {"title": "", "sentence": "The goal of grammar induction is taken to be finding the grammar with the largest a posteriori probability given the training data , that is , finding the grammar G ' where"}
 {"title": "", "sentence": "and where we denote the training data as O , for observations ."}
 {"title": "", "sentence": "As it is unclear how to estimate  directly , we apply"}
 {"title": "", "sentence": "Hence , we can frame the search for G ' as a search with the objective function  , the likelihood of the training data multiplied by the prior probability of the grammar ."}
 {"title": "", "sentence": "We satisfy the goal of favoring smaller grammars by choosing a prior that assigns higher probabilities to such grammars ."}
 {"title": "", "sentence": "In particular ,  , which is closely related to the minimum description length principle later proposed by  ."}
 {"title": "", "sentence": "In the case of grammatical language modeling , this corresponds to taking"}
 {"title": "", "sentence": "where  is the length of the description of the grammar in bits ."}
 {"title": "", "sentence": "The universal a priori probability has many elegant properties , the most salient of which is that it dominates all other enumerable probability distributions multiplicatively ."}
 {"title": "", "sentence": "As described above , we take grammar induction to be the search for the grammar G ' that optimizes the objective function  ."}
 {"title": "", "sentence": "While this framework does not restrict us to a particular grammar formalism , in our work we consider only probabilistic context-free grammars ."}
 {"title": "", "sentence": "We assume a simple greedy search strategy ."}
 {"title": "", "sentence": "We maintain a single hypothesis grammar which is initialized to a small , trivial grammar ."}
 {"title": "", "sentence": "We then try to find a modification to the hypothesis grammar , such as the addition of a grammar rule , that results in a grammar with a higher score on the objective function ."}
 {"title": "", "sentence": "When we find a superior grammar , we make this the new hypothesis grammar ."}
 {"title": "", "sentence": "We repeat this process until we can no longer find a modification that improves the current hypothesis grammar ."}
 {"title": "", "sentence": "For our initial grammar , we choose a grammar that can generate any string , to assure that the grammar can cover the training data ."}
 {"title": "", "sentence": "The initial grammar is listed in Table  ."}
 {"title": "", "sentence": "The sentential symbol S expands to a sequence of X 's , where X expands to every other nonterminal symbol in the grammar ."}
 {"title": "", "sentence": "Initially , the set of nonterminal symbols consists of a different nonterminal symbol expanding to each terminal symbol ."}
 {"title": "", "sentence": "Notice that this grammar models a sentence as a sequence of independently generated nonterminal symbols ."}
 {"title": "", "sentence": "We maintain this property throughout the search process , that is , for every symbol A ' that we add to the grammar , we also add a rule  ."}
 {"title": "", "sentence": "This assures that the sentential symbol can expand to every symbol ; otherwise , adding a symbol will not affect the probabilities that the grammar assigns to strings ."}
 {"title": "", "sentence": "We use the term move set to describe the set of modifications we consider to the current hypothesis grammar to hopefully produce a superior grammar ."}
 {"title": "", "sentence": "Our move set includes the following moves :"}
 {"title": "", "sentence": "Move 1 : Create a rule of the form"}
 {"title": "", "sentence": "Move 2 : Create a rule of the form"}
 {"title": "", "sentence": "For any context-free grammar , it is possible to express a weakly equivalent grammar using only rules of these forms ."}
 {"title": "", "sentence": "As mentioned before , with each new symbol A we also create a rule  ."}
 {"title": "", "sentence": "Consider the task of calculating the objective function  for some grammar G ."}
 {"title": "", "sentence": "Calculating  is inexpensive ; however , calculating  requires a parsing of the entire training data ."}
 {"title": "", "sentence": "We cannot afford to parse the training data for each grammar considered ; indeed , to ever be practical for data sets of millions of words , it seems likely that we can only afford to parse the data once ."}
 {"title": "", "sentence": "To achieve this goal , we employ several approximations ."}
 {"title": "", "sentence": "First , notice that we do not ever need to calculate the actual value of the objective function ; we need only to be able to distinguish when a move applied to the current hypothesis grammar produces a grammar that has a higher score on the objective function , that is , we need only to be able to calculate the difference in the objective function resulting from a move ."}
 {"title": "", "sentence": "This can be done efficiently if we can quickly approximate how the probability of the training data changes when a move is applied ."}
 {"title": "", "sentence": "To make this possible , we approximate the probability of the training data  by the probability of the single most probable parse , or Viterbi parse , of the training data ."}
 {"title": "", "sentence": "Furthermore , instead of recalculating the Viterbi parse of the training data from scratch when a move is applied , we use heuristics to predict how a move will change the Viterbi parse ."}
 {"title": "", "sentence": "For example , consider the case where the training data consists of the two sentences"}
 {"title": "", "sentence": "In Figure  , we display the Viterbi parse of this data under the initial hypothesis grammar used in our algorithm ."}
 {"title": "", "sentence": "Now , let us consider the move of adding the rule"}
 {"title": "", "sentence": "to the initial grammar ( as well as the concomitant rule  ) ."}
 {"title": "", "sentence": "A reasonable heuristic for predicting how the Viterbi parse will change is to replace adjacent X 's that expand to  and  respectively with a single X that expands to B , as displayed in Figure  ."}
 {"title": "", "sentence": "This is the actual heuristic we use for moves of the form  , and we have analogous heuristics for each move in our move set ."}
 {"title": "", "sentence": "By predicting the differences in the Viterbi parse resulting from a move , we can quickly estimate the change in the probability of the training data ."}
 {"title": "", "sentence": "Notice that our predicted Viterbi parse can stray a great deal from the actual Viterbi parse , as errors can accumulate as move after move is applied ."}
 {"title": "", "sentence": "To minimize these effects , we process the training data incrementally ."}
 {"title": "", "sentence": "Using our initial hypothesis grammar , we parse the first sentence of the training data and search for the optimal grammar over just that one sentence using the described search framework ."}
 {"title": "", "sentence": "We use the resulting grammar to parse the second sentence , and then search for the optimal grammar over the first two sentences using the last grammar as the starting point ."}
 {"title": "", "sentence": "We repeat this process , parsing the next sentence using the best grammar found on the previous sentences and then searching for the best grammar taking into account this new sentence , until the entire training corpus is covered ."}
 {"title": "", "sentence": "Delaying the parsing of a sentence until all of the previous sentences are processed should yield more accurate Viterbi parses during the search process than if we simply parse the whole corpus with the initial hypothesis grammar ."}
 {"title": "", "sentence": "In addition , we still achieve the goal of parsing each sentence but once ."}
 {"title": "", "sentence": "In this section , we describe how the parameters of our grammar , the probabilities associated with each grammar rule , are set ."}
 {"title": "", "sentence": "Ideally , in evaluating the objective function for a particular grammar we should use its optimal parameter settings given the training data , as this is the full score that the given grammar can achieve ."}
 {"title": "", "sentence": "However , searching for optimal parameter values is extremely expensive computationally ."}
 {"title": "", "sentence": "Instead , we grossly approximate the optimal values by deterministically setting parameters based on the Viterbi parse of the training data parsed so far ."}
 {"title": "", "sentence": "We rely on the post-pass , described later , to refine parameter values ."}
 {"title": "", "sentence": "Referring to the rules in Table  , the parameter  is set to an arbitrary small constant ."}
 {"title": "", "sentence": "The values of the parameters  are set to the ( smoothed ) frequency of the  reduction in the Viterbi parse of the data seen so far ."}
 {"title": "", "sentence": "The remaining symbols are set to expand uniformly among their possible expansions ."}
 {"title": "", "sentence": "Consider the move of creating a rule of the form  ."}
 {"title": "", "sentence": "This corresponds to  different specific rules that might be created , where k is the current number of symbols in the grammar ."}
 {"title": "", "sentence": "As it is too computationally expensive to consider each of these rules at every point in the search , we use heuristics to constrain which moves are appraised ."}
 {"title": "", "sentence": "For the left-hand side of a rule , we always create a new symbol ."}
 {"title": "", "sentence": "This heuristic selects the optimal choice the vast majority of the time ; however , under this constraint the moves described earlier in this section cannot yield arbitrary context-free languages ."}
 {"title": "", "sentence": "To partially address this , we add the move"}
 {"title": "", "sentence": "Move 3 : Create a rule of the form"}
 {"title": "", "sentence": "With this iteration move , we can construct grammars that generate arbitrary regular languages ."}
 {"title": "", "sentence": "As yet , we have not implemented moves that enable the construction of arbitrary context-free grammars ; this belongs to future work ."}
 {"title": "", "sentence": "To constrain the symbols we consider on the right-hand side of a new rule , we use what we call triggers ."}
 {"title": "", "sentence": "A trigger is a phenomenon in the Viterbi parse of a sentence that is indicative that a particular move might lead to a better grammar ."}
 {"title": "", "sentence": "For example , in Figure  the fact that the symbols  and  occur adjacently is indicative that it could be profitable to create a rule  ."}
 {"title": "", "sentence": "We have developed a set of triggers for each move in our move set , and only consider a specific move if it is triggered in the sentence currently being parsed in the incremental processing ."}
 {"title": "", "sentence": "A conspicuous shortcoming in our search framework is that the grammars in our search space are fairly unexpressive ."}
 {"title": "", "sentence": "Firstly , recall that our grammars model a sentence as a sequence of independently generated symbols ; however , in language there is a large dependence between adjacent constituents ."}
 {"title": "", "sentence": "Furthermore , the only free parameters in our search are the parameters  ; all other symbols ( except S ) are fixed to expand uniformly ."}
 {"title": "", "sentence": "These choices were necessary to make the search tractable ."}
 {"title": "", "sentence": "To address this issue , we use an Inside-Outside algorithm post-pass ."}
 {"title": "", "sentence": "Our methodology is derived from that described by   ."}
 {"title": "", "sentence": "We create n new nonterminal symbols  , and create all rules of the form :"}
 {"title": "", "sentence": "denotes the set of nonterminal symbols acquired in the initial grammar induction phase , and  is taken to be the new sentential symbol ."}
 {"title": "", "sentence": "These new rules replace the first three rules listed in Table  ."}
 {"title": "", "sentence": "The parameters of these rules are initialized randomly ."}
 {"title": "", "sentence": "Using this grammar as the starting point , we run the Inside-Outside algorithm on the training data until convergence ."}
 {"title": "", "sentence": "In other words , instead of using the naive  rule to attach symbols together in parsing data , we now use the  rules and depend on the Inside-Outside algorithm to train these randomly initialized rules intelligently ."}
 {"title": "", "sentence": "This post-pass allows us to express dependencies between adjacent symbols ."}
 {"title": "", "sentence": "In addition , it allows us to train parameters that were fixed during the initial grammar induction phase ."}
 {"title": "", "sentence": "As mentioned , this work employs the ,  ."}
 {"title": "", "sentence": "However ,"}
 {"title": "", "sentence": "Similar research includes work by  and  ."}
 {"title": "", "sentence": "This work also employs a heuristic search within a"}
 {"title": "", "sentence": "However , a different prior probability on grammars is used , and the algorithms are only efficient enough to be applied to small data sets ."}
 {"title": "", "sentence": "The grammar induction algorithms most successful in language modeling include the Inside-Outside algorithm  ,  ,  , a special case of the Expectation-Maximization algorithm  , and work by  ."}
 {"title": "", "sentence": "In the latter work ,"}
 {"title": "", "sentence": "To our knowledge , neither algorithm has surpassed the performance of n-gram models in a language modeling task of substantial scale ."}
 {"title": "", "sentence": "To evaluate our algorithm , we compare the performance of our algorithm to that of n-gram models and the Inside-Outside algorithm ."}
 {"title": "", "sentence": "For n-gram models , we tried  for each domain ."}
 {"title": "", "sentence": "For smoothing a particular n-gram model , we took a linear combination of all lower order n-gram models ."}
 {"title": "", "sentence": "In particular , we follow standard practice  ,  ,  and take the smoothed i-gram probability to be a linear combination of the i-gram frequency in the training data and the smoothed ( i - 1 ) - gram probability , that is ,"}
 {"title": "", "sentence": "where c ( W ) denotes the count of the word sequence W in the training data ."}
 {"title": "", "sentence": "The smoothing parameters  are trained through the Forward-Backward algorithm  on held-out data ."}
 {"title": "", "sentence": "Parameters  are tied together for similar c to prevent data sparsity ."}
 {"title": "", "sentence": "For the Inside-Outside algorithm , we follow the methodology described by"}
 {"title": "", "sentence": "For a given n , we create a probabilistic context-free grammar consisting of all Chomsky normal form rules over the n nonterminal symbols  and the given terminal symbols , that is , all rules"}
 {"title": "", "sentence": "where T denotes the set of terminal symbols in the domain ."}
 {"title": "", "sentence": "All parameters are initialized randomly ."}
 {"title": "", "sentence": "From this starting point , the Inside-Outside algorithm is run until convergence ."}
 {"title": "", "sentence": "For smoothing , we combine the expansion distribution of each symbol with a uniform distribution , that is , we take the smoothed parameter  to be"}
 {"title": "", "sentence": "where  denotes the unsmoothed parameter ."}
 {"title": "", "sentence": "The value  is the number of different ways a symbol expands under the"}
 {"title": "", "sentence": "The parameter  is trained through the Inside-Outside algorithm on held-out data ."}
 {"title": "", "sentence": "This smoothing is also performed on the Inside-Outside post-pass of our algorithm ."}
 {"title": "", "sentence": "For each domain , we tried  ."}
 {"title": "", "sentence": "Because of the computational demands of our algorithm , it is currently impractical to apply it to large vocabulary or large training set problems ."}
 {"title": "", "sentence": "However , we present the results of our algorithm in three medium-sized domains ."}
 {"title": "", "sentence": "In each case , we use 4500 sentences for training , with 500 of these sentences held out for smoothing ."}
 {"title": "", "sentence": "We test on 500 sentences , and measure performance by the entropy of the test data ."}
 {"title": "", "sentence": "In the first two domains , we created the training and test data artificially so as to have an ideal grammar in hand to benchmark results ."}
 {"title": "", "sentence": "In particular , we used a probabilistic grammar to generate the data ."}
 {"title": "", "sentence": "In the first domain , we created this grammar by hand ; the grammar was a small English-like probabilistic context-free grammar consisting of roughly 10 nonterminal symbols , 20 terminal symbols , and 30 rules ."}
 {"title": "", "sentence": "In the second domain , we derived the grammar from manually parsed text ."}
 {"title": "", "sentence": "From a million words of parsed Wall Street Journal data from the Penn treebank , we extracted the 20 most frequently occurring symbols , and the 10 most frequently occurring rules expanding each of these symbols ."}
 {"title": "", "sentence": "For each symbol that occurs on the right-hand side of a rule but which was not one of the most frequent 20 symbols , we create a rule that expands that symbol to a unique terminal symbol ."}
 {"title": "", "sentence": "After removing unreachable rules , this yields a grammar of roughly 30 nonterminals , 120 terminals , and 160 rules ."}
 {"title": "", "sentence": "Parameters are set to reflect the frequency of the corresponding rule in the parsed corpus ."}
 {"title": "", "sentence": "For the third domain , we took English text and reduced the size of the vocabulary by mapping each word to its part-of-speech tag ."}
 {"title": "", "sentence": "We used tagged Wall Street Journal text from the Penn treebank , which has a tag set size of about fifty ."}
 {"title": "", "sentence": "In Tables  -  , we summarize our results ."}
 {"title": "", "sentence": "The ideal grammar denotes the grammar used to generate the training and test data ."}
 {"title": "", "sentence": "For each algorithm , we list the best performance achieved over all n tried , and the best n column states which value realized this performance ."}
 {"title": "", "sentence": "We achieve a moderate but significant improvement in performance over n-gram models and the Inside-Outside algorithm in the first two domains , while in the part-of-speech domain we are outperformed by n-gram models but we vastly outperform the Inside-Outside algorithm ."}
 {"title": "", "sentence": "In Table  , we display a sample of the number of parameters and execution time ( on a Decstation 5000 / 33 ) associated with each algorithm ."}
 {"title": "", "sentence": "We choose n to yield approximately equivalent performance for each algorithm ."}
 {"title": "", "sentence": "The first pass row refers to the main grammar induction phase of our algorithm , and the post-pass row refers to the Inside-Outside post-pass ."}
 {"title": "", "sentence": "Notice that our algorithm produces a significantly more compact model than the n-gram model , while running significantly faster than the Inside-Outside algorithm even though we use an Inside-Outside post-pass ."}
 {"title": "", "sentence": "Part of this discrepancy is due to the fact that we require a smaller number of new nonterminal symbols to achieve equivalent performance , but we have also found that our post-pass converges more quickly even given the same number of nonterminal symbols ."}
 {"title": "", "sentence": "Our algorithm consistently outperformed the Inside-Outside algorithm in these experiments ."}
 {"title": "", "sentence": "While we partially attribute this difference to using a"}
 {"title": "", "sentence": "In particular , though both algorithms employ a greedy hill-climbing strategy , our algorithm gains an advantage by being able to add new rules to the grammar ."}
 {"title": "", "sentence": "In the Inside-Outside algorithm , the gradient descent search discovers the `` nearest '' local minimum in the search landscape to the initial grammar ."}
 {"title": "", "sentence": "If there are k rules in the grammar and thus k parameters , then the search takes place in a fixed k-dimensional space  ."}
 {"title": "", "sentence": "In our algorithm , it is possible to expand the hypothesis grammar , thus increasing the dimensionality of the parameter space that is being searched ."}
 {"title": "", "sentence": "An apparent local minimum in the space  may no longer be a local minimum in the space  ; the extra dimension may provide a pathway for further improvement of the hypothesis grammar ."}
 {"title": "", "sentence": "Hence , our algorithm should be less prone to suboptimal local minima than the Inside-Outside algorithm ."}
 {"title": "", "sentence": "Outperforming n-gram models in the first two domains demonstrates that our algorithm is able to take advantage of the grammatical structure present in data ."}
 {"title": "", "sentence": "However , the superiority of n-gram models in the part-of-speech domain indicates that to be competitive in modeling naturally-occurring data , it is necessary to model collocational information accurately ."}
 {"title": "", "sentence": "We need to modify our algorithm to more aggressively model n-gram information ."}
 {"title": "", "sentence": "This research represents a step forward in the quest for developing grammar-based language models for natural language ."}
 {"title": "", "sentence": "We induce models that , while being substantially more compact , outperform n-gram language models in medium-sized domains ."}
 {"title": "", "sentence": "The algorithm runs essentially in time and space linear in the size of the training data , so larger domains are within our reach ."}
 {"title": "", "sentence": "However , we feel the largest contribution of this work does not lie in the actual algorithm specified , but rather in its indication of the potential of the induction framework described by"}
 {"title": "", "sentence": "We have implemented only a subset of the moves that we have developed , and inspection of our results gives reason to believe that these additional moves may significantly improve the performance of our algorithm ."}
 {"title": "", "sentence": ""}
 {"title": "", "sentence": "After completing the implementation of our move set , we plan to explore the modeling of context-sensitive phenomena ."}
 {"title": "", "sentence": "This work demonstrates that"}
 {"title": "", "sentence": "We describe a corpus-based induction algorithm for probabilistic context-free grammars ."}
 {"title": "", "sentence": "The algorithm employs a greedy heuristic search within a"}
 {"title": "", "sentence": "We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks ."}
 {"title": "", "sentence": "In two of the tasks , the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques ."}
 {"title": "", "sentence": "The third task involves naturally-occurring data , and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Lexicalist approaches to MT , particularly those incorporating the technique of Shake-and-Bake generation  ,  ,  , combine the linguistic advantages of transfer  ,  and interlingual  ,  approaches ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Unfortunately , the generation algorithms described to date have been intractable ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In this paper , we describe an alternative generation component which has polynomial time complexity ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Shake-and-Bake translation assumes a source grammar , a target grammar and a bilingual dictionary which relates translationally equivalent sets of lexical signs , carrying across the semantic dependencies established by the source language analysis stage into the target language generation stage ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The translation process consists of three phases :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A parsing phase , which outputs a multiset , or bag , of source language signs instantiated with sufficiently rich linguistic information established by the parse to ensure adequate translations ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A lexical-semantic transfer phase which employs the bilingual dictionary to map the bag of instantiated source signs onto a bag of target language signs ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A generation phase which imposes an order on the bag of target signs which is guaranteed grammatical according to the monolingual target grammar ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This ordering must respect the linguistic constraints which have been transferred into the target signs ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The Shake-and-Bake generation algorithm of  combines target language signs using the technique known as generate-and-test ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In effect , an arbitrary permutation of signs is input to a shift-reduce parser which tests them for grammatical well-formedness ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If they are well-formed , the system halts indicating success ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If not , another permutation is tried and the process repeated ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The complexity of this algorithm is O(n!) because all permutations ( n! for an input of size n ) may have to be explored to find the correct answer , and indeed must be explored in order to verify that there is no answer ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Proponents of the Shake-and-Bake approach have employed various techniques to improve generation efficiency ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "For example ,  employs a chart to avoid recalculating the same combinations of signs more than once during testing , and  proposes a more general technique for storing which rule applications have been attempted ;  avoids certain pathological cases by employing global constraints on the solution space ; researchers such as  and  provide a system for bag generation that is heuristically guided by probabilities ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "However , none of these approaches is guaranteed to avoid protracted search times if an exact answer is required , because bag generation is NP-complete  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Our novel generation algorithm has polynomial complexity (  ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The reduction in theoretical complexity is achieved by placing constraints on the power of the target grammar when operating on instantiated signs , and by using a more restrictive data structure than a bag , which we call a target language normalised commutative bracketing ( TNCB ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A TNCB records dominance information from derivations and is amenable to incremental updates ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This allows us to employ a greedy algorithm to refine the structure progressively until either a target constituent is found and generation has succeeded or no more changes can be made and generation has failed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In the following sections , we will sketch the basic algorithm , consider how to provide it with an initial guess , and provide an informal proof of its efficiency ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We begin by describing the fundamentals of a greedy incremental generation algorithm ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The crucial data structure that it employs is the TNCB ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We give some definitions , state some key assumptions about suitable TNCBs for generation , and then describe the algorithm itself ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We assume a sign-based grammar with binary rules , each of which may be used to combine two signs by unifying them with the daughter categories and returning the mother ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Combination is the commutative equivalent of rule application ; the linear ordering of the daughters that leads to successful rule application determines the orthography of the mother ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": ""}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "However , the target derivation information itself is not used to assist the algorithm ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Even in  , the derivation information is used simply to cache previous results to avoid exact recomputation at a later stage , not to improve on previous guesses ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The reason why we believe such improvement is possible is that , given adequate information from the previous stages , two target signs cannot combine by accident ; they must do so because the underlying semantics within the signs licenses it ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If the linguistic data that two signs contain allows them to combine , it is because they are providing a semantics which might later become more specified ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "For example , consider the bag of signs that have been derived through the Shake-and-Bake process which represent the phrase :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Now , since the determiner and adjectives all modify the same noun , most grammars will allow us to construct the phrases :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "as well as the ` correct ' one ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Generation will fail if all signs in the bag are not eventually incorporated in the final result , but in the naive algorithm , the intervening computation may be intractable ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In the algorithm presented here , we start from observation that the phrases  to  are not incorrect semantically ; they are simply under-specifications of  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We take advantage of this by recording the constituents that have combined within the TNCB , which is designed to allow further constituents to be incorporated with minimal recomputation ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A TNCB is composed of a sign , and a history of how it was derived from its children ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The structure is essentially a binary derivation tree whose children are unordered ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Concretely , it is either NIL , or a triple :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The second and third items of the TNCB triple are the child TNCBs ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The value of a TNCB is the sign that is formed from the combination of its children , or INCONSISTENT , representing the fact that they cannot grammatically combine , or UNDETERMINED , i.e. it has not yet been established whether the signs combine ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Undetermined TNCBs are commutative , e.g. they do not distinguish between the structures shown in Figure  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In section  we will see that this property is important when starting up the generation process ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Let us introduce some terminology ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A TNCB is"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "well-formed iff its value is a sign ,"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "ill-formed iff its value is INCONSISTENT ,"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "undetermined ( and its value is UNDETERMINED ) iff it has not been demonstrated whether it is well-formed or ill-formed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "maximal iff it is well-formed and its parent ( if it has one ) is ill-formed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In other words , a maximal TNCB is a largest well-formed component of a TNCB ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Since TNCBs are tree-like structures , if a TNCB is undetermined or ill-formed then so are all of its ancestors ( the TNCBs that contain it ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We define five operations on a TNCB ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The first three are used to define the fourth transformation ( move ) which improves ill-formed TNCBs ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The fifth is used to establish the well-formedness of undetermined nodes ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In the diagrams , we use a cross to represent ill-formed nodes and a black circle to represent undetermined ones ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Deletion :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A maximal TNCB can be deleted from its current position ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The structure above it must be adjusted in order to maintain binary branching ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In figure  , we see that when node 4 is deleted , so is its parent node 3 ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The new node 6 , representing the combination of 2 and 5 , is marked undetermined ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Conjunction :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A maximal TNCB can be conjoined with another maximal TNCB if they may be combined by rule ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In figure  , it can be seen how the maximal TNCB composed of nodes 1 , 2 , and 3 is conjoined with the maximal TNCB composed of nodes 4 , 5 and 6 giving the TNCB made up of nodes 1 to 7 ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The new node , 7 , is well-formed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Adjunction :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "A maximal TNCB can be inserted inside a maximal TNCB , i.e. conjoined with a non-maximal TNCB , where the combination is licensed by rule ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In figure  , the TNCB composed of nodes 1 , 2 , and 3 is inserted inside the TNCB composed of nodes 4 , 5 and 6 ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "All nodes ( only 8 in figure  ) which dominate the node corresponding to the new combination ( node 7 ) must be marked undetermined -- such nodes are said to be disrupted ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Movement :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This is a combination of a deletion with a subsequent conjunction or adjunction ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In figure  , we illustrate a move via conjunction ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In the left-hand figure , we assume we wish to move the maximal TNCB 4 next to the maximal TNCB 7 ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This first involves deleting TNCB 4 ( noting it ) , and raising node 3 to replace node 2 ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We then introduce node 8 above node 7 , and make both nodes 7 and 4 its children ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Note that during deletion , we remove a surplus node ( node 2 in this case ) and during conjunction or adjunction we introduce a new one ( node 8 in this case ) thus maintaining the same number of nodes in the tree ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Evaluation :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "After a movement , the TNCB is undetermined as demonstrated in figure  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The signs of the affected parts must be recalculated by combining the recursively evaluated child TNCBs ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The Shake-and-Bake system of  employs a bag generation algorithm because it is assumed that the input to the generator is no more than a collection of instantiated signs ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Full-scale bag generation is not necessary because sufficient information can be transferred from the source language to severely constrain the subsequent search during generation ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The two properties required of TNCBs ( and hence the target grammars with instantiated lexical signs ) are :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Precedence Monotonicity ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The order of the orthographies of two combining signs in the orthography of the result must be determinate -- it must not depend on any subsequent combination that the result may undergo ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This constraint says that if one constituent fails to combine with another , no permutation of the elements making up either would render the combination possible ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This allows bottom-up evaluation to occur in linear time ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In practice , this restriction requires that sufficiently rich information be transferred from the previous translation stages to ensure that sign combination is deterministic ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Dominance Monotonicity ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If a maximal TNCB is adjoined at the highest possible place inside another TNCB , the result will be well-formed after it is re-evaluated ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Adjunction is only attempted if conjunction fails ( in fact conjunction is merely a special case of adjunction in which no nodes are disrupted ) ; an adjunction which disrupts i nodes is attempted before one which disrupts i + 1 nodes ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Dominance monotonicity merely requires all nodes that are disrupted under this top-down control regime to be well-formed when re-evaluated ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We will see that this will ensure the termination of the generation algorithm within n - 1 steps , where n is the number of lexical signs input to the process ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We are currently investigating the mathematical characterisation of grammars and instantiated signs that obey these constraints ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "So far , we have not found these restrictions particularly problematic ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The generator cycles through two phases : a test phase and a rewrite phase ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Imagine a bag of signs , corresponding to `` the big brown dog barked '' , has been passed to the generation phase ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The first step in the generation process is to convert it into some arbitrary TNCB structure , say the one in figure  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In order to verify whether this structure is valid , we evaluate the TNCB ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This is the test phase ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If the TNCB evaluates successfully , the orthography of its value is the desired result ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If not , we enter the rewrite phase ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If we were continuing in the spirit of the original Shake-and-Bake generation process , we would now form some arbitrary mutation of the TNCB and retest , repeating this test-rewrite cycle until we either found a well-formed TNCB or failed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "However , this would also be intractable due to the undirectedness of the search through the vast number of possibilities ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Given the added derivation information contained within TNCBs and the properties mentioned above , we can direct this search by incrementally improving on previously evaluated results ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We enter the rewrite phase , then , with an ill-formed TNCB. Each move operation must improve it ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Let us see why this is so ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The move operation maintains the same number of nodes in the tree ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The deletion of a maximal TNCB removes two ill-formed nodes ( figure  ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "At the deletion site , a new undetermined node is created , which may or may not be ill-formed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "At the destination site of the movement ( whether conjunction or adjunction ) , a new well-formed node is created ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The ancestors of the new well-formed node will be at least as well-formed as they were prior to the movement ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We can verify this by case :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "When two maximal TNCBs are conjoined , nodes dominating the new node , which were previously ill-formed , become undetermined ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "When re-evaluated , they may remain ill-formed or some may now become well-formed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "When we adjoin a maximal TNCB within another TNCB , nodes dominating the new well-formed node are disrupted ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "By dominance monotonicity , all nodes which were disrupted by the adjunction must become well-formed after re-evaluation ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "And nodes dominating the maximal disrupted node , which were previously ill-formed , may become well-formed after re-evaluation ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We thus see that rewriting and re-evaluating must improve the TNCB ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Let us further consider the contrived worst-case starting point provided in figure  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "After the test phase , we discover that every single interior node is ill-formed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We then scan the TNCB , say top-down from left to right , looking for a maximal TNCB to move ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In this case , the first move will be PAST to bark , by conjunction ( figure  ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Once again , the test phase fails to provide a well-formed TNCB , so we repeat the rewrite phase , this time finding dog to conjoin with the ( figure  shows the state just after the second pass through the test phase ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "After further testing , we again re-enter the rewrite phase and this time note that brown can be inserted in the maximal TNCB the dog barked adjoined with dog ( figure  ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Note how , after combining dog and the , the parent sign reflects the correct orthography even though they did not have the correct linear precedence ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "After finding that big may not be conjoined with the brown dog , we try to adjoin it within the latter ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Since it will combine with brown dog , no adjunction to a lower TNCB is attempted ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The final result is the TNCB in figure  , whose orthography is `` the big brown dog barked '' ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We thus see that during generation , we formed a basic constituent , the dog , and incrementally refined it by adjoining the modifiers in place ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "At the heart of this approach is that , once well-formed , constituents can only grow ; they can never be dismantled ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Even if generation ultimately fails , maximal well-formed fragments will have been built ; the latter may be presented to the user , allowing graceful degradation of output quality ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Considering the algorithm described above , we note that the number of rewrites necessary to repair the initial guess is no more than the number of ill-formed TNCBs ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This can never exceed the number of interior nodes of the TNCB formed from n lexical signs ( i.e. n - 2 ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Consequently , the better formed the initial TNCB used by the generator , the fewer the number of rewrites required to complete generation ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In the last section , we deliberately illustrated an initial guess which was as bad as possible ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In this section , we consider a heuristic for producing a motivated guess for the initial TNCB ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Consider the TNCBs in figure  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If we interpret the S , O and V as Subject , Object and Verb we can observe an equivalence between the structures with the bracketings :  ,  ,  ,  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The implication of this equivalence is that if , say , we are translating into a  language from a head-final language and have isomorphic dominance structures between the source and target parses , then simply mirroring the source parse structure in the initial target TNCB will provide a correct initial guess ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "For example , the English sentence  :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "has a corresponding Japanese equivalent  :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "If we mirror the Japanese bracketing structure in English to form the initial TNCB , we obtain : ( ( book the ) ( red is ) ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This will produce the correct answer in the test phase of generation without the need to rewrite at all ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Even if there is not an exact isomorphism between the source and target commutative bracketings , the first guess is still reasonable as long as the majority of child commutative bracketings in the target language are isomorphic with their equivalents in the source language ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Consider the French sentence :"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The TNCB implied by the bracketing in  is equivalent to that in figure  and requires just one rewrite in order to make it well-formed ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We thus see how the TNCBs can mirror the dominance information in the source language parse in order to furnish the generator with a good initial guess ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "On the other hand , no matter how the SL and TL structures differ , the algorithm will still operate correctly with polynomial complexity ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Structural transfer can be incorporated to improve the efficiency of generation , but it is never necessary for correctness or even tractability ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The theoretical complexity of the generator is  , where n is the size of the input ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We give an informal argument for this ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The complexity of the test phase is the number of evaluations that have to be made ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Each node must be tested no more than twice in the worst case ( due to precedence monotonicity ) , as one might have to try to combine its children in either direction according to the grammar rules ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "There are always exactly n - 1 non-leaf nodes , so the complexity of the test phase is O(n) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The complexity of the rewrite phase is that of locating the two TNCBs to be combined ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In the worst case , we can imagine picking an arbitrary child TNCB ( O(n) ) and then trying to find another one with which it combines ( O(n) ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The complexity of this phase is therefore the product of the picking and combining complexities , i.e.  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The combined complexity of the test-rewrite cycle is thus  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Now , in section  , we argued that no more than n - 1 rewrites would ever be necessary , thus the overall complexity of generation ( even when no solution is found ) is  ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Average case complexity is dependent on the quality of the first guess , how rapidly the TNCB structure is actually improved , and to what extent the TNCB must be re-evaluated after rewriting ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "In the SLEMaT system  , we have tried to form a good initial guess by mirroring the source structure in the target TNCB , and allowing some local structural modifications in the bilingual equivalences ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Structural transfer operations only affect the efficiency and not the functionality of generation ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Transfer specifications may be incrementally refined and empirically tested for efficiency ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Since complete specification of transfer operations is not required for correct generation of grammatical target text , the version of Shake-and-Bake translation presented here maintains its advantage over traditional transfer models , in this respect ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The monotonicity constraints , on the other hand , might constitute a dilution of the Shake-and-Bake ideal of independent grammars ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "For instance , precedence monotonicity requires that the status of a clause ( strictly , its lexical head ) as main or subordinate has to be transferred into German ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "It is not that the transfer of information per se compromises the ideal -- such information must often appear in transfer entries to avoid grammatical but incorrect translation ( e.g. a great man translated as un homme grand ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The problem is justifying the main / subordinate distinction in every language that we might wish to translate into German ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "This distinction can be justified monolingually for the other languages that we treat ( English , French , and Japanese ) ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Whether the constraints will ultimately require monolingual grammars to be enriched with entirely unmotivated features will only become clear as translation coverage is extended and new language pairs are added ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We have presented a polynomial complexity generation algorithm which can form part of any Shake-and-Bake style MT system with suitable grammars and information transfer ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The transfer module is free to attempt structural transfer in order to produce the best possible first guess ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We tested a TNCB-based generator in the SLEMaT MT system with the pathological cases described in  against"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "Somewhat more surprisingly , even for short sentences which were not problematic for"}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "The lexicalist approach to Machine Translation offers significant advantages in the development of linguistic descriptions ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "However , the Shake-and-Bake generation algorithm of  is NP-complete ."}
 {"title": "An Efficient Generation Algorithm for Lexicalist MT", "sentence": "We present a polynomial time algorithm for lexicalist MT generation provided that sufficient information can be transferred to ensure more determinism ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional techniques become increasingly widespread  ,  ,  ,  ,  ,  ,  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "However , for many tasks , one is interested in relationships among word senses , not words ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Consider , for example , the cluster containing attorney , counsel , trial , court , and judge , used by  to illustrate a `` semantically sticky '' group of words ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "As is often the case where sense ambiguity is involved , we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Yet a computational system has no choice but to consider other , more awkward possibilities -- for example , this cluster might be capturing a distributional relationship between advice ( as one sense of counsel ) and royalty ( as one sense of court ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This would be a mistake for many applications , such as query expansion in information retrieval , where a surfeit of false connections can outweigh the benefits obtained by using lexical knowledge ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "One obvious solution to this problem would be to extend distributional grouping methods to word senses ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For example , one could construct vector representations of senses on the basis of their co-occurrence with words or with other senses ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Unfortunately , there are few corpora annotated with word sense information , and computing reliable statistics on word senses rather than words will require more data , rather than less ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Furthermore , one widely available example of a large , manually sense-tagged corpus -- the WordNet group 's annotated subset of the Brown corpus -- vividly illustrates the difficulty in obtaining suitable data ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "It is quite small , by current corpus standards ( on the order of hundreds of thousands of words , rather than millions or tens of millions ) ; the direct annotation methodology used to create it is labor intensive (  found that direct annotation takes twice as long as automatic tagging plus correction , for part-of-speech annotation ) ; and the output quality reflects the difficulty of the task ( inter-annotator disagreement is on the order of 10 % , as contrasted with the approximately 3 % error rate reported for part-of-speech annotation by"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "There have been some attempts to capture the behavior of semantic categories in a distributional setting , despite the unavailability of sense-annotated corpora ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For example ,  take steps toward a distributional treatment of WordNet-based classes , using  's approach to constructing vector representations from a large co-occurrence matrix ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "'s algorithm for sense disambiguation can be thought of as a way of determining how Roget 's thesaurus categories behave with respect to contextual features ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "And my own treatment of selectional constraints  provides a way to describe the plausibility of co-occurrence in terms of WordNet 's semantic categories , using co-occurrence relationships mediated by syntactic structure ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In each case , one begins with known semantic categories ( WordNet synsets , Roget 's numbered classes ) and non-sense-annotated text , and proceeds to a distributional characterization of semantic category behavior using co-occurrence relationships ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This paper begins from a rather different starting point ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "As in the above-cited work , there is no presupposition that sense-annotated text is available ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Here , however , I make the assumption that word groupings have been obtained through some black box procedure , e.g. from analysis of unannotated text , and the goal is to annotate the words within the groupings post hoc using a knowledge-based catalogue of senses ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "If successful , such an approach has obvious benefits : one can use whatever sources of good word groupings are available -- primarily unsupervised word clustering methods , but also on-line thesauri and the like -- without folding in the complexity of dealing with word senses at the same time ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The resulting sense groupings should be useful for a variety of purposes , although ultimately this work is motivated by the goal of sense disambiguation for unrestricted text using unsupervised methods ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Let us state the problem as follows ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "We are given a set of words  , with each word  having an associated set  of possible senses ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "We assume that there exists some set  , representing the set of word senses that an ideal human judge would conclude belong to the group of senses corresponding to the word grouping W ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The goal is then to define a membership function  that takes  ,  , and W as its arguments and computes a value in [ 0,1 ] , representing the confidence with which one can state that sense  belongs in sense grouping W ' ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Note that , in principle , nothing precludes the possibility that multiple senses of a word are included in W ' ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Example ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Consider the following word group :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Restricting our attention to noun senses in WordNet , only lookout and crate are polysemous ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Treating this word group as W , one would expect  to assign a value of 1 to the unique senses of the monosemous words , and to assign a high value to lookout 's sense as"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Low ( or at least lower ) values of  would be expected for the senses of lookout that correspond to an observation tower , or to the activity of watching ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Crate 's two WordNet senses correspond to the physical object and the quantity ( i.e. , crateful , as in `` a crateful of oranges '' ) ; my own intuition is that the first of these would more properly be included in W ' than the second , and should therefore receive a higher value of  , though of course neither I nor any other individual really constitutes an `` ideal human judge . ''"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The core of the disambiguation algorithm is a computation of semantic similarity using the WordNet taxonomy , a topic recently investigated by a number of people  , ,  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In this paper , I restrict my attention to WordNet 's taxonomy for nouns , and take an approach in which semantic similarity is evaluated on the basis of the information content shared by the items being compared ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The intuition behind the approach is simple : the more similar two words are , the more informative will be the most specific concept that subsumes them both ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "( That is , their least upper bound in the taxonomy ; here a concept corresponds to a WordNet synset . )"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes  ,  also captures this , albeit indirectly , when the semantic network is just an hierarchy : if the minimal path of links between two nodes is long , that means it is necessary to go high in the taxonomy , to more abstract concepts , in order to find their least upper bound ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "However , there are problems with the simple path-length definition of semantic similarity , and experiments using WordNet show that other measures of semantic similarity , such as the one employed here , provide a better match to human similarity judgments than simple path length does  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Given two words  and  , their semantic similarity is calculated as"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "where  is the set of WordNet synsets that subsume ( i.e. , are ancestors of ) both  and  , in any sense of either word ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The concept c that maximizes the expression in  will be referred to as the most informative subsumer of  and  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Although there are many ways to associate probabilities with taxonomic classes , it is reasonable to require that concept probability be non-decreasing as one moves higher in the taxonomy ; i.e. , that   implies  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This guarantees that `` more abstract '' does indeed mean `` less informative , '' defining informativeness in the traditional way in terms of log likelihood ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Probability estimates are derived from a corpus by computing"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "where  is the set of nouns having a sense subsumed by concept c. Probabilities are then computed simply as relative frequency :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "where N is the total number of noun instances observed ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Singular and plural forms are counted as the same noun , and nouns not covered by WordNet are ignored ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Although the WordNet noun taxonomy has multiple root nodes , a single , `` virtual '' root node is assumed to exist , with the original root nodes as its children ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Note that by equations  through  , if two senses have the virtual root node as their only upper bound then their similarity value is 0 ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Example ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The following table shows the semantic similarity computed for several word pairs , in each case shown with the most informative subsumer ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Probabilities were estimated using the Penn Treebank version of the Brown corpus ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The pairs come from an example given by  , illustrating the words that human subjects most frequently judged as being associated with the word doctor ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "( The word sick also appeared on the list , but is excluded here because it is not a noun . )"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Doctors are minimally similar to medicine and hospitals , since these things are all instances of `` something having concrete existence , living or nonliving '' ( WordNet class entity ) , but they are much more similar to lawyers , since both are kinds of professional people , and even more similar to nurses , since both are professional people working specifically within the health professions ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Notice that similarity is a more specialized notion than association or relatedness : doctors and sickness may be highly associated , but one would not judge them to be particularly similar ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The disambiguation algorithm for noun groups is inspired by the observation that when two polysemous words are similar , their most informative subsumer provides information about which sense of each word is the relevant one ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In the above table , for example , both doctor and nurse are polysemous : WordNet records doctor not only as a kind of health professional , but also as someone who holds a Ph. D. , and nurse can mean not only a health professional but also a nanny ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "When the two words are considered together , however , the shared element of meaning for the two relevant senses emerges in the form of the most informative subsumer ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "It may be that other pairings of possible senses also share elements of meaning ( for example , doctor / Ph. D. and nurse / nanny are both descendants of person , individual ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "However , in cases like those illustrated above , the more specific or informative the shared ancestor is , the more strongly it suggests which senses come to mind when the words are considered together ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The working hypothesis in this paper is that this holds true in general ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Turning that observation into an algorithm requires two things : a way to assign credit to word senses based on similarity with co-occurring words , and a tractable way to generalize to the case where more than two polysemous words are involved ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The algorithm given in Figure  does both quite straightforwardly ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This algorithm considers the words in W pairwise , avoiding the tractability problems in considering all possible combinations of senses for the group (  if each word had m senses ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For each pair considered , the most informative subsumer is identified , and this pair is only considered as supporting evidence for those senses that are descendants of that concept ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Notice that by equation  , support [ i , k ] is a sum of log probabilities , and therefore preferring senses with high support is equivalent to optimizing a product of probabilities ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Thus considering words pairwise in the algorithm reflects a probabilistic independence assumption ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Example ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The most informative subsumer for doctor and nurse is health professional , and therefore that pairing contributes support to the sense of doctor as an M.D. , but not a Ph. D."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Similarly , it contributes support to the sense of nurse as a health professional , but not a nanny ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The amount of support contributed by a pairwise comparison is proportional to how informative the most informative subsumer is ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Therefore the evidence for the senses of a word will be influenced more by more similar words and less by less similar words ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "By the time this process is completed over all pairs , each sense of each word in the group has had the potential of receiving supporting evidence from a pairing with every other word in the group ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The value assigned to that sense is then the proportion of support it did receive , out of the support possible ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "( The latter is kept track of by array normalization in the pseudocode . )"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The intuition behind this algorithm is essentially the same intuition exploited by  ,  , and others : the most plausible assignment of senses to multiple co-occurring words is the one that maximizes relatedness of meaning among the senses chosen ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Here I make an explicit comparison with"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": ""}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "From this ,"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "These are the non-stopword nouns in the paragraph that appear in WordNet ( he used version 1.2 ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The description of"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "However , there are some important differences , as well ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "First , unlike"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This provides some justification for restricting attention to similarity ( reflected by the scaffolding of links in the taxonomy ) , as opposed to the more general notion of association ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Second , this difference is reflected algorithmically by the fact that"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Third , unlike ,  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Fourth , the combinatorics are handled differently :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The algorithm presented here falls between those two alternatives ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "A final , important difference between this algorithm and previous algorithms for sense disambiguation is that it offers the possibility of assigning higher-level WordNet categories rather than lowest-level sense labels ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "It is a simple modification to the algorithm to assign values of  not only to synsets directly containing words in W , but to any ancestors of those synsets -- one need only let the list of synsets associated with each word  ( i.e. ,  in the problem statement of Section  ) also include any synset that is an ancestor of any synset containing word  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Assuming that num _ senses ( w [ i ] ) and sense [ i , k ] are reinterpreted accordingly , the algorithm will compute  not only for the synsets directly including words in W , but also for any higher-level abstractions of them ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Example ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Consider the word group doctor , nurse , lawyer ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "If one were to include all subsuming concepts for each word , rather than just the synsets of which they are directly members , the concepts with non-zero values of  would be as follows :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For doctor :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For nurse :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For lawyer :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Given assignments of  at all levels of abstraction , one obvious method of semantic annotation is to assign the highest-level concept for which  is at least as large as the sense-specific value of  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For instance , in the previous example , one would assign the annotation health professional to both doctor and nurse ( thus explicitly capturing a generalization about their presence in the word group , at the appropriate level of abstraction ) , and the annotation professional to lawyer ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In this section I present a number of examples for evaluation by inspection ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In each case , I give the source of the noun grouping , the grouping itself , and for each word a description of word senses together with their values of  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Distributional cluster  : head , body , hands , eye , voice , arm , seat , hair , mouth ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This group was among classes hand-selected by"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Distributional cluster  : tie , jacket , suit ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This cluster was derived by"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Distributional cluster  : cost , expense , risk , profitability , deferral , earmarks , capstone , cardinality , mintage , reseller ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This cluster was one presented by"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "( I hand-selected it from that group for presentation here , however . )"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Distributional neighborhood  : burglars , thief , rob , mugging , stray , robbing , lookout , chase , crate ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "As noted in Section  , this group represents a set of words similar to burglar , according to"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In this case , words rob and robbing were excluded because they were not nouns in WordNet ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The word stray probably should be excluded also , since it most likely appears on this list as an adjective ( as in `` stray bullet '' ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Machine-generated thesaurus entry  : method , test , mean , procedure , technique ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "I chose this grouping at random from a thesaurus created automatically by"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The group comes from from the thesaurus entry for the word method ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Note that mean probably should be means ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "There is a tradition in sense disambiguation of taking particularly ambiguous words and evaluating a system 's performance on those words ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Here I look at one such case , the word line ; the goal is to see what sense the algorithm chooses when considering the word in the contexts of each of the Roget 's Thesaurus classes in which it appears , where a `` class '' includes all the nouns in one of the numbered categories ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The following list provides brief descriptions of the 25 senses of line in WordNet :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Since line appears in 13 of the numbered categories in Roget 's thesaurus , a full description of the values of  would be too large for the present paper ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Indeed , showing all the nouns in the numbered categories would take up too much space : they average about 70 nouns apiece ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Instead , I identify the numbered category , and give the three WordNet senses of line for which  was greatest ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Qualitatively , the algorithm does a good job in most of the categories ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The reader might find it an interesting exercise to try to decide which of the 25 senses he or she would choose , especially in the cases where the algorithm did less well ( e.g. categories #200 , #203 , #466 ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The previous section provided illustrative examples , demonstrating the performance of the algorithm on some interesting cases ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In this section , I present experimental results using a more rigorous evaluation methodology ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Input for this evaluation came from the numbered categories of Roget 's ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Test instances consisted of a noun group ( i.e. , all the nouns in a numbered category ) together with a single word in that group to be disambiguated ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "To use an example from the previous section , category #590 ( `` Writing '' ) contains the following :"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Any word or phrase in that group that appears in the noun taxonomy for WordNet would be a candidate as a test instance -- for example , line , or secret writing ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The test set , chosen at random , contained 125 test cases ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "( Note that because of the random choice , there were some cases where more than one test instance came from the same numbered category . )"}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Two human judges were independently given the test cases to disambiguate ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For each case , they were given the full set of nouns in the numbered category ( as shown above ) together with descriptions of the WordNet senses for the word to be disambiguated ( as , for example , the list of 25 senses for line given in the previous section , though thankfully few words have that many senses ! ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "It was a forced-choice task ; that is , the judge was required to choose exactly one sense ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In addition , for each judgment , the judge was required to provide a confidence value for this decision , ranging from 0 ( not at all confident ) to 4 ( highly confident ) ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Results are presented here individually by judge ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For purposes of evaluation , test instances for which the judge had low confidence ( i.e. confidence ratings of 0 or 1 ) were excluded ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For Judge 1 , there were 99 test instances with sufficiently high confidence to be considered ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "As a baseline , ten runs were done selecting senses by random choice , with the average percent correct being 34.8 % , standard deviation 3.58 ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "As an upper bound , Judge 2 was correct on 65.7 % of those test instances ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The disambiguation algorithm shows considerable progress toward this upper bound , with 58.6 % correct ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "For Judge 2 , there were 86 test instances with sufficiently high confidence to be considered ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "As a baseline , ten runs were done selecting senses by random choice , with the average percent correct being 33.3 % , standard deviation 3.83 ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "As an upper bound , Judge 1 was correct on 68.6 % of those test instances ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Again , the disambiguation algorithm performs well , with 60.5 % correct ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The results of the evaluation are extremely encouraging , especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs ,  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "A note worth adding : it is not clear that the `` exact match '' criterion -- that is , evaluating algorithms by the percentage of exact matches of sense selection against a human-judged baseline -- is the right task ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In particular , in many tasks it is at least as important to avoid inappropriate senses than to select exactly the right one ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This would be the case in query expansion for information retrieval , for example , where indiscriminately adding inappropriate words to a query can degrade performance  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The examples presented in Section  are encouraging in this regard : in addition to performing well at the task of assigning a high score to the best sense , it does a good job of assigning low scores to senses that are clearly inappropriate ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Regardless of the criterion for success , the algorithm does need further evaluation ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Immediate plans include a larger scale version of the experiment presented here , involving thesaurus classes , as well as a similarly designed evaluation of how the algorithm fares when presented with noun groups produced by distributional clustering ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "In addition , I plan to explore alternative measures of semantic similarity , for example an improved variant on simple path length that has been proposed by  ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Ultimately , this algorithm is intended to be part of a suite of techniques used for disambiguating words in running text with respect to WordNet senses ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "I would argue that success at that task will require combining knowledge of the kind that WordNet provides , primarily about relatedness of meaning , with knowledge of the kind best provided by corpora , primarily about usage in context ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The difficulty with the latter kind of knowledge is that , until now , the widespread success in characterizing lexical behavior in terms of distributional relationships has applied at the level of words -- indeed , word forms -- as opposed to senses ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This paper represents a step toward getting as much leverage as possible out of work within that paradigm , and then using it to help determine relationships among word senses , which is really where the action is ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Word groupings useful for language processing tasks are increasingly available , as thesauri appear on-line , and as distributional word clustering techniques improve ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "However , for many tasks , one is interested in relationships among word senses , not words ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns -- the kind of data one finds in on-line thesauri , or as the output of distributional clustering algorithms ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "Disambiguation is performed with respect to WordNet senses , which are fairly fine-grained ; however , the method also permits the assignment of higher-level WordNet categories rather than sense labels ."}
 {"title": "Disambiguating Noun Groupings with Respect to WordNet Senses", "sentence": "The method is illustrated primarily by example , though results of a more rigorous evaluation are also presented ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "If parsing is taken to be the first step in taming the natural language understanding task , then broad coverage NLP remains a jungle inhabited by wild beasts ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For instance , parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Yet , far from being an obscure , endangered species , the noun compound is flourishing in modern language ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "While substantial work on noun compounds exists in both linguistics  ,  and computational linguistics  ,  ,  , techniques suitable for broad coverage parsing remain unavailable ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This paper explores the application of corpus statistics  to noun compound parsing ( other computational problems are addressed in  ,  and  ) ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The task is illustrated in example  :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The parses assigned to these two compounds differ , even though the sequence of parts of speech are identical ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The problem is analogous to the prepositional phrase attachment task explored in  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The approach they propose involves computing lexical associations from a corpus and using these to select the correct parse ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A similar architecture may be applied to noun compounds ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In the experiments below the accuracy of such a system is measured ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Comparisons are made across five dimensions :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Each of two analysis models are applied : adjacency and dependency ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Each of a range of training schemes are employed ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Results are computed with and without tuning factors suggested in the literature ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Each of two parameterisations are used : associations between words and associations between concepts ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Results are collected with and without machine tagging of the corpus ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "While  use a partial parser to acquire training data , such machinery appears unnecessary for noun compounds ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "has proposed the use of simple word patterns for the acquisition of verb subcategorisation information ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "An analogous approach to compounds is used in  and constitutes one scheme evaluated below ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "While such patterns produce false training examples , the resulting noise often only introduces minor distortions ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A more liberal alternative is the use of a co-occurrence window ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "uses a fixed 100 word window to collect information used for sense disambiguation ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Similarly ,  uses a six content word window to extract significant collocations ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A range of windowed training schemes are employed below ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Importantly , the use of a window provides a natural means of trading off the amount of data against its quality ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "When data sparseness undermines the system accuracy , a wider window may admit a sufficient volume of extra accurate data to outweigh the additional noise ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "There are at least four existing corpus-based algorithms proposed for syntactically analysing noun compounds ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Only two of these have been subjected to evaluation , and in each case , no comparison to any of the other three was performed ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In fact all authors appear unaware of the other three proposals ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "I will therefore briefly describe these algorithms ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Three of the algorithms use what I will call the ADJACENCY MODEL , an analysis procedure that goes back to  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Therein , the procedure is stated in terms of calls to an oracle which can determine if a noun compound is acceptable ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "It is reproduced here for reference :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Given three nouns  ,  and  :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "If either [   ] or [   ] is not semantically acceptable then build the alternative structure ;"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "otherwise , if [   ] is semantically preferable to [   ] then build [   ] ;"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "otherwise , build [   ] ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Only more recently has it been suggested that corpus statistics might provide the oracle , and this idea is the basis of the three algorithms which use the adjacency model ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The simplest of these is reported in  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Given a three word compound , a search is conducted elsewhere in the corpus for each of the two possible subcomponents ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Whichever is found is then chosen as the more closely bracketed pair ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For example , when backup compiler disk is encountered , the analysis will be :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Since this is proposed merely as a rough heuristic , it is not stated what the outcome is to be if neither or both subcomponents appear ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Nor is there any evaluation of the algorithm ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The proposal of  is more sophisticated and allows for the frequency of the words in the compound ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Again , there is no evaluation of the method other than a demonstration that four examples work correctly ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The third proposal based on the adjacency model appears in  and is rather more complex again ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The SELECTIONAL ASSOCIATION between a predicate and a word is defined based on the contribution of the word to the conditional entropy of the predicate ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The association between each pair of words in the compound is then computed by taking the maximum selectional association from all possible ways of regarding the pair as predicate and argument ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Whilst this association metric is complicated , the decision procedure still follows the outline devised by  above ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "used unambiguous noun compounds from the parsed Wall Street Journal ( WSJ ) corpus to estimate the association values and analysed a test set of around 160 compounds ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "After some tuning , the accuracy was about 73 % , as compared with a baseline of 64 % achieved by always bracketing the first two nouns together ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The fourth algorithm , first described in  , differs in one striking manner from the other three ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "It uses what I will call the DEPENDENCY MODEL ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This model utilises the following procedure when given three nouns  ,  and  :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Determine how acceptable the structures [   ] and [   ] are ;"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "if the latter is more acceptable , build [   ] first ;"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "otherwise , build [   ] first ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Figure  shows a graphical comparison of the two analysis models ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In  , the degree of acceptability is again provided by statistical measures over a corpus ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The metric used is a mutual information-like measure based on probabilities of modification relationships ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This is derived from the idea that parse trees capture the structure of semantic relationships within a noun compound ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The dependency model attempts to choose a parse which makes the resulting relationships as acceptable as possible ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For example , when backup compiler disk is encountered , the analysis will be :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "I claim that the dependency model makes more intuitive sense for the following reason ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Consider the compound calcium ion exchange , which is typically left-branching ( that is , the first two words are bracketed together ) ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "There does not seem to be any reason why calcium ion should be any more frequent than ion exchange ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Both are plausible compounds and regardless of the bracketing , ions are the object of an exchange ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Instead , the correct parse depends on whether calcium characterises the ions or mediates the exchange ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Another significant difference between the models is the predictions they make about the proportion of left and right-branching compounds ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "show that under a dependency model , left-branching compounds should occur twice as often as right-branching compounds ( that is two-thirds of the time ) ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In the test set used here and in that of  , the proportion of left-branching compounds is 67 % and 64 % respectively ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In contrast , the adjacency model appears to predict a proportion of 50 ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The dependency model has also been proposed by  for analysing Japanese noun compounds , apparently independently ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Using a corpus to acquire associations , they bracket sequences of Kanji with lengths four to six ( roughly equivalent to two or three words ) ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A simple calculation shows that using their own preprocessing hueristics to guess a bracketing provides a higher accuracy on their test set than their statistical model does ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This renders their experiment inconclusive ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A test set of syntactically ambiguous noun compounds was extracted from our 8 million word Grolier 's encyclopedia corpus in the following way ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Because the corpus is not tagged or parsed , a somewhat conservative strategy of looking for unambiguous sequences of nouns was used ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "To distinguish nouns from other words , the University of Pennsylvania morphological analyser ( described in  ) was used to generate the set of words that can only be used as nouns ( I shall henceforth call this set  ) ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "All consecutive sequences of these words were extracted , and the three word sequences used to form the test set ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For reasons made clear below , only sequences consisting entirely of words from Roget 's thesaurus were retained , giving a total of 308 test triples ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "These triples were manually analysed using as context the entire article in which they appeared ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In some cases , the sequence was not a noun compound ( nouns can appear adjacent to one another across various constituent boundaries ) and was marked as an error ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Other compounds exhibited what  have termed SEMANTIC INDETERMINACY where the two possible bracketings cannot be distinguished in the context ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The remaining compounds were assigned either a left-branching or right-branching analysis ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Table  shows the number of each kind and an example of each ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Accuracy figures in all the results reported below were computed using only those 244 compounds which received a parse ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "One problem with applying lexical association to noun compounds is the enormous number of parameters required , one for every possible pair of nouns ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Not only does this require a vast amount of memory space , it creates a severe data sparseness problem since we require at least some data about each parameter ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "coined the term CONCEPTUAL ASSOCIATION to refer to association values computed between groups of words ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "By assuming that all words within a group behave similarly , the parameter space can be built in terms of the groups rather than in terms of the words ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In this study , conceptual association is used with groups consisting of all categories from the 1911 version of Roget 's thesaurus ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Given two thesaurus categories  and  , there is a parameter which represents the degree of acceptability of the structure  where  is a noun appearing in  and  appears in  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "By the assumption that words within a group behave similarly , this is constant given the two categories ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Following  we can formally write this parameter as  where the event  denotes the modification of a noun in  by a noun in  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "To ensure that the test set is disjoint from the training data , all occurrences of the test noun compounds have been removed from the training corpus ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Two types of training scheme are explored in this study , both unsupervised ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The first employs a pattern that follows  in counting the occurrences of subcomponents ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A training instance is any sequence of four words  where  and  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Let  be the number of times a sequence  occurs in the training corpus with  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The second type uses a window to collect training instances by observing how often a pair of nouns co-occur within some fixed number of words ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In this study , a variety of window sizes are used ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For  , let  be the number of times a sequence  occurs in the training corpus where  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Note that windowed counts are asymmetric ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In the case of a window two words wide , this yields the mutual information metric proposed by  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Using each of these different training schemes to arrive at appropriate counts it is then possible to estimate the parameters ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Since these are expressed in terms of categories rather than words , it is necessary to combine the counts of words to arrive at estimates ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In all cases the estimates used are :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "where"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Here  is the number of categories in which w appears ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "It has the effect of dividing the evidence from a training instance across all possible categories for the words ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The normaliser ensures that all parameters for a head noun sum to unity ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Given the high level descriptions in section  it remains only to formalise the decision process used to analyse a noun compound ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Each test compound presents a set of possible analyses and the goal is to choose which analysis is most likely ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For three word compounds it suffices to compute the ratio of two probabilities , that of a left-branching analysis and that of a right-branching one ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "If this ratio is greater than unity , then the left-branching analysis is chosen ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "When it is less than unity , a right-branching analysis is chosen ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "If the ratio is exactly unity , the analyser guesses left-branching , although this is fairly rare for conceptual association as shown by the experimental results below ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For the adjacency model , when the given compound is  , we can estimate this ratio as :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For the dependency model , the ratio is :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In both cases , we sum over all possible categories for the words in the compound ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Because the dependency model equations have two factors , they are affected more severely by data sparseness ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "If the probability estimate for  is zero for all possible categories  and  then both the numerator and the denominator will be zero ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This will conceal any preference given by the parameters involving  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In such cases , we observe that the test instance itself provides the information that the event  can occur and we recalculate the ratio using  for all possible categories  where k is any non-zero constant ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "However , no correction is made to the probability estimates for  and  for unseen cases , thus putting the dependency model on an equal footing with the adjacency model above ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The equations presented above for the dependency model differ from those developed in  in one way ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "There , an additional weighting factor ( of 2.0 ) is used to favour a left-branching analysis ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This arises because their construction is based on the dependency model which predicts that left-branching analyses should occur twice as often ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Also , the work reported in  uses simplistic estimates of the probability of a word given its thesaurus category ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The equations above assume these probabilities are uniformly constant ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Section  below shows the result of making these two additions to the method ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The schemes used are :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "the pattern given in section  ;"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "and windowed training schemes with window widths of 2 , 3 , 4 , 5 , 10 , 50 and 100 words ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The accuracy on the test set for all these experiments is shown in figure  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "As can be seen , the dependency model is more accurate than the adjacency model ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This is true across the whole spectrum of training schemes ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The proportion of cases in which the procedure was forced to guess , either because no data supported either analysis or because both were equally supported , is quite low ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For the pattern and two-word window training schemes , the guess rate is less than 4 % for both models ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In the three-word window training scheme , the guess rates are less than 1 % ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For all larger windows , neither model is ever forced to guess ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In the case of the pattern training scheme , the difference between 68.9 % for adjacency and 77.5 % for dependency is statistically significant at the 5 % level ( p = 0.0316 ) , demonstrating the superiority of the dependency model , at least for the compounds within Grolier 's encyclopedia ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In no case do any of the windowed training schemes outperform the pattern scheme ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "It seems that additional instances admitted by the windowed schemes are too noisy to make an improvement ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Initial results from applying these methods to the EMA corpus have been obtained by  , and support the conclusion that the dependency model is superior to the adjacency model ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "suggest two improvements to the method used above ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "These are :"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "a factor favouring left-branching which arises from the formal dependency construction ; and"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "factors allowing for naive estimates of the variation in the probability of categories ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "While these changes are motivated by the dependency model , I have also applied them to the adjacency model for comparison ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "To implement them , equations  and  must be modified to incorporate a factor of  in each term of the sum and the entire ratio must be multiplied by two ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Five training schemes have been applied with these extensions ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The accuracy results are shown in figure  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "For comparison , the untuned accuracy figures are shown with dotted lines ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A marked improvement is observed for the adjacency model , while the dependency model is only slightly improved ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "To determine the difference made by conceptual association , the pattern training scheme has been retrained using lexical counts for both the dependency and adjacency model , but only for the words in the test set ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "If the same system were to be applied across all of  ( a total of 90,000 nouns ) , then around 8.1 billion parameters would be required ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Left-branching is favoured by a factor of two as described in the previous section , but no estimates for the category probabilities are used ( these being meaningless for the lexical association method ) ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Accuracy and guess rates are shown in figure  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Conceptual association outperforms lexical association , presumably because of its ability to generalise ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "One problem with the training methods given in section  is the restriction of training data to nouns in  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Many nouns , especially common ones , have verbal or adjectival usages that preclude them from being in  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Yet when they occur as nouns , they still provide useful training information that the current system ignores ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "To test whether using tagged data would make a difference , the freely available  was applied to the corpus ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Since no manually tagged training data is available for our corpus , the tagger 's default rules were used ( these rules were produced by"}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This results in rather poor tagging accuracy , so it is quite possible that a manually tagged corpus would produce better results ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Three training schemes have been used and the tuned analysis procedures applied to the test set ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Figure  shows the resulting accuracy , with accuracy values from figure  displayed with dotted lines ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "If anything , admitting additional training data based on the tagger introduces more noise , reducing the accuracy ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "However , for the pattern training scheme an improvement was made to the dependency model , producing the highest overall accuracy of 81 % ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The experiments above demonstrate a number of important points ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "At the very least , this information can be applied in broad coverage parsing to assist in the control of search ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "I have also shown that with a corpus of moderate size it is possible to get reasonable results without using a tagger or parser by employing a customised training pattern ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "While using windowed co-occurrence did not help here , it is possible that under more data sparse conditions better performance could be achieved by this method ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The significance of the use of conceptual association deserves some mention ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "I have argued that without it a broad coverage system would be impossible ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In this study , not only has the technique proved its worth by supporting generality , but through generalisation of training information it outperforms the equivalent lexical association approach given the same information ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Amongst all the comparisons performed in these experiments one stands out as exhibiting the greatest contrast ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In all experiments the dependency model provides a substantial advantage over the adjacency model , even though the latter is more prevalent in proposals within the literature ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "This result is in accordance with the informal reasoning given in section  ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "In all , the most accurate technique achieved an accuracy of 81 % as compared to the 67 % achieved by guessing left-branching ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Given the high frequency of occurrence of noun compounds in many texts , this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "A variety of statistical methods for noun compound analysis are implemented and compared ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "The results support two main conclusions ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "First , the use of conceptual association not only enables a broad coverage , but also improves the accuracy ."}
 {"title": "Corpus Statistics Meet the Noun Compound : Some Empirical Results", "sentence": "Second , an analysis model based on dependency grammar is substantially more accurate than one based on deepest constituents , even though the latter is more prevalent in the literature ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "A number of researchers have shown that there is organisation in discourse above the level of the individual utterance  ,  ,  ,  ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The current exploratory study uses control as a parameter for identifying these higher level structures ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We then go on to address how conversational participants co-ordinate moves between these higher level units , in particular looking at the ways they use to signal the beginning and end of such high level units ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Previous research has identified three means by which speakers signal information about discourse structure to listeners : Cue words and phrases  ,  ; Intonation  ; Pronominalisation  ,  ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In the cue words approach ,  has claimed that phrases like `` because '' , `` so '' , and `` but '' offer explicit information to listeners about how the speaker 's current contribution to the discourse relates to what has gone previously ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For example a speaker might use the expression `` so '' to signal that s / he is about to conclude what s / he has just said ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "relate the use of such phrases to changes in attentional state ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "An example would be that `` and '' or `` but '' signal to the listener that a new topic and set of referents is being introduced whereas `` anyway '' and `` in any case '' indicate a return to a previous topic and referent set ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "A second indirect way of signalling discourse structure is intonation ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "showed that intonational contour is closely related to discourse segmentation with new topics being signalled by changes in intonational contour ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "A final more indirect cue to discourse structure is the speaker 's choice of referring expressions and grammatical structure ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "A number of researchers  ,  ,  ,  have given accounts of how these relate to the continuing , retaining or shifting of focus ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The above approaches have concentrated on particular surface linguistic phenomena and then investigated what a putative cue serves to signal in a number of dialogues ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The problem with this approach is that the cue may only be an infrequent indicator of a particular type of shift ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "If we want to construct a general theory of discourse than we want to know about the whole range of cues serving this function ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "This study therefore takes a different approach ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We begin by identifying all shifts of control in the dialogue and then look at how each shift was signalled by the speakers ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "A second problem with previous research is that the criteria for identifying discourse structure are not always made explicit ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In this study explicit criteria are given : we then go on to analyse the relation between cues and this structure ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The data were recordings of telephone conversations between clients and an expert concerning problems with software ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The tape recordings from four dialogues were then transcribed and the analysis conducted on the typewritten transcripts rather than the raw recordings ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "There was a total of 450 turns in the dialogues ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Each utterance in the dialogue was classified into one of four categories :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Assertions - declarative utterances which were used to state facts ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Yes or no answers to questions were also classified as assertions on the grounds that they were supplying the listener with factual information ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Commands - utterances which were intended to instigate action in their audience ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "These included various utterances which did not have imperative form , ( e.g. `` What I would do if I were you is to relink X '' ) but were intended to induce some action ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Questions - utterances which were intended to elicit information from the audience ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "These included utterances which did not have interrogative form ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "e.g. `` So my question is ... ''"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "They also included paraphrases , in which the speaker reformulated or repeated part or all of what had just been said ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Paraphrases were classified as questions on the grounds that the effect was to induce the listener to confirm or deny what had just been stated ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Prompts - These were utterances which did not express propositional content ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Examples of prompts were things like `` Yes '' and `` Uhu '' ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We devised several rules to determine the location of control in the dialogues ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Each of these rules related control to utterance type :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For questions , the speaker was defined as being in control unless the question directly followed a question or command by the other conversant ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The reason for this is that questions uttered following questions or commands are normally attempts to clarify the preceding utterance and as such are elicited by the previous speaker 's utterance rather than directing the conversation in their own right ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For assertions , the speaker was defined as being in control unless the assertion was made in response to a question , for the same reasons as those given for questions ; an assertion which is a response to a question could not be said to be controlling the discourse ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For commands , the speaker was defined as controlling the conversation ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Indirect commands ( i.e. utterances which did not have imperative form but served to elicit some actions ) were also classified in this way ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For prompts , the listener was defined as controlling the conversation , as the speaker was clearly abdicating his / her turn ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In cases where a turn consisted of several utterances , the control rules were only applied to the final utterance ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We applied the control rules and found that control did not alternate from speaker to speaker on a turn by turn basis , but that there were long sequences of turns in which control remained with one speaker ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "This seemed to suggest that the dialogues were organised above the level of individual turns into phases where control was located with one speaker ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The mean number of turns in each phase was 8.03 ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We then went on to analyse how control was exchanged between participants at the boundaries of these phases ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We first examined the last utterance of each phase on the grounds that one mechanism for indicating the end of a phase would be for the speaker controlling the phase to give some cue that he ( both participants in the dialogues were always male ) no longer wished to control the discourse ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "There was a total of 56 shifts of control over the 4 dialogues and we identified 3 main classes of cues used to signal control shifts"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "These were prompts , repetitions and summaries ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We also looked at when no signal was given ( interruptions ) ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "On 21 of the 56 shifts ( 38 % ) , the utterance immediately prior to the control shift was a prompt ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We might therefore explain these shifts as resulting from the person in control explicitly indicating that he had nothing more to say ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "( In the following examples a line indicates a control shift )"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "On a further 15 occasions ( 27 % ) , we found that the person in control of the dialogue signalled that they had no new information to offer ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "They did this either by repeating what had just been said ( 6 occasions ) , or by giving a summary of what they had said in the preceding utterances of the phase ( 9 occasions ) ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We defined a repetition as an assertion which expresses part or all of the propositional content of a previous assertion but which contains no new information ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "A summary consisted of concise reference to the entire set of information given about the client 's problem or the solution plan ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Half the repetitions were accompanied by cue words ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "These were `` and '' , `` well '' and `` so '' , which prefixed the assertion ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "What are the linguistic characteristics of summaries ?"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "suggests that `` so '' might be a summary cue on the part of the speaker but we found only one example of this , although there were 3 instances of `` and '' , one `` now '' one `` but '' and one `` so '' ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In our dialogues the summaries seemed to be characterised by the concise reference to objects or entities which had earlier been described in detail , e.g.  `` Now , I 'm wondering how the two are related '' in which `` the two '' refers to the two error messages which it had taken several utterances to describe previously ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The other characteristic of summaries is that they contrast strongly with the extremely concrete descriptions elsewhere in the dialogues , e.g. `` err the system program standard call file doesn't complete this means that the file does not have a tail record '' followed by `` And I 've no clue at all how to get out of the situation '' ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Example  also illustrates this change from specific (  ,  ,  ) to general  ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "How then do repetitions and summaries operate as cues ?"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In summarising , the speaker is indicating a natural breakpoint in the dialogue and they also indicate that they have nothing more to add at that stage ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Repetitions seem to work in a similar way : the fact that a speaker reiterates indicates that he has nothing more to say on a topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In the previous cases , the person controlling the dialogue gave a signal that control might be exchanged ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "There were 20 further occasions ( 36 % of shifts ) on which no such indication is given ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We therefore went on to analyse the conditions in which such interruptions occurred ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "These seem to fall into 3 categories :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "vital facts ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "responses to vital facts ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "clarifications ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "On a total of 6 occasions ( 11 % of shifts ) the client interrupted to contradict the speaker or to supply what seemed to be relevant information that he believed the expert did not know ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Two of these 6 interjections were to supply extra information and one was marked with the cue `` as well '' ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The other four were to contradict what had just been said and two had explicit markers `` though '' and `` well actually '' : the remaining two being direct denials ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The next class of interruptions occur after the client has made some interjection to supply a missing fact or when the client has blocked a plan or rejected an explanation that the expert has produced ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "There were 8 such occasions ( 14 % of shifts ) ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The interruption in the previous example illustrates the reversion of control to the expert after the client has supplied information which he ( the client ) believes to be highly relevant to the expert ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In the following example , the client is already in control ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Participants can also interrupt to clarify what has just been said ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "This happened on 6 occasions ( 11 % ) of shifts ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "On two occasions clarifications were prefixed by `` now '' and twice by `` so '' ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "On the final two occasions there was no such marker , and a direct question was used ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We have just described the circumstances in which interruptions occur , but can we now explain why they occur ?"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We suggest the following two principles might account for interruptions : these principles concern :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "the information upon which the participants are basing their plans , and"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "the plans themselves ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Information quality :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Both expert and client must believe that the information that the expert has about the problem is true and that this information is sufficient to solve the problem ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "This can be expressed by the following two rules which concern the truth of the information and the ambiguity of the information :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "if the listener believes a fact P and believes that fact to be relevant and either believes that the speaker believes not P or that the speaker does not know P then interrupt ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "If the listener believes that the speaker 's assertion is relevant but ambiguous then interrupt ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Plan quality :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Both expert and client must believe that the plan that the expert has generated is adequate to solve the problem and it must be comprehensible to the client ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The two rules which express this principle concern the effectiveness of the plan and the ambiguity of the plan :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "(B1) If the listener believes P and either believes that P presents an obstacle to the proposed plan or believes that part of the proposed plan has already been satisfied , then interrupt ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "(B2) If the listener believes that an assertion about the proposed plan is ambiguous , then interrupt ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In this framework , interruptions can be seen as strategies produced by either conversational participant when they perceive that a either principle is not being adhered to ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We also investigated whether there were occasions when prompts , repetitions and summaries failed to elicit the control shifts we predicted ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We considered two possible types of failure : either the speaker could give a cue and continue or the speaker could give a cue and the listener fail to respond ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We found no instances of the first case ; although speakers did produce phrases like `` OK '' and then continue , the `` OK '' was always part of the same intonational contour as that further information and there was no break between the two , suggesting the phrase was a prefix and not a cue ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We did , however , find instances of the second case : twice following prompts and once following a summary , there was a long pause , indicating that the speaker was not ready to respond ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We conducted a similar analysis for those cue words that have been identified in the literature ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Only 21 of the 35 repetitions , summaries and interruptions had cue words associated with them and there were also 19 instances of the cue words `` now '' , `` and '' , `` so '' , `` but '' and `` well '' occurring without a control shift ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The analysis so far has been concerned with control shifts where shifts were identified from a series of rules which related utterance type and control ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Examination of the dialogues indicated that there seemed to be different types of control shifts : after some shifts there seemed to be a change of topic , whereas for others the topic remained the same ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We next went on to examine the relationship between topic shift and the different types of cues and interruptions described earlier ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "To do this it was necessary first to classify control shifts according to whether they resulted in shifts of topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We identified topic shifts in the following way : Five judges were presented with the four dialogues and in each of the dialogues we had marked where control shifts occurred ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The judges were asked to state for each control shift whether it was accompanied by a topic shift ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "All five judges agreed on 24 of the 56 shifts , and 4 agreed for another 22 of the shifts ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Where there was disagreement , the majority judgment was taken ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Analysing each type of control shift , it is clear that there are differences between the cues used for the topic shift and the no shift cases ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For interruptions , 90 % occur within topic , i.e. they do not result in topic shifts ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The pattern is not as obvious for prompts and repetitions / summaries , with 57 % of prompts occurring within topic and 67 % of repetitions / summaries occurring within topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "This suggests that change of topic is a carefully negotiated process ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The controlling participant signals that he is ready to close the topic by producing either a prompt or a repetition / summary and this may or may not be accepted by the other participant ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "What is apparent is that it is highly unusual for a participant to seize control and change topic by interruption ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "It seems that on the majority of occasions ( 63 % ) participants wait for the strongest possible cue ( the prompt ) before changing topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We also looked at more general aspects of control within and between topics ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We investigated the number of utterances for which each participant was in control and found that there seemed to be organisation in the dialogues above the level of topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We found that each dialogue could be divided into two parts separated by a topic shift which we labelled the central shift ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The two parts of the dialogue were very different in terms of who controlled and initiated each topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Before the central shift , the client had control for more turns per topic and after it , the expert had control for more turns per topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The respective numbers of turns client and expert are in control before and after the central shift are : Before 11 - 7,22 - 8,12 - 6,21 - 6 ; After 12 - 33,16 - 23,2 - 11,0 - 5 for the four dialogues ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "With the exception of the first topic in Dialogues  and  , the client has control of more turns in every topic before the central shift , whereas after it , the expert has control for more turns in every topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In addition we looked at who initiated each topic , i.e. who produced the first utterance of each topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We found that in each dialogue , the client initiates all the topics before the central shift , whereas the expert initiates the later ones ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We also discovered a close relationship between topic initiation and topic dominance ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In 19 of the 21 topics , the person who initiated the topic also had control of more turns ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "As we might expect , the point at which the expert begins to have control over more turns per topic is also the point at which the expert begins to initiate new topics ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The main result of this exploratory study is the finding that control is a useful parameter for identifying discourse structure ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Using this parameter we identified three levels of structure in the dialogues :"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "control phases ;"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "topic ; and"}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "global organisation ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For the control phases , we found that three types of utterances ( prompts , repetitions and summaries ) were consistently used to signal control shifts ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "For the low level structures we identified , ( i.e. control phases ) , cue words and phrases were not as reliable in predicting shifts ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "This result challenges the claims of recent discourse theories ,  which argue for a the close relation between cue words and discourse structure ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We also examined how utterance type related to topic shift and found that few interruptions introduced a new topic ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Finally there was evidence for high level structures in these dialogues as evidenced by topic initiation and control , with early topics being initiated and dominated by the client and the opposite being true for the later parts ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Another focus of current research has been the modelling of speaker and listener goals  ,  but there has been little research on real dialogues investigating how goals are communicated and inferred ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "This study identifies surface linguistic phenomena which reflect the fact that participants are continuously monitoring their goals ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "When plans are perceived as succeeding , participants use explicit cues such as prompts , repetitions and summaries to signal their readiness to move to the next stage of the plan ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In other cases , where participants perceive obstacles to their goals being achieved , they resort to interruptions and we have tried to make explicit the rules by which they do this ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "In addition our methodology is different from other studies because we have attempted to provide an explanation for whole dialogues rather than fragments of dialogues , and used explicit criteria in a bottom-up manner to identify discourse structures ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "The number of dialogues was small and taken from a single problem domain ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "It seems likely therefore that some of our findings ( e.g the central shift ) will be specific to the diagnostic dialogues we studied ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Further research applying the same techniques to a broader set of data should establish the generality of the control rules suggested here ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We conducted an empirical analysis into the relation between control and discourse structure ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We applied control criteria to four dialogues and identified 3 levels of discourse structure ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "We investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control ."}
 {"title": "Cues and control in Expert-Client Dialogues", "sentence": "Participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This paper is concerned with symmetrical coordination , where the order of the conjuncts ( the items being coordinated by a conjunction such as and or or ) can be altered without affecting acceptability ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Coordination of this kind is traditionally split into constituent coordination , where each conjunct forms a constituent according to ` standard ' phrase structure grammars , and non-constituent coordination ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Constituent and non-constituent coordination have been treated as entirely separate phenomena ( see  for discussion ) , and different mechanisms have been proposed for each ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , by considering grammaticality judgements alone , there seems little justification for such a division ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "To illustrate this , consider the sentence :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Each of the final proper substrings of the sentence ( i.e. some books , Mary some books etc. ) can be used as a conjunct e.g."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Similarly , each of the initial substrings can be used as a conjunct e.g ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "and so can each of the middle substrings e.g ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Only examples  are constituent coordinations ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Example  seems slightly unnatural , but it is much improved if we replace books by a heavier string such as books about gardening ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Thus , for this example , any substring of the sentence can form a viable conjunct ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In the last twenty to thirty years there have been a series of accounts of coordination involving various deletion mechanisms ( from e.g.  to  ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , from the following ` antecedent ' sentence ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": ""}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "resulting in the sentence :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Most deletion accounts assume that deletion is performed under identity of words , but don't analyse what it means for two words to be identical ( an exception is"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Consider the following example of deletion ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Here the two cases of drive are phonologically identical , but have different syntactic categories ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Now consider :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "These are cases of ` zeugma ' and are unacceptable except as jokes ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "It therefore seems that the deleted words must have the same major syntactic category , and the same lexical meaning ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , even if we fix both syntactic category and lexical meaning , we still get some weird coordinations ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , consider :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In example  the two prepositions are attached differently , one to the verb saw , the other to the noun , man ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In example  , attributed to Paul Dekker , the two conjuncts require Mary 's handbag to have a different syntactic structure : the bracketing appropriate for the first conjunct is [ [ a friend of Mary ] 's handbag ] ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The unacceptability of these examples suggests that word by word identity is insufficient , and that deleted material must have identical syntactic structure , as well as identical lexical meanings ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Some of the most compelling arguments against deletion have been semantic ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example ,  argued that deletion accounts are inappropriate for certain constituent coordinations such as :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "since the ` antecedent ' sentence John are alike and Mary are alike is nonsensical ( it is also ungrammatical if we consider number agreement ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , semantically inappropriate or nonsensical ` antecedents ' are also possible when we consider non-constituent coordination ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , consider ` antecedents ' for the following :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "is non-constituent coordination under the primary reading where the scope of former does not contain living in England i.e. where the semantic bracketing is :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Examples  and  could be expanded out at the NP level , but not at the S level ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However  cannot be expanded out at any constituent level , whilst retaining an appropriate semantics ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , expansion at the VP level gives :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Thus , although"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Consider the sentence :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Instead of thinking of John gave and by Chomsky as deleted , we can also think of them as shared by the two conjuncts ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This structure can be represented as follows :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "From the result of the previous section , each conjunct must share not just the phonological material , but also the syntactic structure and the lexical meanings ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "There are three main methods by which this sharing of structure can be achieved : phrasal coordination , 3-D coordination , and processing strategies ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "At first sight , analysing non-constituent coordination using phrasal ( i.e. constituent ) coordination seems nonsensical ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This is not the case ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Coordinations are classified as non-constituent coordination if the conjuncts fail to be constituents in a ` standard ' phrase structure grammar ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , they may well be constituents in other grammars ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , it has been argued that the weaker notion of constituency provided by Categorial Grammars is exactly what is required to allow all conjuncts to be treated as constituents  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Phrasal coordination is exemplified by the schema : X  X Conj X The shared material is necessarily treated identically for each conjunct since there is only a single copy : the conjunction is embedded in a single syntax tree ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The phrasal coordination schema requires each conjunct to be given a single type , and for the conjuncts and the conjunction as a whole to be of the same type ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Problems with the latter requirement were pointed out by  , who gave the following counterexamples :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": ""}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , the two conjuncts in  share the feature +MANNER ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "As it stands , the account does not deal with examples such as the following ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Here the adverbial phrase would presumably be +MANNER , and the prepositional phrase , +TEMP ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Further examples which are problematic for  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "An alternative , suggested by  and similar to  , is to use the following coordination schema :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "X  Y  X Conj Y"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This does not impose any condition that the two categories X and Y share anything in common ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , the new category X  Y is used to ensure that both categories are appropriate in the context ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example ,  is acceptable since the coordination type is NP  AP , and is subcategorises for both NPs and APs ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "A rather more difficult problem is that of providing types for all possible conjuncts ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Consider the following :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "is a conjunction of two pairs of noun phrases ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "is a case of ` unbounded Right-Node Raising ' where the noun phrase Peter is embedded at different depths in the two conjuncts ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "There have been two main approaches to dealing with examples such as  using phrasal coordination ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The first is to introduce an explicit product operator  , allowing types of the form NP * NP ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The second is to use a calculus in which types can undergo ` type-raising '  , or can be formed by abstraction ( as in the Lambek Calculus ,  ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The effect is to treat Fred a book as a verb phrase missing its verb ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The advantage of adopting a general abstraction mechanism , as in the Lambek Calculus , is that this also provides a treatment of examples such as  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Unfortunately , the ability to perform abstraction of categories with functional types ( which is required for  ) also allows shared material to get different syntactic analyses , resulting in acceptance of all the sentences predicted by deletion accounts where identity of lexical categories and lexical semantics is respected , but not identity of syntactic structure ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Reconsider :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "We can obtain identical syntactic types for a friend of and the manufacturer of by subtracting the lexical types of I , saw , Mary , 's , and handbag from the sentence type S ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Since the types are identical , coordination can then take place ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Thus the ability to ` subtract ' one type from another allows the Lambek Calculus to replicate a deletion account , and it thereby suffers from the same problems ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "There have been some proposals to restrict the Lambek Calculus in order to prevent such overgeneration ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "propose a calculus in which  is dealt with using a product operation , and abstraction is limited to categories which do not act as a function in the derivation ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This account makes reasonably good empirical predictions , though it does fail for the following examples :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In  , each conjunct contains different numbers of modifiers of different types ( an adverbial phrase with two prepositional phrases ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In  the subcategorisation order is swapped in the two conjuncts ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Successful treatment of non-constituent coordination using phrasal coordination seems to require elaborate encoding in the conjunct type of a simple generalisation : conjuncts can coordinate provided they are acceptable within the same syntactic context ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The 3-D approaches and processing strategies use syntactic context more directly , and it is to these methods which we now turn ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Let us briefly reconsider our explanation of deletion ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Example  was explained by saying that the two strings by Chomsky and Sue gave are deleted under some notion of identity ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , we could equally well have described this as a process whereby the first instance of by Chomsky is merged with the second ( under some notion of identity ) , and the second instance of Sue gave is merged with the first ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Merging word strings instead of deleting them does not help with the problems of deletion accounts which we outlined earlier ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In particular , it does not help to exclude examples  and  which suggest shared material must have identical syntactic structure ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , once we have started to think in terms of merging , there is an obvious next step , which is to move from merging of word strings to merging of syntax trees ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This is the move made by  , who advocates treating coordination as a union of phrase markers : `` a ` pasting together ' one on top of the other of two trees , with any identical nodes merging together ''  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "We can visualise the result in terms of a three-dimensional tree structure , where the merged material is on one plane , and the syntax trees for each conjunct are on two other planes ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , consider the 3-D tree for example  given in Fig.  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The merged part of the tree includes all the nodes which dominate the shared material Sue gave ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The conjuncts retain separate planes ( denoted here by using stars or crosses for branches ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": ""}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However these can be incorporated into a 3-D account  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "There are various technical difficulties with  ,  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "There is also a fundamental problem concerning semantic interpretation of coordinated structures ( see  which provides a revised and more complex 3-D account based on  ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For coordination of unlike categories , as in the examples in  ,  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However there is still a problem in dealing with examples where there are different numbers of modifiers , such as  or the following :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The syntactic structure appropriate for TNT deliver efficiently has one S node and two VP nodes ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , the structure for TNT deliver after 5 pm in Edinburgh requires one S node and three VP nodes ( or three S nodes and one VP node ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The two structures therefore fail to merge since the structure dominating the shared material TNT deliver must be identical ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The use of ordered phrase structure trees also excludes examples such as  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In summary , the 3-D approaches correctly enforce identity of syntactic structure for shared material ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , the way of characterising syntactic structure using ( parts of ) standard phrase structure trees results in an overly strict requirement of parallelism between the conjuncts ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "We will now consider processing strategies , where syntactic structure of shared material is characterised more indirectly by the state of the parser ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "There have been several attempts to treat coordination by adapting pre-existing parsing strategies ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , ATNs were adapted by  , DCGs by  , and chart parsers by  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": ""}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": ""}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , in parsing ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "at the point after encountering and , the parser can reaccess the configuration after parsing John gave i.e. a stack consisting of a sentence and a verb-phrase , and an arc traversal by the verb ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The second conjunct is then parsed according to this configuration ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "SYSCONJ does not immediately merge the two stack configurations after completing the second conjunct , but , instead , separately parses both conjuncts in parallel until a constituent is completed ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , on parsing the sentence ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "John gave Mary a book and Peter a paper about subjacency"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "the SYSCONJ system separately parses Peter a paper about subjacency and Mary a book about subjacency before conjoining at the level of some enclosing constituent ( for example the verb phase ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The result is therefore similar to starting with the sentence :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "As noted by"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The mechanism also causes problems for dealing with nested coordination ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Consider the sentence :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The smallest constituent containing to study medicine when he was eleven is the verb phase wanted to study medicine when he was eleven ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , if coordination of the first two conjuncts occurs at this level , it is difficult to see how to deal with the final conjunct ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Both"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Thus once something is popped off the stack its internal structure cannot be accessed by the coordination routine ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This rules out examples such as the following ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "where the NP , some books is completed prior to the conjunction being reached ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Although processing accounts can provide reasonable coverage of the coordination data , the exact predictions often require detailed examination of the code ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This suggests a need for the more abstract level of description which dynamic grammars provide ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Dynamics is just the study of states and transitions between states ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "It can be used to specify the states of a left to right parser and the possible mappings between states ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example ,  provides a dynamic description of a shift reduce parser , and a dynamic description of a fully incremental parser based on dependency grammar ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Suitable languages for dynamics are both formal and declarative , and are therefore also appropriate to express linguistic generalisations ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In a Dynamic Grammar  , each word is regarded as an action which performs some change in the syntactic and semantic context ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , a parse of the sentence John likes Mary becomes a mapping between an initial state , c  , through some intermediate states , c  , c  to a final state c  i.e. c   c   c   c  If we use a dynamic grammar to describe a shift reduce parser , states encode the current stack configuration , and are related by rules which correspond to shifting and reducing ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Since there are arbitrarily large numbers of different stack configurations ( the stack can be of arbitrary size ) , the dynamics for shift reduce parsing involves the use of an infinite number of states ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "It thus differs from , say ATNs  , which have a finite number of states , augmented by an explicit recursion mechanism ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Dynamic grammars can be presented as rewrite grammars by using transition types instead of the more usual S or NP ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , to get the parse above we need the lexical entries :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "and a single combination rule schema which states that ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "A string of words is a sentence if it has the type , c  c  where c  and c  are appropriate initial and final states for a parse ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In a dynamic grammar , any substring of a sentence can be assigned a type ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , likes and Mary can be combined to get the type c   c  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Thus we have an appropriate level to perform substring coordination ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Dynamic grammars may be extended using the following combination rule ( and and or are both given the special transition type CONJ ) : F"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Similar to SYSCONJ , this allows coordination when two conjuncts map between the same pairs of states ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Processing is also similar , with the encountering of a conjunction causing back-up to an earlier stage in the parsing history ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However , since there is no popping of a stack , the full parsing history is available ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , Ben gave some books to Sue has the transitions :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "we can then parse papers to Joe using the transitions :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Since the final state c  matches the state immediately before the conjunction , the two strings can combine ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The resulting transition diagram is as follows :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Iterated coordination ( e.g. for examples such as Mary , Peter and Sue ) can be treated in the same way as iterated constituent coordination is treated in phrase structure grammars ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , each transition type can be augmented with a feature ( +/ - ) denoting whether or not that transition has been iterated ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The coordination rule becomes :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Iterated types are formed as follows :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The precise grammaticality predictions made by the dynamic approach depend upon the characterisation of the states , and hence depend on the particular parsing strategy which is specified by the dynamics ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However there are some general predictions which can be made ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Firstly , consider conjuncts which correspond one to one in the categories of the corresponding words ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Here the conjuncts must provide the same transitions , and hence must be able to coordinate ( this is a reflection of the fact that processing can back up to any point in the parsing history ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This predicts that any substring of a sentence can coordinate with itself , and hence that any substring of a sentence can act as a conjunct ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For convenience we will call this the substring hypothesis ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This hypothesis has been broadly adopted in the work of  ,  , and by work on the Lambek Calculus  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Apparent counterexamples are as follows :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "However it is difficult to exclude these using syntactic constraints , without also excluding the more acceptable :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "More natural examples where conjuncts are formed by fragments from different constituents are the following :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The relative unacceptability of the examples in  is perhaps best explained as due to violations of intonational requirements , rather than syntactic requirements  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "One case where the dynamic grammars correctly violate the substring hypothesis is when a string already involves a coordination ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Here , the internal states are not accessible , so we can't get interleaving of two coordinations , as in :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "There may be an argument for similarly blocking coordination in cases which would involve the breaking apart of idioms or other structures which are not standard cases of lexical subcategorisation ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "An example ( due to"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "As noted above , the precise grammaticality predictions depend on the kind of parsing model which is encoded in the states ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "In  , the dynamics specifies a word-by-word incremental parser for a lexicalised version of dependency grammar ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Each state is a recursively defined category , similar to a category in Categorial Grammar ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , after parsing You can call me one possible state is a sentence missing a sentence modifier ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This state is appropriate as the initial state for a parse of both directly , or of after 3 pm through my secretary , resulting in a final state of category sentence ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Thus examples such as  are dealt with , since the syntactic context after You can call me does not distinguish between one or more than one subsequent modifier ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This lack of distinction as to whether one or more modifier is expected is actually a necessary prerequisite for performing decidable fully word-by-word incremental interpretation  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Some of the problems with categorial grammar accounts of coordination do reoccur with a dynamic account based on the parser used in  ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "is predicted to be acceptable , as are the following ,"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This second batch of examples is particularly difficult to exclude without making changes to the characterisation of the states ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "A feature plus or minus tensed verb on each conjunct does block them , but is difficult to motivate ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Dynamic grammars can be regarded purely as formal systems , as direct representations of processing , or as something inbetween ( for example , in the packed parallel parser described in  , the actual parsing states are packed versions of the states in the grammar ) ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "If we consider the dynamics to be a direct representation of processing , then a dependence of linguistic data upon parsing states would only seem plausible if the parsing process corresponds , at least to some extent , with actual human language processing ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This brings up the intriguing possibility that we can predict coordination facts from known processing data , and vice versa ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "For example , consider the well known example of garden pathing :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The choice between the use of raced as the main verb , or as part of the reduced relative is usually assumed to be within the fragment the horse raced , suggesting that there are two distinguished parsing states after raced ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Thus this correctly predicts the unacceptability of the following :"}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This paper has sketched various problems with some of the linguistic accounts of coordination ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "It suggested that this was primarily due to difficulty in encoding a proper notion of syntactic context ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "The paper then considered various processing accounts , where the syntactic context is encoded within the state of the parser ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Finally it showed how dynamics can be used as a formal description of processing accounts which use a full parsing history , and how the characterisations of parsing states can be chosen to enforce the requisite degree of parallelism between conjuncts ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Despite the large amount of theoretical work done on non-constituent coordination during the last two decades , many computational systems still treat coordination using adapted parsing strategies , in a similar fashion to the SYSCONJ system developed for ATNs ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "This paper reviews the theoretical literature , and shows why many of the theoretical accounts actually have worse coverage than accounts based on processing ."}
 {"title": "Non-Constituent Coordination : Theory and Practice", "sentence": "Finally , it shows how processing accounts can be described formally and declaratively in terms of Dynamic Grammars ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "The acquisition of case frame patterns normally involves the following three subproblems :"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Extracting case frames from corpus data ,"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Generalizing case frame slots within these case frames ,"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Learning dependencies that exist between these generalized case frame slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In this paper , we propose a method of learning dependencies between case frame slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "By ` dependency ' is meant the relation that exists between case slots which constrains the possible values assumed by each of those case slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "As illustrative examples , consider the following sentences ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We see that an ` airline company ' can be the subject of verb ` fly ' ( the value of slot ` arg 1 ' ) , when the direct object ( the value of slot ` arg 2 ' ) is an ` airplane ' but not when it is an ` airline company ' ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "These examples indicate that the possible values of case slots depend in general on those of the other case slots : that is , there exist ` dependencies ' between different case slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "The knowledge of such dependencies is useful in various tasks in natural language processing , especially in analysis of sentences involving multiple prepositional phrases , such as"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Note in the above example that the slot of ` from ' and that of ` to ' should be considered dependent and the attachment site of one of the prepositional phrases ( case slots ) can be determined by that of the other with high accuracy and confidence ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "There has been no method proposed to date , however , that learns dependencies between case slots in the natural language processing literature ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In the past research , the distributional pattern of each case slot is learned independently , and methods of resolving ambiguity are also based on the assumption that case slots are independent  ,  ,  ,  ,  ,  ,  , or dependencies between at most two case slots are considered  , ,  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Thus , provision of an effective method of learning dependencies between case slots , as well as investigation of the usefulness of the acquired dependencies in disambiguation and other natural language processing tasks would be an important contribution to the field ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In this paper , we view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Since the number of parameters that exist in a multi-dimensional joint distribution is exponential if we allow n-ary dependencies in general , it is infeasible to estimate them with high accuracy with a data size available in practice ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It is also clear that relatively few of these random variables ( case slots ) are actually dependent on each other with any significance ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Thus it is likely that the target joint distribution can be approximated reasonably well by the product of component distributions of low order , drastically reducing the number of parameters that need to be considered ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This is indeed the approach we take in this paper ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Now the problem is how to approximate a joint distribution by the product of lower order component distributions ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Recently ,  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We employ"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We conducted some experiments to automatically acquire case frame patterns from the Penn Tree Bank bracketed corpus ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Our experimental results indicate that for some classes of verbs the accuracy achieved in a disambiguation experiment can be improved by using the acquired knowledge of dependencies between case slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Suppose that we have data of the type shown in Figure  , given by instances of the case frame of verb ` fly ' automatically extracted from a corpus , using conventional techniques ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "As explained in Introduction , the problem of learning case frame patterns can be viewed as that of estimating the underlying multi-dimensional joint distribution which gives rise to such data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In this research , we assume that case frame instances with the same head are generated by a joint distribution of type ,"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "where index Y stands for the head , and each of the random variables  represents a case slot ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In this paper , we use ` case slots ' to mean surface case slots , and we uniformly treat obligatory cases and optional cases ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Thus the number n of the random variables is roughly equal to the number of prepositions in English ( and less than 100 ) ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "These models can be further classified into three types of probabilistic models according to the type of values each random variable  assumes ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "When  assumes a word or a special symbol ` 0 ' as its value , we refer to the corresponding model  as a ` word-based model ' ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Here ` 0 ' indicates the absence of the case slot in question ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "When  assumes a word-class or ` 0 ' as its value , the corresponding model is called a ` class-based model '."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "When  takes on 1 or 0 as its value , we call the model a ` slot-based model '."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Here the value of ` 1 ' indicates the presence of the case slot in question , and ` 0 ' absence ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "For example , the data in Figure  can be generated by a word-based model , and the data in Figure  by a class-based model ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Suppose for simplicity that there are only 4 possible case slots corresponding respectively to the subject , direct object , ` from ' phrase , and ` to ' phrase ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Then ,"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "is given a specific probability value by a word-based model ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In contrast ,"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "is given a specific probability by a class-based model , where  and  denote word classes ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Finally ,"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "is assigned a specific probability by a slot-based model ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We then formulate the dependencies between case slots as the probabilistic dependencies between the random variables in each of these three models ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In the absence of any constraints , however , the number of parameters in each of the above three models is exponential ( even the slot-based model has  parameters ) , and thus it is infeasible to estimate them in practice ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "A simplifying assumption that is often made to deal with this difficulty is that random variables ( case slots ) are mutually independent ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Suppose for example that in the analysis of the sentence"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "the following alternative interpretations are given ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We wish to select the more appropriate of the two interpretations ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "A heuristic word-based method for disambiguation , in which the random variables ( case slots ) are assumed to be dependent , is to calculate the following values of word-based likelihood and to select the interpretation corresponding to the higher likelihood value ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "If on the other hand we assume that the random variables are independent , we only need to calculate and compare  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "and"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "The independence assumption can also be made in the case of a class-based model or a slot-based model ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "For slot-based models , with the independence assumption , the following probabilities ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "are to be compared  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Assuming that random variables ( case slots ) are mutually independent would drastically reduce the number of parameters ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "( Note that under the independence assumption the number of parameters in a slot-based model becomes O(n) . )"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "As illustrated in Section  , this assumption is not necessarily valid in practice ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "What seems to be true in practice is that some case slots are in fact dependent but overwhelming majority of them are independent , due partly to the fact that usually only a few case slots are obligatory and most others are optional ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Thus the target joint distribution is likely to be approximable by the product of several component distributions of low order , and thus have in fact a reasonably small number of parameters ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We are thus lead to the approach of approximating the target joint distribution by such a simplified model , based on corpus data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Without loss of generality , any n-dimensional joint distribution can be written as"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "for some permutation (  ) of 1,2 , .. , n , where we let  denote  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "A plausible assumption on the dependencies between random variables is intuitively that each variable directly depends on at most one other variable ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "( Note that this assumption is the simplest among those that relax the independence assumption . )"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "For example , if a joint distribution  over 3 random variables  can be written ( approximated ) as follows , it ( approximately ) satisfies such an assumption ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Such distributions are referred to as ` dendroid distributions ' in the literature ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "A dendroid distribution can be represented by a dependency forest ( i.e. a set of dependency trees ) , whose nodes represent the random variables , and whose directed arcs represent the dependencies that exist between these random variables , each labeled with a number of parameters specifying the probabilistic dependency ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "( A dendroid distribution can also be considered as a restricted form of the Bayesian network  . )"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It is not difficult to see that there are 7 and only 7 such representations for the joint distribution  ( See Figure  ) , disregarding the actual numerical values of the probabilistic parameters ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Now we turn to the problem of how to select the best dendroid distribution from among all possible ones to approximate a target joint distribution based on input data ` generated ' by it ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This problem has been investigated in the area of machine learning and related fields ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "A classical method is  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "More recently  , allowing for the possibility of learning one group of random variables to be completely independent of another ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Since many of the random variables ( case slots ) in case frame patterns are essentially independent , this feature is crucial in our context , and we thus employ"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": ""}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It then puts a link between a node pair with the largest mutual information value I , provided that I exceeds a certain threshold which depends on the node pair and adding that link will not create a loop in the current dependency graph ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It repeats this process until no node pair is left unprocessed ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Figure  shows the detail of this algorithm , where  denotes the number of possible values assumed by  , N the input data size , and  denotes the logarithm to the base 2 ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It is easy to see that the number of parameters in a dendroid distribution is of the order  , where k is the maximum of all  , and n is the number of random variables , and the time complexity of the algorithm is of the order  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We will now show how the algorithm works by an illustrative example ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Suppose that the data is given as in Figure  and there are 4 nodes ( random variables )  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "The values of mutual information and thresholds for all node pairs are shown in Table  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Based on this calculation the algorithm constructs the dependency forest shown in Figure  , because the mutual information between  and  ,  and  are large enough , but not the others ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "The result indicates that slot ` arg 2 ' and ` from ' should be considered dependent on ` to ' ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Note that ` arg 2 ' and ` from ' should also be considered dependent via ` to ' but to a somewhat weaker degree ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": ",  ,  ,  ,  which is a principle for statistical estimation in information theory ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It is known that as a strategy of estimation , MDL is guaranteed to be near optimal ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In applying MDL , we usually assume that the given data are generated by a probabilistic model that belongs to a certain class of models and selects a model within the class which best explains the data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It tends to be the case usually that a simpler model has a poorer fit to the data , and a more complex model has a better fit to the data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Thus there is a trade-off between the simplicity of a model and the goodness of fit to data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "MDL resolves this trade-off in a disciplined way : It selects a model which is reasonably simple and fits the data satisfactorily as well ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In our current problem , a simple model means a model with less dependencies , and thus MDL provides a theoretically sound way to learn only those dependencies that are statistically significant in the given data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "An especially interesting feature of MDL is that it incorporates the input data size in its model selection criterion ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This is reflected , in our case , in the derivation of the threshold  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Note that when we do not have enough data ( i.e. for small N ) , the thresholds will be large and few nodes tend to be linked , resulting in a simple model in which most of the case slots are judged independent ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This is reasonable since with a small data size most case slots cannot be determined to be dependent with any significance ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We conducted some experiments to test the performance of the proposed method as a method of acquiring case frame patterns ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In particular , we tested to see how effective the patterns acquired by our method are in a structural disambiguation experiment ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We will describe the results of this experimentation in this section ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In our first experiment , we tried to acquire slot-based case frame patterns ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "First , we extracted 181,250 case frames from the Wall Street Journal ( WSJ ) bracketed corpus of the Penn Tree Bank  as training data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "There were 357 verbs for which more than 50 case frame examples appeared in the training data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Table  shows the verbs that appeared in the data most frequently and the number of their occurrences ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "First we acquired the slot-based case frame patterns for all of the 357 verbs ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We then conducted a ten-fold cross validation to evaluate the ` test data perplexity ' of the acquired case frame patterns , that is , we used nine tenth of the case frames for each verb as training data ( saving what remains as test data ) , to acquire case frame patterns , and then calculated perplexity using the test data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We repeated this process ten times and calculated the average perplexity ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Table  shows the average perplexity obtained for some randomly selected verbs ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We also calculated the average perplexity of the ` independent slot models ' acquired based on the assumption that each case slot is independent ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Our experimental results shown in Table  indicate that the use of the dendroid models can achieve up to  perplexity reduction as compared to the independent slot models ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It seems safe to say therefore that the dendroid model is more suitable for representing the true model of case frames than the independent slot model ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We also used the acquired dependency knowledge in a pp-attachment disambiguation experiment ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We used the case frames of all 357 verbs as our training data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We used the entire bracketed corpus as training data because we wanted to utilize as many training data as possible ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We extracted  or  patterns from the WSJ tagged corpus as test data , using pattern matching techniques such as that described in  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We took care to ensure that only the part of the tagged ( non-bracketed ) corpus which does not overlap with the bracketed corpus is used as test data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "( The bracketed corpus does overlap with part of the tagged corpus . )"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We acquired case frame patterns using the training data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Figure  shows an example of the results , which is part of the case frame pattern ( dendroid distribution ) for the verb ` buy ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "' Note in the model that the slots ` for , ' 'on , ' etc , are dependent on ` arg 2 , ' while ` arg 1 ' and ` from ' are independent ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We found that there were 266 verbs , whose ` arg 2 ' slot is dependent on some of the other preposition slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Table  shows 37 of the verbs whose dependencies between arg 2 and other case slots are positive and exceed a certain threshold , i.e. P ( arg 2 = 1 , prep = 1 ) # GT 0.25 ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "The dependencies found by our method seem to agree with human intuition in most cases ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "There were 93 examples in the test data (  pattern ) in which the two slots ` arg 2 ' and prep of verb are determined to be positively dependent and their dependencies are stronger than the threshold of 0.25 ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We forcibly attached  to verb for these 93 examples ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "For comparison , we also tested the disambiguation method based on the independence assumption proposed by  on these examples ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Table  shows the results of these experiments , where ` Dendroid ' stands for the former method and ` Independent ' the latter ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We see that using the information on dependency we can significantly improve the disambiguation accuracy on this part of the data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Since we can use existing methods to perform disambiguation for the rest of the data , we can improve the disambiguation accuracy for the entire test data using this knowledge ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Furthermore , we found that there were 140 verbs having inter-dependent preposition slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Table  shows 22 out of these 140 verbs such that their case slots have positive dependency that exceeds a certain threshold , i.e.  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Again the dependencies found by our method seem to agree with human intuition ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In the test data ( which are of  pattern ) , there were 21 examples that involves one of the above 22 verbs whose preposition slots show dependency exceeding 0.25 ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We forcibly attached both  and  to verb on these 21 examples , since the two slots  and  are judged dependent ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Table  shows the results of this experimentation , where ` Dendroid ' and ` Independent ' respectively represent the method of using and not using the dependencies ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Again , we find that for the part of the test data in which dependency is present , the use of the dependency knowledge can be used to improve the accuracy of a disambiguation method , although our experimental results are inconclusive at this stage ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We also used the 357 verbs and their case frames used in Experiment 1 to acquire class-based case frame patterns using the proposed method ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We randomly selected 100 verbs among these 357 verbs and attempted to acquire their case frame patterns ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We generalized the case slots within each of these case frames using the method proposed by  to obtain class-based case slots , and then replaced the word-based case slots in the data with the obtained class-based case slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "What resulted are class-based case frame examples like those shown in Figure  ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We used these data as input to the learning algorithm and acquired case frame patterns for each of the 100 verbs ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We found that no two case slots are determined as dependent in any of the case frame patterns ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This is because the number of parameters in a class based model is very large compared to the size of the data we had available ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Our experimental result verifies the validity in practice of the assumption widely made in statistical natural language processing that class-based case slots ( and also word-based case slots ) are mutually independent , at least when the data size available is that provided by the current version of the Penn Tree Bank ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This is an empirical finding that is worth noting , since up to now the independence assumption was based solely on human intuition , to the best of our knowledge ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "To test how large a data size is required to estimate a class-based model , we conducted the following experiment ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We defined an artificial class-based model and generated some data according to its distribution ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We then used the data to estimate a class-based model ( dendroid distribution ) , and evaluated the estimated model by measuring the number of dependencies ( dependency arcs ) it has and the KL distance between the estimated model and the true model ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We repeatedly generated data and observed the ` learning curve , ' namely the relationship between the number of dependencies in the estimated model and the data size used in estimation , and the relationship between the KL distance between the estimated and true model and the data size ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We defined two other models and conducted the same experiments ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Figure  shows the results of these experiments for these three artificial models averaged over 10 trials ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "( The number of parameters in Model 1 , Model 2 , and Model 3 are 18 , 30 , and 44 respectively , while the number of dependencies are 1 , 3 , and 5 respectively . )"}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We see that to accurately estimate a model the data size required is as large as 100 times the number of parameters ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Since a class-based model tends to have more than 100 parameters usually , the current data size available in the Penn Tree Bank ( See Table  ) is not enough for accurate estimation of the dependencies within case frames of most verbs ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We conclude this paper with the following remarks ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "The primary contribution of research reported in this paper is that we have proposed a method of learning dependencies between case frame slots , which is theoretically sound and efficient , thus providing an effective tool for acquiring case dependency information ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "For the slot-based model , sometimes case slots are found to be dependent ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Experimental results demonstrate that using the dependency information , when dependency does exist , structural disambiguation results can be improved ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "For the word-based or class-based models , case slots are judged independent , with the data size currently available in the Penn Tree Bank ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This empirical finding verifies the independence assumption widely made in practice in statistical natural language processing ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We proposed to use dependency forests to represent case frame patterns ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "It is possible that more complicated probabilistic dependency graphs like Bayesian networks would be more appropriate for representing case frame patterns ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "This would require even more data and thus the problem of how to collect sufficient data would be a crucial issue , in addition to the methodology of learning case frame patterns as probabilistic dependency graphs ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Finally the problem of how to determine obligatory / optional cases based on dependencies acquired from data should also be addressed ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We thank Mr. K. Nakamura , Mr. T. Fujita , and Dr. K. Kobayashi of NEC C & C Res. Labs. for their constant encouragement ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We thank Mr. R. Isotani of NEC Information Technology Res. Labs. for his comments ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We thank Ms. Y. Yamaguchi of NIS for her programming effort ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We address the problem of automatically acquiring case frame patterns ( selectional patterns ) from large corpus data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In particular , we propose a method of learning dependencies between case frame slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution , where random variables represent case slots ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "We then formalize the dependencies between case slots as the probabilistic dependencies between these random variables ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Since the number of parameters in a multi-dimensional joint distribution is exponential in general , it is infeasible to accurately estimate them in practice ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions , based on corpus data ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task ."}
 {"title": "Learning Dependencies between Case Frame Slots", "sentence": "Our experimental results indicate that for certain classes of verbs , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "A text is not just a sequence of words , but it also has coherent structure ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The meaning of each word in a text depends on the structure of the text ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Recognizing the structure of text is an essential task in text understanding  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "One of the valuable indicators of the structure of text is lexical cohesion  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Lexical cohesion is the relationship between words , classified as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Reiteration :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Semantic relation :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Reiteration of words is easy to capture by morphological analysis ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Semantic relation between words , which is the focus of this paper , is hard to recognize by computers ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We consider lexical cohesion as semantic similarity between words ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Similarity is computed by spreading activation ( or association )  on a semantic network constructed systematically from an English dictionary ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Whereas it is edited by some lexicographers , a dictionary is a set of associative relation shared by the people in a linguistic community ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity between words is a mapping  :  , where L is a set of words ( or lexicon ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The following examples suggest the feature of the similarity :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The value of  increases with strength of semantic relation between w and w ' ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The following section examines related work in order to clarify the nature of the semantic similarity ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Section  describes how the semantic network is systematically constructed from the English dictionary ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Section  explains how to measure the similarity by spreading activation on the semantic network ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Section  shows applications of the similarity measure -- computing similarity between texts , and measuring coherence of a text ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Section  discusses the theoretical aspects of the similarity ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Words in a language are organized by two kinds of relationship ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "One is a syntagmatic relation : how the words are arranged in sequential texts ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The other is a paradigmatic relation : how the words are associated with each other ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Similarity between words can be defined by either a syntagmatic or a paradigmatic relation ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Syntagmatic similarity is based on co-occurrence data extracted from corpora  , definitions in dictionaries  , and so on ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Paradigmatic similarity is based on association data extracted from thesauri  , psychological experiments  , and so on ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This paper concentrates on paradigmatic similarity , because a paradigmatic relation can be established both inside a sentence and across sentence boundaries , while syntagmatic relations can be seen mainly inside a sentence -- like syntax deals with sentence structure ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The rest of this section focuses on two related works on measuring paradigmatic similarity -- a psycholinguistic approach and a thesaurus-based approach ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Psycholinguists have been proposed methods for measuring similarity ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "One of the pioneering works is ` semantic differential '  which analyses meaning of words into a range of different dimensions with the opposed adjectives at both ends ( see Figure  ) , and locates the words in the semantic space ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Recent works on knowledge representation are somewhat related to"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Most of them describe meaning of words using special symbols like microfeatures  ,  that correspond to the semantic dimensions ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "However , the following problems arise from the semantic differential procedure as measurement of meaning ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The procedure is not based on the denotative meaning of a word , but only on the connotative emotions attached to the word ; it is difficult to choose the relevant dimensions , i.e. the dimensions required for the sufficient semantic space ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "used"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For example , the semantic relation of truck / car and drive / car are captured in the following way :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This method can capture almost all types of semantic relations ( except emotional and situational relation ) , such as paraphrasing by superordinate ( ex . cat / pet ) , systematic relation ( ex .  north / east ) , and non-systematic relation ( ex .  theatre / film ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "However , thesauri provide neither information about semantic difference between words juxtaposed in a category , nor about strength of the semantic relation between words -- both are to be dealt in this paper ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The reason is that thesauri are designed to help writers find relevant words , not to provide the meaning of words ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We analyse word meaning in terms of the semantic space defined by a semantic network , called Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Paradigme is systematically constructed from Glossme , a subset of an English dictionary ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "A dictionary is a closed paraphrasing system of natural language ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each of its headwords is defined by a phrase which is composed of the headwords and their derivations ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "A dictionary , viewed as a whole , looks like a tangled network of words ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We adopted Longman Dictionary of Contemporary English   as such a closed system of English ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "LDOCE has a unique feature that each of its 56,000 headwords is defined by using the words in Longman Defining Vocabulary ( hereafter , LDV ) and their derivations ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "LDV consists of 2,851 words ( as the headwords in LDOCE ) based on the survey of restricted vocabulary  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We made a reduced version of LDOCE , called Glossme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Glossme has every entry of LDOCE whose headword is included in LDV ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Thus , LDV is defined by Glossme , and Glossme is composed of LDV ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Glossme is a closed subsystem of English ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Glossme has 2,851 entries that consist of 101,861 words ( 35.73 words / entry on the average ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "An item of Glossme has a headword , a word-class , and one or more units corresponding to numbered definitions in the entry of LDOCE ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each unit has one head-part and several det-parts ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The head-part is the first phrase in the definition , which describes the broader meaning of the headword ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The det-parts restrict the meaning of the head-part ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( See Figure  . )"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We then translated Glossme into a semantic network Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each entry in Glossme is mapped onto a node in Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Paradigme has 2,851 nodes and 295,914 unnamed links between the nodes ( 103.79 links / node on the average ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Figure  shows a sample node red_1 ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each node consists of a headword , a word-class , an activity-value , and two sets of links : a rfrant and a rfr ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "A rfrant of a node consists of several subrfrants correspond to the units of Glossme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "As shown in Figure  and  , a morphological analysis maps the word brownish in the second unit onto a link to the node brown_1 , and the word colour onto two links to colour_1 ( adjective ) and colour_2 ( noun ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "A rfr of a node p records the nodes referring to p ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For example , the rfr of red_1 is a set of links to nodes ( ex. apple_1 ) that have a link to red_1 in their rfrants ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The rfr provides information about the extension of red_1 , not the intension shown in the rfrant ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each link has thickness  , which is computed from the frequency of the word  in Glossme and other information , and normalized as  in each subrfrant or rfr ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each subrfrant also has thickness ( for example , 0.333333 in the first subrfrant of red_1 ) , which is computed by the order of the units which represents significance of the definitions ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Appendix A describes the structure of Paradigme in detail ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Similarity between words is computed by spreading activation on Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each of its nodes can hold activity , and it moves through the links ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each node computes its activity value  at time  as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "where  and  are the sum of weighted activity ( at time T ) of the nodes referred in the rfrant and rfr respectively ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "And ,  is activity given from outside ( at time T ) ; to ` activate a node ' is to let  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The output function  sums up three activity values in appropriate proportion and limits the output value to [ 0,1 ] ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Appendix B gives the details of the spreading activation ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Activating a node for a certain period of time causes the activity to spread over Paradigme and produce an activated pattern on it ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The activated pattern approximately gets equilibrium after 10 steps , whereas it will never reach the actual equilibrium ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The pattern thus produced represents the meaning of the node or of the words related to the node by morphological analysis ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The activated pattern , produced from a word w , suggests similarity between w and any headword in LDV ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity  is computed in the following way ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( See also Figure  )"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Reset activity of all nodes in Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Activate w with strength s(w) for 10 steps , where s(w) is significance of the word w."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then , an activated pattern P(w) is produced on Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Observe  -- an activity value of the node w ' in P(w) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then ,  is  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The word significance  is defined as the normalized information of the word w in the corpus  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For example , the word red appears 2,308 times in the 5,487,056 - word corpus , and the word and appears 106,064 times ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "So ,  and  are computed as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We estimated the significance of the words excluded from the word list  at the average significance of their word classes ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This interpolation virtually enlarged"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For example , let us consider the similarity between red and orange ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "First , we produce an activated pattern  on Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( See Figure  . ) In this case , both of the nodes red_1 ( adjective ) and red_2 ( noun ) are activated with strength  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Next , we compute  , and observe  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then , the similarity between red and orange is obtained as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The procedure described above can compute the similarity  between any two words w , w ' in LDV and their derivations ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Computer programs of this procedure -- spreading activation ( in C ) , morphological analysis and others ( in Common Lisp ) -- can compute  within 2.5 seconds on a workstation ( SPARCstation 2 ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity  between words works as an indicator of the lexical cohesion ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The following examples illustrate that  increases with the strength of semantic relation :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity  also increases with the co-occurrence tendency of words , for example :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Note that  has direction ( from w to w ' ) , so that  may not be equal to  :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Meaningful words should have higher similarity ; meaningless words ( especially , function words ) should have lower similarity ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity  increases with the significance s(w) and s ( w ' ) that represent meaningfulness of w and w ' :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Note that the reflective similarity  also depends on the significance s(w) , so that  :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity of words in LDV and their derivations is measured directly on Paradigme ; the similarity of extra words is measured indirectly on Paradigme by treating an extra word as a word list  of its definition in LDOCE ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( Note that each  is included in LDV or their derivations . )"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity between the word lists W , W ' is defined as follows ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( See also Figure  . )"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "where P(w) is the activated pattern produced from W by activating each  with strength  for 10 steps ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "And ,  is an output function which limits the value to [ 0,1 ] ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "As shown in Figure  , bottle_1 and wine_1 have high activity in the pattern produced from the phrase `` red alcoholic drink '' ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "So , we may say that the overlapped pattern implies `` a bottle of wine '' ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For example , the similarity between linguistics and stylistics , both are the extra words , is computed as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Obviously , both  and  , where W is an extra word and w is not , are also computable ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Therefore , we can compute the similarity between any two headwords in LDOCE and their derivations ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This section shows the application of the similarity between words to text analysis -- measuring similarity between texts , and measuring text coherence ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Suppose a text is a word list without syntactic structure ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then , the similarity  between two texts X , X ' can be computed as the similarity of extra words described above ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The following examples suggest that the similarity between texts indicates the strength of coherence relation between them :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "It is worth noting that meaningless iteration of words ( especially , of function words ) has less influence on the text similarity :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The text similarity provides a semantic space for text retrieval -- to recall the most similar text in  to the given text X ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Once the activated pattern P ( X ) of the text X is produced on Paradigme , we can compute and compare the similarity  immediately ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( See Figure  . ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Let us consider the reflective similarity  of a text X , and use the notation c ( X ) for  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then , c ( X ) can be computed as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The activated pattern P ( X ) , as shown in Figure  , represents the average meaning of  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "So , c ( X ) represents cohesiveness of X -- or semantic closeness of  , or semantic compactness of X ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( It is also closely related to distortion in clustering . )"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The following examples suggest that c ( X ) indicates the strength of coherence of X :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "= 0.502510 ( coherent ) ,"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "= 0.250840 ( incoherent ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "However , a cohesive text can be incoherent ; the following example shows cohesiveness of the incoherent text -- three sentences randomly selected from LDOCE :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "= 0.560172 ( incoherent , but cohesive ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Thus , c ( X ) can not capture all the aspects of text coherence ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This is because c ( X ) is based only on the lexical cohesion of the words in X ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The structure of Paradigme represents the knowledge system of English , and an activated state produced on it represents word meaning ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This section discusses the nature of the structure and states of Paradigme , and also the nature of the similarity computed on it ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The set of all the possible activated patterns produced on Paradigme can be considered as a semantic space where each state is represented as a point ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The semantic space is a 2,851 - dimensional hypercube ; each of its edges corresponds to a word in LDV ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "LDV is selected according to the following information : the word frequency in written English , and the range of contexts in which each word appears ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "So , LDV has a potential for covering all the concepts commonly found in the world ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This implies the completeness of LDV as dimensions of the semantic space ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": ""}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Our method can be applied to construct a semantic network from an ordinary dictionary whose defining vocabulary is not restricted ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Such a network , however , is too large to spread activity over it ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Paradigme is the small and complete network for measuring the similarity ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The proposed similarity is based only on the denotational and intensional definitions in the dictionary LDOCE ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Lack of the connotational and extensional knowledge causes some unexpected results of measuring the similarity ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For example , consider the following similarity :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This is due to the nature of the dictionary definitions -- they only indicate sufficient conditions of the headword ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For example , the definition of tree in LDOCE tells nothing about leaves : tree n 1 a tall plant with a wooden trunk and branches , that lives for many years 2 a bush or other plant with a treelike form 3 a drawing with a branching form , esp ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "as used for showing family relationships However , the definition is followed by pictures of leafy trees providing readers with connotational and extensional stereotypes of trees ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "In the proposed method , the definitions in LDOCE are treated as word lists , though they are phrases with syntactic structures ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Let us consider the following definition of lift :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Anyone can imagine that something is moving upward ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "But , such a movement can not be represented in the activated pattern produced from the phrase ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The meaning of a phrase , sentence , or text should be represented as pattern changing in time , though what we need is static and paradigmatic relation ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This paradox also arises in measuring the similarity between texts and the text coherence ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "As we have seen in Section  , there is a difference between the similarity of texts and the similarity of word lists , and also between the coherence of a text and cohesiveness of a word list ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "However , so far as the similarity between words is concerned , we assume that activated patterns on Paradigme will approximate the meaning of words , like a still picture can express a story ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We described measurement of semantic similarity between words ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity between words is computed by spreading activation on the semantic network Paradigme which is systematically constructed from a subset of the English dictionary LDOCE ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Paradigme can directly compute the similarity between any two words in LDV , and indirectly the similarity of all the other words in LDOCE ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity between words provides a new method for analysing the structure of text ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "It can be applied to computing the similarity between texts , and measuring the cohesiveness of a text which suggests coherence of the text , as we have seen in Section  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "And , we are now applying it to text segmentation  ,  , i.e. to capture the shifts of coherent scenes in a story ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "In future research , we intend to deal with syntagmatic relations between words ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Meaning of a text lies in the texture of paradigmatic and syntagmatic relations between words  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Paradigme provides the former dimension -- an associative system of words -- as a screen onto which the meaning of a word is projected like a still picture ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The latter dimension -- syntactic process -- will be treated as a film projected dynamically onto Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This enables us to measure the similarity between texts as a syntactic process , not as word lists ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "We regard Paradigme as a field for the interaction between text and episodes in memory -- the interaction between what one is hearing or reading and what one knows  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The meaning of words , sentences , or even texts can be projected in a uniform way on Paradigme , as we have seen in Section  and  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Similarly , we can project text and episodes , and recall the most relevant episode for interpretation of the text ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The semantic network Paradigme is systematically constructed from the small and closed English dictionary Glossme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each entry of Glossme is mapped onto a node of Paradigme in the following way ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( See also Figure  and  . )"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For each entry  in Glossme , map each unit  in  onto a subrfrant  of the corresponding node  in Paradigme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each word  is mapped onto a link or links in  , in the following way :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Let  be the reciprocal of the number of appearance of  ( as its root form ) in Glossme ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "If  is in a head-part , let  be doubled ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Find nodes  corresponds to  ( ex. red  { red_1 , red_2 } ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then , divide  into  in proportion to their frequency ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Add links  to  , where  is a link to the node  with thickness  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Thus ,  becomes a set of links :  , where  is a link with thickness  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then , normalize thickness of the links as  , in each  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Step 2 ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For each node  , compute thickness  of each subrfrant  in the following way :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Let  be the number of subrfrants of  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Let  be  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "( Note that  = 2:1 . )"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Normalize thickness  as  , in each  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Step 3 ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Generate rfr of each node in Paradigme , in the following way :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For each node  in Paradigme , let its rfr  be an empty set ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "For each  , for each subrfrant  of  , for each link  in  :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Let  be the node referred by  , and let  be thickness of  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Add a new link l ' to rfr of  , where l ' is a link to  with thickness  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Thus , each  becomes a set of links :  , where  is a link with thickness  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Then , normalize thickness of the links as  , in each  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Each node  of the semantic network Paradigme computes its activity value  at time  as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "where  and  are activity ( at time T ) collected from the nodes referred in the rfrant and rfr respectively ;  is activity given from outside ( at time T ) ; the output function  limits the value to [ 0,1 ] ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "is activity of the most plausible subrfrant in  , defined as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "where  is thickness of the j-th subrfrant of  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "is the sum of weighted activity of the nodes referred in the j-th subrfrant of  , defined as follows :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "where  is thickness of the k-th link of  , and  is activity ( at time T ) of the node referred by the k-th link of  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "is weighted activity of the nodes referred in the rfr  of  :"}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "where  is thickness of the k-th link of  , and  is activity ( at time T ) of the node referred by the k-th link of  ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "This paper proposes a method for measuring semantic similarity between words as a new tool for text analysis ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity is measured on a semantic network constructed systematically from a subset of the English dictionary , LDOCE ( Longman Dictionary of Contemporary English ) ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "Spreading activation on the network can directly compute the similarity between any two words in the Longman Defining Vocabulary , and indirectly the similarity of all the other words in LDOCE ."}
 {"title": "Similarity between Words Computed by Spreading Activation on an  English Dictionary", "sentence": "The similarity represents the strength of lexical cohesion or semantic relation , and also provides valuable information about similarity and coherence of texts ."}
